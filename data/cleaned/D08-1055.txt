Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 523â€“532,
Honolulu, October 2008. cÂ©2008 Association for Computational Linguistics
A Japanese Predicate Argument Structure Analysis using Decision Lists
Hirotoshi Taira, Sanae Fujita, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho,
Keihanna Science City,
Kyoto 619-0237, Japan
{{taira,sanae}@cslab.kecl, nagata.masaaki@lab}.ntt.co.jp
Abstract
This paper describes a new automatic method
for Japanese predicate argument structure
analysis. The method learns relevant features
to assign case roles to the argument of the tar-
get predicate using the features of the words
located closest to the target predicate under
various constraints such as dependency types,
words, semantic categories, parts of speech,
functional words and predicate voices. We
constructed decision lists in which these fea-
turesweresortedbytheirlearnedweights. Us-
ing our method, we integrated the tasks of se-
mantic role labeling and zero-pronoun iden-
tification, and achieved a 17% improvement
compared with a baseline method in a sen-
tence level performance analysis.
1 Introduction
Recently, predicate argument structure analysis has
attracted the attention of researchers because this
information can increase the precision of text pro-
cessing tasks, such as machine translation, informa-
tion extraction (Hirschman et al., 1999), question
answering (Narayanan and Harabagiu, 2004) (Shen
and Lapata, 2007), and summarization (Melli et
al., 2005). In English predicate argument structure
analysis, large corpora such as FrameNet (Fillmore
et al., 2001), PropBank (Palmer et al., 2005) and
NomBank (Meyers et al., 2004) have been created
and utilized. Recently, the GDA Corpus (Hashida,
2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al.,
2002) and NAIST Text Corpus (Iida et al., 2007)
were constructed in Japanese, and these corpora
have become the target of an automatic Japanese
predicate argument structure analysis system. We
conducted Japanese predicate argument structure
(PAS) analysis for the NAIST Text Corpus, which
is the largest of these three corpora, and, as far as
we know, this is the first time PAS analysis has been
conducted for whole articles of the corpus.
The NAIST Text Corpus has the following char-
acteristics, i) semantic roles for both predicates and
eventnounsareannotatedinthecorpus, ii)threema-
jor case roles,
1
namely the ga, wo and ni-cases in
Japanese are annotated for the base form of pred-
icates and event nouns, iii) both the case roles in
sentences containing the target predicates and those
outside the sentences (zero-pronouns) are annotated,
and iv) coreference relations are also annotated.
As regards i), recently there has been an increase
in the number of papers dealing with nominalized
predicates (Pradhan et al., 2004) (Jiang and Ng,
2006) (Xue, 2006) (Liu and Ng, 2007). For exam-
ple, â€˜tripâ€™ in the sentence â€œDuring my trip to Italy, I
met him.â€ refers not only to the event â€œI met himâ€
but also to the event â€œI traveled to Italy.â€ As in this
example, nounssometimeshaveargumentstructures
referring to an event. Such nouns are called event
nouns (Komachi et al., 2007) in the NAIST Text
Corpus. At the same time, the problems related to
compound nouns are also important. In Japanese, a
compound noun sometimes simultaneously contains
both an event noun and its arguments. For example,
the compound noun, â€˜'ï¿½	)(corporate buyout)â€™
contains an event noun â€˜	)(buyout)â€™ and its ac-
cusative, â€˜'ï¿½(corporate).â€™ However, compound
1
Kyoto Text Corpus has about 15 case roles.
523
nouns provide no information about syntactic de-
pendency or about case markers, so it is difficult to
specify the predicate-argument structure. Komachi
et al. investigated the argument structure of event
nouns using the co-occurrence of target nouns and
their case roles in the same sentence (Komachi et
al., 2007). In these approaches, predicates and event
nouns are dealt with separately. Here, we try to
unify these different argument structures using de-
cision lists.
As regards ii), for example, in the causative sen-
tence, â€˜ï¿½ï¿½ï¿½ï¿½xï¿½ï¿½t&	ï¿½ï¿½^ï¿½dï¿½(Mary
makes Tom fix dinner),â€™ the basic form of the
causative verb, â€˜^ï¿½dï¿½(make fix)â€™ is â€˜^ï¿½(fix),â€™
and its nominative is â€˜ï¿½ï¿½(Tom)â€™ and the ac-
cusative case role (wo-case) is â€˜&	ï¿½(dinner),â€™ al-
though the surface case particle is ni (dative). We
must deal with syntactic transformations in passive,
causative, and benefactive constructions when ana-
lyzing the corpus.
As regards iii) and iv), in Japanese, zero pronouns
often occur, especially when the argument has al-
ready been mentioned in previous sentences. There
have been many studies of zero-pronoun identifica-
tion (Walker et al., 1994) (Nakaiwa, 1997) (Iida et
al., 2006).
In this paper, we present a general procedure for
handling both the case role assignment of predicates
and event nouns, and zero-pronoun identification.
We use the decision list learning of rules to find the
closest words with various constraints, because with
decision lists the readability of learned lists is high
and the learning is fast.
The rest of this paper is organized as follows. We
describe the NAIST Text Corpus, which is our tar-
get corpus in Section 2. We describe our proposed
method in Section 3. The result of experiments us-
ing the NAIST Text Corpus and our method are re-
ported in Section 4 and our conclusions are provided
in Section 5.
2 NAIST
Text Corpus
In the NAIST Text Corpus, three major obligatory
Japanese case roles are annotated, namely the ga-
case (nominative or subjective case), the wo-case
(accusative or direct object) and the ni-case (da-
tive or in-direct object). The NAIST Text Corpus
is based on the Kyoto Text Corpus Ver. 3.0, which
contains 38,384 sentences in 2,929 texts taken from
news articles and editorials in a Japanese newspaper,
the â€˜Mainichi Shinbunâ€™.
We divided these case roles into four types by lo-
cation in the article as in (Iida et al., 2006), i) the
case role depends on the predicate or the predicate
depends on the case role in the intra-sentence (â€˜de-
pendency relationsâ€™), ii) the case role does not de-
pend on the predicate and the predicate does not de-
pend on the case role in the intra-sentence (â€˜zero-
anaphoric (intra-sentential)â€™), iii) the case role is
not in the sentence containing the predicate (â€˜zero-
anaphoric (inter-sentential)â€™), and iv) the case role
and the predicate are in the same phrase (â€˜in same
phraseâ€™). Here, we do not deal with exophora.
We show the distribution of the above four types
in test samples in our split of the NAIST Text
Corpus in Tables 1 and 2. In predicates, the
â€˜dependency relationsâ€™ type in the wo-case and
the ni-case occur frequently. In event nouns,
the â€˜zero-anaphoric (intra-sentential)â€™ and â€˜zero-
anaphoric (inter-sentential)â€™ types in the ga-case oc-
cur frequently. With respect to the â€˜in same phraseâ€™
type, the wo-case occurs frequently.
3 Predicate
Argument Structure Analysis
using Features of Closest Words
In this section, we describe our algorithm. In the
algorithm, we used various constraints when search-
ing for the words located closest to the target predi-
cate. We described these constraints as features with
thedirectproductsofdependencytypes(ic, oc, ga c,
wo c, ni c, sc, nc, fw and bw), generalization levels
(words, semantic categories, parts of speech), func-
tional words and voices.
3.1 Dependency
Types
In Japanese, the functional words in a phrase (Bun-
setsu in Japanese) and the interdependency of bun-
setsu phrases are important for determining the
predicate argument structure. In accordance with
the character of the dependency between the case
roles and the predicates or event nouns, we divided
Japanese word dependency into the following seven
types that cover all dependency types in Japanese.
Additionally,weusetwooptionaldependencytypes.
524
Table 1: Distribution of case roles for predicates (Test Data)
predicate
ga (Nominative) wo (Accusative) ni (Dative)
all 15,996 (100.00%) 8,348 (100.00%) 4,871 (100.00%)
dependency relations 9,591 ( 59.96%) 7,184 ( 86.06%) 4,276 ( 87.78%)
zero-anaphoric (intra-sentential) 3,856 ( 24.11%) 870 ( 10.42%) 360 ( 7.39%)
zero-anaphoric (inter-sentential) 2,496 ( 15.60%) 225 ( 2.70%) 132 ( 2.71%)
in same phrase 53 ( 0.33%) 69 ( 0.83%) 103 ( 2.11%)
Table 2: Distribution of case roles for event nouns (Test Data)
event noun
ga (Nominative) wo (Accusative) ni (Dative)
all 4,099 (100.00%) 2,314 (100.00%) 423 (100.00%)
dependency relations 977 (23.84%) 648 (28.00%) 105 (24.82%)
zero-anaphoric (intra-sentential) 1,672 (40.79%) 348 (15.04%) 135 (31.91%)
zero-anaphoric (inter-sentential) 1,040 (25.37%) 165 (7.13%) 44 (10.40%)
in same phrase 410 (10.00%) 1,153 (49.83%) 139 (32.86%)
Figure 1: Type ic
3.1.1 Incoming
Connection Type (ic)
With this type, the target case role is the head-
word of a bunsetsu phrase and the case role phrase
depends on the target predicate phrase (Figure 1).
3.1.2 Outgoing
Connection Type (oc)
Withthistype, thetargetcaseroleistheheadword
ofaphraseandaphrasecontainingatargetpredicate
or event noun depends on the case role phrase (Fig-
ure 2).
Figure 2: Type oc
525
Figure 3: Type sc
Figure 4: Type ga c, wo c, ni c
3.1.3 â€˜Within the Same Phraseâ€™ Type (sc)
With this type, the target case role and the target
predicate or event noun are in the same phrase (Fig-
ure 3).
3.1.4 â€˜Connection into Other Case role Types
(ga c, wo c, ni c)
With these types, a phrase containing the target
case role depends on a phrase containing another
predeterminedcaserole(Figure4). Weusetheterms
â€˜ga câ€™, â€˜wo câ€™ and â€˜ni câ€™ when the predetermined
case roles are the ga-case, wo-case and ni-case, re-
spectively.
Figure 5: Type nc
3.1.5 Non-connection Type (nc)
With this type, a phrase containing the target case
role and a phrase containing the target predicate or
event noun are in the same article, but these phrases
do not depend on each other (Figure 5).
3.1.6 Optional
Type (fw and bw)
Type fw and bw stand for â€˜forwardâ€™ and â€˜back-
wardâ€™ types, respectively. Type fw means the word
located closest to the target predicate or event noun
without considering functional words or voices.
With fw, the word is located between the top of the
article containing the target predicate and the target
predicate or event noun. Similarly, type bw means
the word located closest to the target predicate or
noun, which is located between the targeted predi-
cate or event noun, and the tail of the article con-
taining the predicate.
3.2 Generalization
Levels
We used three levels of generalization for every case
role candidate, that is, word, semantic category, and
part of speech. Every word is annotated with a part
of speech in the Kyoto Text Corpus, and we used
these annotations. With regard to semantic cate-
gories, we annotated every word with a semantic
category based on a Japanese thesaurus, Nihongo
Goi Taikei. The thesaurus consists of a hierarchy
of 2,710 semantic classes, defined for over 264,312
nouns, with a maximum depth of twelve (Ikehara et
al., 1997). We mainly used the semantic classes of
526
Figure 6: Top 3 levels of the Japanese thesaurus, â€˜Ni-
hongo Goi Taikeiâ€™
the third level, and partly the fourth level, which are
similar to semantic roles. We show the top three lev-
els of the Nihongo Goi Taikei common noun the-
saurus in Figure 6. We annotated the words with
their semantic category by hand.
3.3 Functional
Word and Voice
We used a functional word in the phrase containing
the target case role and active and passive voices for
the predicate as base features.
3.4 Training
Algorithm
Thetrainingalgorithmusedforourmethodisshown
in Figure 7. First, the algorithm constructs features
that search for the words located closest to the tar-
get predicate under various constraints. Next, the
algorithm learns by using linear Support Vector Ma-
chines (SVMs) (Vapnik, 1995). SVMs learn effec-
tive features by the one vs. rest method for every
case role. We used TinySVM
2
as an SVM imple-
mentation. Moreover, we construct decision lists
sorted by weight from linear SVMs. Finally, the al-
gorithm calculates the existing probabilities of case
roles for every predicate or event noun. This step
2
http://chasen.org/
Ëœ
taku/software/TinySVM/
produces the criterion that decides whether or not
we will determine the case roles when there is no in-
terdependency between the case role candidate and
the predicate.
Our split of the NAIST Text Corpus has only
62,264trainingsamplesfor2,874predicates,andwe
predict that there will be a shortage of training sam-
ples when adopting traditional learning algorithms,
such as learning algorithms using entropy. So, we
used SVMs with a high generalization capability to
learn the decision lists.
3.5 Test
Algorithm
The test algorithm of our method is shown in Fig-
ure 8. In the test phase, we analyzed test samples
using decision lists and the existing probabilities of
case roles learned in the training phase. In step 1, we
determinedcaserolesusingadecisionlistconsisting
of features exhibiting case role and predicate inter-
dependency, thatis, ic, oc, ga c, wo c, andni c. This
is because there are many cases in Japanese where
the syntactic constraint is stronger than the seman-
tic constraint when we determine the case roles. In
step 2, we determined case roles using a decision list
of sc (â€˜in same phraseâ€™) for the case roles that were
not determined in step 1. This step was mainly for
event nouns. Japanese event nouns frequently form
compound nouns that contain case roles. In step 3,
we decided whether or not to proceed to the next
step by using the existing probabilities of case roles.
If the probability was less than a certain threshold
(50%), then the algorithm stopped. In step 4, we de-
termined case roles using a decision list of the fea-
tures that have no interdependency, that is, nc, fw
and bw. This step will be executed when the target
case role is syntactically necessary and determined
by the co-occurrence of the case roles and predicate
or event noun without syntactic clues, such as de-
pendency, functional words and voices.
4 Experimental
Results
4.1 Experimental
Setting
We performed our experiments using the NAIST
Text Corpus 1.4Î² (Iida et al., 2007). We used
49,527 predicates and 12,737 event nouns from arti-
cles published from January 1st to January 11th and
the editorials from January to August as training ex-
527
for each predicate p
i
in all predicates appeared in the training corpus do
feature list(p
i
)={} ; n â† 0
clear (x, y)
for each instance p
ij
of p
i, in the training corpus do
Clear order() for all features
a
ij
â†the article including p
ij
W
ij
â†the number of words in a
ij
pred index â†the word index of p
ij
in a
ij
for (m = pred indexâˆ’1; m â‰¥ 1; mâˆ’âˆ’) do
n ++
dep type = get dependency type(w
m,p
ij
)
if dep type == â€˜icâ€™, â€˜ncâ€™, â€˜ga câ€™, â€˜wo câ€™ or â€˜ni câ€™ then inc order(n, dep type, w
m, p
ij
)
else if dep type == â€˜scâ€™ then inc order(n, dep type, â€˜â€™, â€˜â€™)
endif
inc order(n, â€˜fwâ€™, â€˜â€™, â€˜â€™)
if w
m
is the ga-case role then y
n,ga
â† 1 else y
n,ga
â† 0
if w
m
is the wo-case role then y
n,wo
â† 1 else y
n,wo
â† 0
if w
m
is the ni-case role then y
n,ni
â† 1 else y
n,ni
â† 0
end for
for (m = pred index +1; m â‰¤ W
ij
; m ++) do
n ++
dep type = get dependency type(w
m,p
ij
)
if dep type == â€˜ocâ€™, â€˜ncâ€™, â€˜ga câ€™, â€˜wo câ€™ or â€˜ni câ€™ then inc order(n,dep type,w
m,p
ij
)
else if dep type == â€˜scâ€™ then inc order(n, dep type, â€˜â€™, â€˜â€™)
endif
inc order(n, â€˜bwâ€™, â€˜â€™, â€˜â€™)
if w
m
is the ga-case role then y
n,ga
â† 1 else y
n,ga
â† 0
if w
m
is the wo-case role then y
n,wo
â† 1 else y
n,wo
â† 0
if w
m
is the ni-case role then y
n,ni
â† 1 else y
n,ni
â† 0
end for
end for
Learn linear SVMs using (x
1, y
1,ga
), ..., (x
n, y
n,ga
)
Learn linear SVMs using (x
1, y
1,wo
), ..., (x
n, y
n,wo
)
Learn linear SVMs using (x
1, y
1,ni
), ..., (x
n, y
n,ni
)
Make the decision list for p
i, sorting features by weight.
Calculate the existing probabilities of case roles for p
i
.
end for
procedure get dependency type(w
m, p
ij
)
if phrase(w
m
) depends on phrase(p
ij
) then return â€˜icâ€™
else if phrase(p
ij
) depends on phrase(w
m
) then return â€˜ocâ€™
else if phrase(w
m
) depends on phrase(p
ga
) then return â€˜ga câ€™
else if phrase(w
m
) depends on phrase(p
wo
) then return â€˜wo câ€™
else if phrase(w
m
) depends on phrase(p
ni
) then return â€˜ni câ€™
else if phrase(w
m
) equals phrase(p
ij
) then return â€˜scâ€™
else return â€˜ncâ€™
end procedure
procedure inc order(n,dep type,func,voice)
Set a feature f
w
=(w
m,dep type,func,voice) ; order(f
w
)++ ; if order(f
w
) == 1 then x
n,f
w
â† 1
Set a feature f
s
=(sem(w
m
),dep type,func,voice) ; order(f
s
)++ ; if order(f
s
) == 1 then x
n,f
s
â† 1
Set a feature f
p
=(pos(w
m
),dep type,func,voice) ; order(f
p
)++ ; if order(f
p
) == 1 then x
n,f
p
â† 1
feature list(p
i
) â† feature list(p
i
)
uniontext
{f
w,f
s,f
p
}
end procedure
Figure 7: Training algorithm
528
Step 1. Determine case roles using a decision list concerning ic, oc, ga c, wo c and ni c.
Step 2. Determine case roles using a decision list concerning sc for undetermined case roles in
Step.1.
Step 3. If the existing probability of case roles < 50 % then the program ends.
Step 4. Determine case roles using a decision list concerning nc, fw and bw types.
Figure 8: Test algorithm
amples. We used 11,023 predicates and 3,161 event
nouns from articles published on January 12th and
13th and the September editorials as development
examples. And we used 19,501 predicate and 5,276
event nouns from articles dated January 14th to 17th
and editorials dated October to December as test ex-
amples. This is a typical way to split the data.
We used the annotations in the Kyoto Text Corpus
astheinterdependencyofbunsetsuphrases. Weused
bothindividualandmultiplewordsascaseroles. We
used the phrase boundaries annotated in the NAIST
Text Corpus in the training phase, and used those
annotated automatically by our system using POSs
and simple rules in the test phase. The accuracy of
the automatic annotation is about 90%.
4.2 Baseline
Method
To evaluate our algorithm, we conducted experi-
ments using a baseline method. With the method,
we used only nouns that depended on predicates or
event nouns as case role candidates. If the functional
word(post-positionalcase)inthephraseisâ€˜gaâ€™,â€˜woâ€™
and â€˜niâ€™, we determined the ga-case, wo-case, or ni-
case for the candidates. Next, as regardsevent nouns
in compound nouns, if there was another word in a
compound noun containing an event noun and it co-
occurred with the event noun as a case role with a
higher probability in the training samples, then the
word was selected for the case role.
4.3 Entropy
Method
The conventional approach for making decision lists
utilizes the entropy of samples selected by the
rules (Yarowsky, 1994) (Goodman, 2002). We per-
formed comparative experiments using Yarowskyâ€™s
entropy algorithm (Yarowsky, 1994).
Table3: Existingprobabilities ofcaserolesforpredicates
and event nouns
Predicate Existing Probability
or Event Noun ga (NOM) wo (ACC) ni (DAT)
ï¿½O(use) 44.72% 82.92% 5.33%
ï¿½	ï¿½(negotiation) 77.41% 30.70% 0.00%
ï¿½C(participation) 87.09% 0.00% 72.46%
,nX(based on) 81.89% 0.00% 100.00%
4.4 Overall
Results
The overall results are shown in Table 7. Here, â€˜en-
tropyâ€™ indicates Yarowskyâ€™s algorithm, which uses
entropy (Yarowsky, 1994). Throughout the test data,
the F-measure (%) of our method exceeded that of
the baseline system and the â€˜entropyâ€™ system. With
thega-case(nominative)inparticular,theF-measure
increased 9 points.
Table 3 shows some examples of the existing
probabilities of case roles for predicates or event
nouns. When the probabilities are extreme values
such as the ni-case (dative) ofï¿½	ï¿½(negotiation), the
wo-case (accusative) ofï¿½C(participation), and the
wo-case and ni-base of,nX(based on), we can
decide to fill the targeted case role or not with high
precision. However, it is difficult to decide to fill
the targeted case role or not when the probability is
close to 50 percent as in the ga-case ofï¿½O(use).
We show the learned decision list of the ic type
(the case role depends on the predicate or event
noun), sc type (in the same phrase) and the other
typesforeventnounï¿½	ï¿½(negotiation)inTables4,5
and 6, respectively. Here, â€˜wordâ€™ in the â€˜levelâ€™
column means â€˜base form of predicateâ€™ and â€˜semâ€™
means â€™semantic category of predicate.â€™ In the ic
and sc type decision lists, features with semantic
categories, such as â€˜REGIONâ€™, â€™LOCATIONâ€™ and
â€˜EVENTâ€™, occupy a higher order. In contrast, in
the list of the other types, the features that occupy
the higher order are the features of the word base
529
Table 4: Decision list for ic type of event nounï¿½	ï¿½(negotiation)
order case dep type level head word functional voice weight
word
1 ga ic wordzï¿½
ï¿½
ï¿½ï¿½ï¿½(North Korea)w(of) active 0.9820
2 ga ic semï¿½ï¿½(REGION)w(of) active 0.6381
3 ga ic wordï¿½ï¿½(both Japan and U.S.)w(of) active 0.5502
4 wo ic wordï¿½+qï¿½
ï¿½q(establishment of joint ventures)w(of) active 0.5288
5 wo ic word?>ï¿½	ï¿½ï¿½ï¿½(telecommunications)w(of) active 0.4142
6 wo ic wordzï¿½
ï¿½
ï¿½ï¿½ï¿½(North Korea)qw(for) active 0.3168
7 wo ic wordï¿½ï¿½(ACTION)w(of) active 0.3083
8 ga ic semï¿½ï¿½ï¿½ï¿½(OOV NOUN)w(of) active 0.2939
9 wo ic wordï¿½ï¿½	~ï¿½ï¿½ï¿½ï¿½ï¿½(car and auto parts sector)w(of) active 0.2775
10 wo ic sem	ï¿½(LOCATION)w(of) active 0.2471
Table 5: Decision list for sc type of event nounï¿½	ï¿½(negotiation)
order case dep type level head word weight
1 wo sc semï¿½	ï¿½(EVENT) 1.1738
2 wo sc wordï¿½(arrangement) 1.0000
3 ga sc wordï¿½ï¿½ï¿½ï¿½(airline of Japan and China) 0.9392
4 wo sc semï¿½ï¿½(MENTAL STATE) 0.8958
5 ga sc wordï¿½ï¿½%ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½(financial services of Japan and U.S.) 0.8371
6 wo sc wordï¿½ï¿½~(contract extension) 0.7870
7 wo sc wordï¿½+(joint venture) 0.7865
8 wo sc wordï¿½$	tV(intellectual property rights) 0.7224
9 wo sc wordï¿½ï¿½	~ï¿½ï¿½ï¿½(car and auto parts) 0.7196
10 ga sc wordï¿½ï¿½(Japan and North Korea) 0.6771
Table 6: Decision list for other types of event nounï¿½	ï¿½(negotiation)
order case dep type level head word functional word voice weight
1 ga fw wordï¿½(Japan and U.S.) 1.9954
2 ga fw wordFï¿½(Taiwan) 1.9952
3 ga fw wordï¿½(U.S. and North Korea) 1.4979
4 ga fw wordï¿½ï¿½(U.K. and China) 1.1773
5 ga nc wordï¿½(both nations)x(TOP) active 1.1379
6 wo fw wordï¿½
Y	ï¿½=(diplomatic normalization) 1.0000
7 ga bw wordï¿½(U.S. and North Korea) 1.0000
8 ga fw wordï¿½ï¿½(capital and labor) 1.0000
9 wo fw wordï¿½ï¿½	ï¿½ï¿½(automotive area) 1.0000
10 ga nc word
ï¿½M(both sides)x(TOP) active 1.0000
Table 7: Overall results for NAIST Text Corpus (F-measure(%))
training data test data
sentence ga (NOM) wo (ACC) ni (DAT) sentence ga (NOM) wo (ACC) ni (DAT)
baseline 25.32 32.58 74.51 82.70 21.34 30.08 69.48 76.62
entropy 73.46 89.53 92.72 91.09 33.10 45.67 73.28 77.77
our method 64.81 86.76 92.52 92.20 38.06 55.07 75.82 80.45
530
Table 8: Results for predicates in test sets (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 34.44 / 57.40 77.00 / 79.50 79.83 / 83.15
dependency relations 51.96 / 75.53 85.42 / 88.20 81.83 / 89.51
zero-anaphoric (intra-sentential) 0.00 / 30.15 0.00 / 11.41 0.00 / 3.66
zero-anaphoric (inter-sentential) 1.85 / 23.45 3.00 / 9.32 0.00 / 11.76
in same phrase 0.00 / 75.00 0.00 / 51.78 0.00 / 84.65
Table 9: Results for event nouns (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 11.05 / 45.64 32.30 / 61.80 20.85 / 38.88
dependency relations 12.98 / 68.01 25.00 / 62.46 40.00 / 56.05
zero-anaphoric (intra-sentential) 0.00 / 36.19 0.00 / 20.46 0.00 / 6.62
zero-anaphoric (inter-sentential) 1.40 / 23.25 1.06 / 10.37 0.00 / 3.51
in same phrase 58.76 / 78.93 47.44 / 77.96 28.91 / 58.13
form. This means local knowledge of relations be-
tween case roles and predicates or event nouns in
the word level is more important than semantic level
knowledge.
4.5 Results
for Predicates in Test Sets
We show the results we obtained for predicates in
Table 8. The results reveal that our method is supe-
rior to the baseline system. Our algorithm is partic-
ularly effective in the ga-case.
4.6 Results
for Event Nouns in Test Sets
We show the results we obtained for event nouns in
Table 9. This also shows that our method is superior
to the baseline system. The precision with sc type
is high and our method is effective as regards event
nouns.
5 Conclusion
We presented a new method for Japanese automatic
predicate argument structure analysis using deci-
sion lists based on the features of the words located
closest to the target predicate under various con-
straints. The method learns the relative weights of
these different features for case roles and ranks them
using decision lists. Using our method, we inte-
grated the knowledge of case role determination and
zero-pronoun identification, and generally achieved
a high precision in Japanese PAS analysis. In par-
ticular, we can extract knowledge at various levels
from the corpus for event nouns. In future, we will
use richer constraints and research better ways of
distinguishing whether or not cases are obligatory.
Acknowledgments
We thank Ryu Iida and Yuji Matsumoto of NAIST
for the definitions of the case roles in the NAIST
Text Corpus and functional words, and Franklin
Chang for valuable comments.
References
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proc. of the Pacific Asian
Conference on Language, Information and Computa-
tion (PACLING).
Joshua Goodman. 2002. An incremental decision
list learner. In Proc. of the ACL-02 Conference
on Empirical Methods in Natural Language Process-
ing(EMNLP02), pages 17â€“24.
Kouichi Hashida. 2005. Global document annotation
(GDA) manual. http://i-content.org/GDA/.
LynetteHirschman, PatriciaRobinson, LisaFerro, Nancy
Chinchor, Erica Brown, Ralph Grishman, and Beth
Sundheim. 1999. Hub-4 Eventâ€™99 general guidelines.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proc. of the 21st International Confer-
531
ence on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 625â€“632.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus
with predicate-argument and coreference relations. In
Proc. of ACL 2007 Workshop on Linguistic Annota-
tion, pages 132â€“139.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Nihongo Goi
Taikei, A Japanese Lexicon. Iwanami Shoten, Tokyo.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. InProc.oftheConferenceonEmpiricalMeth-
ods in Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Koichi
Hashida. 2002. Construction of a Japanese relevance-
tagged corpus (in Japanese). Proc. of the 8th Annual
Meeting of the Association for Natural Language Pro-
cessing, pages 495â€“498.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Learning-based argument structure
analysis of event-nouns in Japanese. In Proc. of the
Conference of the Pacific Association for Computa-
tional Linguistics (PACLING), pages 120â€“128.
Chang Liu and Hwee Tou Ng. 2007. Learning predictive
structures for semantic role labeling of NomBank. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 208â€“215.
Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of SQUASH,
the SFU question answering summary handler for the
DUC-2005 summarization task. In Proc. of DUC
2005.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In Proc. of HLT-NAACL 2004 Workshop on
Frontiers in Corpus Annotation.
Hiromi Nakaiwa. 1997. Automatic identification of zero
pronouns and their antecedents within aligned sen-
tence pairs. In Proc. of the 3rd Annual Meeting of
the Association for Natural Language Processing (in
Japanese).
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proc. of
the 20th International Conference on Computational
Linguistics (COLING).
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71â€“106.
Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow seman-
tic parsing using support vector machines. In Proc.
of the Human Language Technology Conference/North
American Chapter of the Association of Computa-
tional Linguistics HLT/NAACL 2004.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proc. of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP/CoNLL), pages 12â€“21.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer-Verlag, New York.
M. Walker, M. Iida, and S. Cote. 1994. Japanese dis-
course and the process of centering. Computational
Linguistics, 20(2):193â€“233.
Nianwen Xue. 2006. Semantic role labeling of nomi-
nalized predicates in Chinese. In Proc. of the HLT-
NAACL, pages 431â€“438.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restoration
in Spanish and French. In Proc. of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 88â€“95.
532

