Robust Parsing of the Proposition Bank Gabriele Musillo Depts of Linguistics and Computer Science University of Geneva 2 Rue de Candolle 1211 Geneva 4 Switzerland musillo4@etu.unige.ch Paola Merlo Department of Linguistics University of Geneva 2 Rue de Candolle 1211 Geneva 4 Switzerland merlo@lettres.unige.ch Abstract In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels.
Our results show that the model can be robustly extended to produce more complex output parse trees without any loss in performance and suggest that joint inference of syntactic and semantic representations is a viable alternative to approaches based on a pipeline of local processing steps.
1 Introduction
Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence.
Moving towards a shallow semantic level of representation is a first initial step towards the distant goal of natural language understanding and has immediate applications in question-answering and information extraction.
For example, an automatic flight reservation system processing the sentence I want to book a flight from Geneva to Trento will need to know that from Geneva denotes the origin of the flight and to Trento denotes its destination.
Knowing that these two phrases are prepositional phrases, the information provided by a syntactic parser, is only moderately useful.
The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (Baker et al., 1998) or the Proposition Bank (Palmer et al., 2005), that can be used as training resources for a number of supervised learning paradigms.
We focus here on the Proposition Bank (PropBank).
PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al., 1993).
Verbal predicates in the Penn Treebank (PTB) receive a label REL and their arguments are annotated with abstract semantic role labels A0-A5 or AA for those complements of the predicative verb that are considered arguments while those complements of the verb labelled with a semantic functional label in the original PTB receive the composite semantic role label AM-X, where X stands for labels such as LOC, TMP or ADV, for locative, temporal and adverbial modifiers respectively.
A tree structure with PropBank labels for a sentence from the PTB (section 00) is shown in Figure 1 below.
PropBank uses two levels of granularity in its annotation, at least conceptually.
Arguments receiving labels A0-A5 or AA do not express consistent semantic roles and are specific to a verb, while arguments receiving an AM-X label are supposed to be adjuncts and the respective roles they express are consistent across all verbs.1 Recent approaches to learning semantic role labels are based on two-stage architectures.
The first stage selects the elements to be labelled, while the second determines the labels to be assigned to the selected elements.
While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full 1There are thirteen semantic role labels for modifiers.
See (Palmer et al., 2005) for a detailed discussion of PropBank semantic roles labels.
11 S
a8a8 a8a8 a8a8 a8a8 a8a8 a72a72 a72a72 a72a72 a72a72 a72a72 NP-A1 a16a16a16 a16a16a16 a16 a80a80a80 a80a80a80 a80 the government’s borrowing authority VP a24a24a24a24 a24a24a24a24 a24a24a24a24 a24a24a24a24a24 a8a8 a8a8 a8a8 a8a8a8 a72a72 a72a72 a72a72 a72a72a72 a88a88a88a88 a88a88a88a88 a88a88a88a88 a88a88a88a88a88 VBD-REL dropped PP-AM-TMP a8a8 a72a72IN at NP NN midnight NP-AM-TMP NNP Tuesday PP-A4 a8a8 a72a72TO to NP QP a16a16a16 a80a80a80$ 2.80 trillion PP-A3 a8a8 a72a72IN from NP QP a16a16a16 a80a80a80$ 2.87 trillion Figure 1: A sample syntactic structure from the PropBank with semantic role annotations.
parse (CoNNL, 2004; CoNLL, 2005).
Because of the way the problem has been formulated – as a pipeline of parsing (or chunking) feeding into labelling – specific investigations of integrated approaches that solve both the parsing and the semantic role labelling problems at the same time have not been studied.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser’s accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics.
Weachievepromisingresultsbothonthesimple parsing task, where the accuracy of the parser is measured on the standard Parseval measures, and also on the parsing task where the more complex labelsofPropBankaretakenintoaccount.
Wewill call the former task Penn Treebank parsing (PTB parsing) and the latter task PropBank parsing below.
These results have several consequences.
On the one hand, we show that it is possible to build a single integrated robust system successfully.
This is a meaningful achievement, as a task combining semantic role labelling and parsing is more complex than simple syntactic parsing.
While the shallow semantics of a constituent and its structural position are often correlated, they sometimes diverge.
For example, some nominal temporal modifiers occupy an object position without being objects, like Tuesday in Figure 1 below.
On the other hand, our results indicate that the proposed models are robust.
To model our task accurately, additional parameters must be estimated.
However, given the current limited availability of annotated treebanks, this more complex task will have to be solved with the same overall amount of data, aggravating the difficulty of estimating the model’s parameters due to sparse data.
The limited availability of data is increased further by the high variability of the argumental labels A0-A5 whose semantics is specific to a given verb or a given verb sense.
Solving this more complex problem successfully, then, indicates that the models used are robust.
Finally, we achieve robustness without simplifying the parsing architecture.
Specifically, robustness is achieved without resorting to the stipulation of strong independence assumptions to compensate for the limited availability and high variability of data.
Consequently, such an achievement demonstrates not only that the robustness of the parsing model, but also its scalability and portability.
2 The
Basic Parsing Architecture To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, and are therefore likely to adapt without much modification to the current problem.
This architecture has shown state-of-the-art performance.
SSN parsers comprise two components, one which estimates the parameters of a stochastic model for syntactic trees, and one which searches for the most probable syntactic tree given the 12 parameter estimates.
As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing.
Events in such a model are derivation moves.
The set of well-formed sequences of derivation moves in this parser is defined by a Predictive LR pushdown automaton (Nederhof, 1994), which implements a form of left-corner parsing strategy.
The derivation moves include: projecting a constituent with a specified label, attaching one constituent to another, and shifting a tag-word pair onto the pushdown stack.
Unlike standard history-based models, SSN parsers do not state any explicit independence assumptions between derivation steps.
They use a neural network architecture, called Simple Synchrony Network (Henderson and Lane, 1998), to induce a finite history representation of an unbounded sequence of moves.
The history representation of a parse history d1,...,di−1, which we denote h(d1,...,di−1), is assigned to the constituent that is on the top of the stack before the ith move.
The representation h(d1,...,di−1) is computed from a set f of features of the derivation move di−1 and from a finite set D of recent history representations h(d1,...,dj), where j < i − 1.
Because the history representation computed for the move i − 1 is included in the inputs to the computation of the representation for the next move i, virtually any information about the derivation history could flow from history representation to history representation and be used to estimate the probability of a derivation move.
However, the recency preference exhibited by recursively defined neural networks biases learning towards information which flows through fewer history representations.
(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step.
In addition to history representations, the inputs to h(d1,...,di−1) include handcrafted features of the derivation history that are meant to be relevant to the move to be chosen at step i.
For each of the experiments reported here, the set D that is input to the computation of the history representation of the derivation moves d1,...,di−1 includes the most recent history representation of the following nodes: topi, the node on top of the pushdown stack before the ith move; the left-corner ancestor of topi (that is, the second top-most node on the parser’s stack); the leftmost child of topi; and the most recent child of topi, if any.
The set of features f includes the last move in thederivation,thelabelortagoftopi,thetag-word pair of the most recently shifted word, and the leftmost tag-word pair that topi dominates.
Given the hidden history representation h(d1,···,di−1) of a derivation, a normalized exponential output function is computed by SSNs to estimate a probability distribution over the possible next derivation moves di.2 The second component of SSN parsers, which searches for the best derivation given the parameter estimates, implements a severe pruning strategy.
Such pruning handles the high computational cost of computing probability estimates with SSNs, and renders the search tractable.
The space of possible derivations is pruned in two different ways.
The first pruning occurs immediately after a tag-word pair has been pushed onto the stack: only a fixed beam of the 100 best derivations ending in that tag-word pair are expanded.
For training, the width of such beam is set to five.
A second reduction of the search space prunes the space of possible project or attach derivation moves: a best-first search strategy is applied to the five best alternative decisions only.
The next section describes our model, extended to produce richer output parse trees annotated with semantic role labels.
3 Learning
Semantic Role Labels Previous work on learning function labels during parsing (Merlo and Musillo, 2005; Musillo and Merlo, 2005) assumed that function labels represent the interface between lexical semantics and syntax.
We extend this hypothesis to the semantic role labels assigned in PropBank, as they are an exhaustive extension of function labels, which have been reorganised in a coherent inventory of labelsandassignedexhaustivelytoallsentencesin the PTB.
Because PropBank is built on the PTB, it inherits in part its notion of function labels which is directly integrated into the AM-X role labels.
A0-A5 or AA labels correspond to many of the unlabelled elements in the PTB and also to those elements that PTB annotators had classified as re2The on-line version of Backpropagation is used to train SSN parsing models.
It performs a gradient descent with a maximum likelihood objective function and weight decay regularization (Bishop, 1995).
13 S
a8a8 a8a8 a8a8 a8a8 a8a8 a72a72 a72a72 a72a72 a72a72 a72a72 NP-A1 a16a16a16 a16a16a16 a16 a80a80a80 a80a80a80 a80 the government’s borrowing authority VP a24a24a24a24 a24a24a24a24 a24a24a24a24 a24a24a24a24a24 a8a8 a8a8 a8a8 a8a8a8 a72a72 a72a72 a72a72 a72a72a72 a88a88a88a88 a88a88a88a88 a88a88a88a88 a88a88a88a88a88 VBD-REL dropped PP-AM-TMP a8a8a8 a72a72a72IN(-AM-TMP) at NP NN midnight NP-AM-TMP NNP(-AM-TMP) Tuesday PP-A4 a8a8 a72a72TO to NP QP a16a16a16 a80a80a80$ 2.80 trillion PP-A3 a8a8 a72a72IN from NP QP a16a16a16 a80a80a80$ 2.87 trillion Figure 2: A sample syntactic structure with semantic role labels lowered onto the preterminals.
ceiving a syntactic functional label such as SBJ (subject) or DTV (dative).
Because they are projections of the lexical semantics of the elements in the sentence, semantic role labels are projected bottom-up, they tend to appear low in the tree and they are infrequently found on the higher levels of the parse tree, where projections of grammatical, as opposed to lexical, elements usually reside.
Because they are the interface level with syntax, semantic labels are also subject to distributional constraints that govern syntactic dependencies, such as argument structure or subcategorization.
We attempt to capture such constraints by modelling the c-command relation.
Recall that the c-command relation relates two nodes in a tree, even if they are not close to each other, provided that the first node dominating one node also dominate the other.
This notion of c-command captures both linear and hierarchical constraints and defines the domain in which semantic role labelling applies.
While PTB function labels appear to overlap to alargeextentwithPropBanksemanticrolellabels, work by (Ye and Baldwin, 2005) on semantic labelling prepositional phrases, however, indicates that the function labels in the Penn Treebank are assigned more sporadically and heterogeneously than in PropBank.
Apparently only the “easy” cases have been tagged functionally, because assigning these function tags was not the main goal of the annotation.
PropBank instead was annotated exhaustively, taking all cases into account, annotating multiple roles, coreferences and discontinuous constituents.
It is therefore not void of interest to test our hypothesis that, like function labels, semantic role labels are the interface between syntax and semantics, and they need to be recovered by applying constraints that model both higher level nodes and lower level ones.
We assume that semantic roles are very often projected by the lexical semantics of the words in the sentence.
We introduce this bottom-up lexical information by fine-grained modelling of semantic role labels.
Extending a technique presented in (Klein and Manning, 2003) and adopted in (Merlo and Musillo, 2005; Musillo and Merlo, 2005) for function labels, we split some part-of-speech tags into tags marked with semantic role labels.
The semantic role labels attached to a non-terminal directly projected by a preterminal and belonging to a few selected categories (DIR, EXT, LOC, MNR, PNC, CAUS and TMP) were propagated down to the pre-terminal part-of-speech tag of its head.
To affect only labels that are projections of lexical semantics properties, the propagation takes into account the distance of the projection from the lexical head to the label, and distances greater than two are not included.
Figure 2 illustrates the result of this operation.
In our augmented model, inputs to each history representation are selected according to a linguistically motivated notion of structural locality over which dependencies such as argument structure or subcategorization could be specified.
In SSN parsing models, the set D of nodes that are structurally local to a given node on top of the stack defines the structural distance between this given node and other nodes in the tree.
Such a notion of distance determines the number of history representations through which information passes 14 α β γ δ ǫ ζ η θ φ1 φ2......
...... S VP C-COMMAND Figure 3: Flow of information in original SSN parsers (dashed lines), enhanced by biases specific to semantic role labels to capture the notion of c-command (solid lines).
to flow from the representation of a node i to the representation of a node j.
By adding nodes to the set D, one can shorten the structural distance between two nodes and enlarge the locality domain over which dependencies can be specified.
To capture a locality domain appropriate for semantic role parsing, we add the most recent child oftopi labelledwithasemanticrolelabeltotheset D.
These additions yield a model that is sensitive to regularities in structurally defined sequences of nodes bearing semantic role labels, within and across constituents.
This modification of the biases is illustrated in Figure 3.
This figure displays two constituents, S and VP with some of their respective child nodes.
The VP node is assumed to be on the top of the parser’s stack, and the S one is supposed to be its leftcorner ancestor.
The directed arcs represent the information that flows from one node to another.
According to the original SSN model in (Henderson, 2003), only the information carried over by the leftmost child and the most recent child of a constituent directly flows to that constituent.
In the figure above, only the information conveyed by the nodes α and δ is directly input to the node S.
Similarly, the only bottom-up information directly input to the VP node is conveyed by the child nodes ǫ and θ.
In the original SSN models, nodes bearing a function label such as φ1 and φ2 are not directly input to their respective parents.
In our extended model, information conveyed by φ1 and φ2 directly flows to their respective parents.
So the distance between the nodes φ1 and φ2, which stand in a c-command relation, is shortened.
For more information on this technique to capture domains induced by the c-command relation, see (Musillo and Merlo, 2005).
We report the effects of these augmentations on parsingresultsintheexperimentsdescribedbelow.
4 Experiments
Our extended semantic role SSN parser was trained on sections 2-21 and validated on section 24 from the PropBank.
Training, validating and testing data sets consist of the PTB data annotated with PropBank semantic roles labels, as provided in the CoNLL-2005 shared task (Carreras and Marquez, 2005).
Our augmented model has a total 613 of nonterminals to represents both the PTB and PropBank labels of constituents, instead of the 33 of theoriginalSSNparser.
The580newlyintroduced labels consist of a standard PTB label followed by a set of one or more PropBank semantic role such as PP-AM-TMP or NP-A0-A1.
As a result of lowering the six AM-X semantic role labels, 240 new part-of-speech tags were introduced to partition the original tag set which consisted of 45 tags.
SSN parsers do not tag their input sentences.
To provide the augmented model with tagged input sentences, we trained an SVM tagger whose features and parameters are described in detail in (GimenezandMarquez, 2004).
Trainedonsection 2-21, the tagger reaches a performance of 95.45% on the test set (section 23) using our new tag set.
As already mentioned, argumental labels A0-A5 are specific to a given verb or a given verb sense, thus their distribution is highly variable.
To reduce variability, we add some of the tag-verb pairs licensing these argumental labels to the vocabu15 F R P PropBank training and PropBank parsing task 82.3 82.1 82.4 PropBank training and PTB parsing task 88.8 88.6 88.9 PTB training and PTB parsing task (Henderson, 2003) 88.6 88.3 88.9 Table 1: Percentage F-measure (F), recall (R), and precision (P) of our SSN parser on two different tasks and the original SSN parser.
lary of our model.
We reach a total of 4970 tagword pairs.3 This vocabulary comprises the original 512 pairs of the original SSN model, and our added pairs which must occur at least 10 times in the training data.
Our vocabulary as well as the new 240 POS tags and the new 580 non-terminal labels are included in the set f of features input to the history representations as described in section 2.
We perform two different evaluations on our model trained on PropBank data.
Recall that we distinguish between two parsing tasks: the PropBank parsing task and the PTB parsing task.
To evaluate the first parsing task, we compute the standard Parseval measures of labelled recall and precision of constituents, taking into account not only the 33 original labels but also the 580 newly introduced PropBank labels.
This evaluation gives us an indication of how accurately and exhaustively we can recover this richer set of nonterminal labels.
The results, computed on the testing data set from the PropBank, are shown on the first line of Table 1.
To evaluate the PTB task, we compute the labelled recall and precision of constituents, ignoring the set of PropBank semantic role labels that our model assigns to constituents.
This evaluation indicates how well we perform on the standard PTB parsing task alone, and its results on the testing data set from the PTB are shown on the second line of Table 1.
The third line of Table 1 gives the performance on the simpler PTB parsing task of the original SSN parser (Henderson, 2003), that was trained on the PTB data sets contrary to our SSN model trained on the PropBank data sets.
5 Discussion
These results clearly indicate that our model can perform the PTB parsing task at levels of per3Suchpairsconsistsofatagandawordtoken.
Noattempt at collecting word types was made.
formance comparable to state-of-the-art statistical parsing, by extensions that take the nature of the richer labels to be recovered into account.
They also suggest that the relationship between syntactic PTB parsing and semantic PropBank parsing is strict enough that an integrated approach to the problem of semantic role labelling is beneficial.
In particular, recent models of semantic role labelling separate input indicators of the correlation between the structural position in the tree and the semantic label, such as path, from those indicators that encode constraints on the sequence, such as the previously assigned role (Kwon et al., 2004).
Inthisway,theycanneverencodedirectlytheconstraining power of a certain role in a given structural position onto a following node in its structural position.
In our augmented model, we attempt to capture these constraints by directly modelling syntactic domains defined by the notion of c-command.
Our results also confirm the findings in (Palmer et al., 2005).
They take a critical look at some commonly used features in the semantic role labelling task, such as the path feature.
They suggest that the path feature is not very effective because it is sparse.
Its sparseness is due to the occurrence of intermediate nodes that are not relevant for the syntactic relations between an argument and its predicate.
Our model of domains is less noisy, and consequently more robust, because it can focus only on c-commanding nodes bearing semantic role labels, thus abstracting away from those nodes that smear the pertinent relations.
(Yi and Palmer, 2005) share the motivation of our work.
Like the current work, they observe that the distributions of semantic labels could potentially interact with the distributions of syntactic labels and redefine the boundaries of constituents, thus yielding trees that reflect generalisations over both these sources of information.
Toourknowledge, noresultshaveyetbeenpublished on parsing the PropBank.
Accordingly, it is not possible to draw a straigthforward quantitative 16 F R P (Haghighi et al., 2005) 83.4 83.1 83.7 (Pradhan et al., 2005) 83.3 83.0 83.5 (Punyakanok et al., 2005) 83.1 82.8 83.3 (Marquez et al., 2005) 83.1 82.8 83.3 (Surdeanu and Turmo, 2005) 82.7 82.5 83.0 PropBank SSN 81.6 81.3 81.9 Table 2: Percentage F-measure (F), recall (R), and precision (P) of our Propbank SSN parser and stateof-the-art semantic role labelling systems on the PropBank parsing task (1267 sentences from PropBank validating data sets; Propbank data sets are available at http://www.lsi.upc.edu/ srlconll/st05/st05.html).
comparison between our PropBank SSN parser and other PropBank parsers.
However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels.
An indirect way of comparing our parser with semantic role labellers suggests itself.
We merge the partial trees output by a semantic role labeller with the output of a parser it was trained on, and compute PropBank parsing performance measures on the resulting parse trees.
The first five lines of Table 2 report such measures for the five best semantic role labelling systems (Haghighi et al., 2005; Pradhan et al., 2005; Punyakanok et al., 2005; Marquez et al., 2005; Surdeanu and Turmo, 2005) according to (CoNLL, 2005).
The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)’s parser.
These systems use (Charniak, 2000)’s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al., 2005; Haghighi et al., 2005) or chunks (Marquez et al., 2005; Pradhan et al., 2005) and named entities (Surdeanu and Turmo, 2005).
While our preliminary results indicated in the last line of Table 2 are not state-of-the-art, they do demonstrate the viability of SSN parsers for joint inference of syntactic and semantic representations.
6 Conclusions
In this paper, we have explored extensions to an existing state-of-the-art parsing model.
We have achieved promising results on parsing the Proposition Bank, showing that our extensions are sufficiently robust to produce parse trees annotated with shallow semantic information.
Future work will lie in extracting semantic role relations from such richly annotated trees, for applications such as information extraction or question answering.
In addition, further research will explore the relevance of semantic role features to parse reranking.
Acknowledgements We thank the Swiss National Science Foundation for supporting this research under grant number 101411-105286/1.
We also thank James Henderson and Ivan Titov for allowing us to use and modify their SSN software, Xavier Carreras for providing the CoNLL-2005 shared task data sets and the anonymous reviewers for their valuable comments.
References Collin F.
Baker, Charles J.
Fillmore, and John B.
Lowe. 1998.
The Berkeley FrameNet project.
In Proceedings of the Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics (ACL-COLING’98), pages 86–90, Montreal, Canada.
Christopher M.
Bishop. 1995.
Neural Networks for Pattern Recognition.
Oxford University Press, Oxford, UK.
Xavier Carreras and Lluis Marquez.
2005. Introduction to the conll-2005 shared task: Semantic role labeling.
In Proceedings of CoNLL-2005, Ann Arbor, MI USA.
Eugene Charniak.
2000. A maximum-entropyinspired parser.
In Proceedings of the 1st Meeting of North American Chapter of Association for Computational Linguistics (NAACL’00), pages 132–139, Seattle, Washington.
Michael John Collins.
1999. Head-Driven Statistical Models for Natural Language Parsing.
Ph.D. thesis, Department of Computer Science, University of Pennsylvania.
CoNLL. 2005.
Ninth Conference on Computational Natural Language Learning (CoNLL-2005).
Ann Arbor, MI, USA.
CoNNL. 2004.
Eighth Conference on Computational Natural Language Learning (CoNLL-2004).
Boston, MA, USA.
Daniel Gildea and Daniel Jurafsky.
2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245–288.
Daniel Gildea and Martha Palmer.
2002. The necessity of parsing for predicate argument recognition.
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 239–246, Philadelphia, PA.
Jesus Gimenez and Lluis Marquez.
2004. Svmtool: A general POS tagger generator based on Support Vector Machines.
In Proceedings of the 4th InternationalConference onLanguage Resources andEvaluation (LREC’04), Lisbon, Portugal.
Aria Haghighi, Kristina Toutanova, and Christopher Manning.
2005. A joint model for semantic role labeling.
In Proceedings of CoNLL-2005, Ann Arbor, MI USA.
JamesHendersonandPeterLane. 1998.
Aconnectionist architecture for learning to parse.
In Proceedings of 17th International Conference on Computational Linguistics and the 36th Annual Meeting of the Association for Computational Linguistics (COLINGACL‘98), pages 531–537, University of Montreal, Canada.
Jamie Henderson.
2003. Inducing history representations for broad-coverage statistical parsing.
In Proceedings of the Joint Meeting of the North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conference (NAACL-HLT’03), pages 103–110, Edmonton, Canada.
Dan Klein and Christopher D.
Manning. 2003.
Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting of the ACL (ACL’03), pages 423–430, Sapporo, Japan.
Namhee Kwon, Michael Fleischman, and Eduard Hovy.
2004. Senseval automatic labeling of semantic roles using maximum entropy models.
In Senseval-3, pages 129–132, Barcelona, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19:313–330.
Lluis Marquez, Pere Comas, Jesus Gimenez, and Neus Catala.
2005. Semantic role labeling as sequential tagging.
In Proceedings of CoNLL-2005, Ann Arbor, MI USA.
Paola Merlo and Gabriele Musillo.
2005. Accurate function parsing.
In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 620–627, Vancouver, British Columbia, Canada, October.
Gabriele Musillo and Paola Merlo.
2005. Lexical and structural biases for function parsing.
In Proceedings of the Ninth International Workshop on Parsing Technology, pages 83–92, Vancouver, British Columbia, October.
Mark Jan Nederhof.
1994. Linguistic Parsing and Program Transformations.
Ph.D. thesis, Department of Computer Science, University of Nijmegen.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of semantic roles.
Computational Linguistics, 31:71–105.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky.
2005. Semantic role chunking combining complementary syntactic views.
In Proceedings of CoNLL-2005, Ann Arbor, MI USA.
Vasin Punyakanok, Peter Koomen, Dan Roth, and Wen tau Yih.
2005. Generalized inference with multiple semantic role labeling systems.
In Proceedings of CoNLL-2005, Ann Arbor, MI USA.
Mihai Surdeanu and Jordi Turmo.
2005. Semantic role labeling using complete syntactic analysis.
In Proceedings of CoNLL-2005, Ann Arbor, MI USA.
Patrick Ye and Timothy Baldwin.
2005. Semantic role labelling of prepositional phrases.
In Proceedings of the Second International Joint Conference on Natural Language Processing (IJCNLP-05), pages pp.
779–791, Jeju, South Korea.
Szu-ting Yi and Martha Palmer.
2005. The integration of semantic parsing and semantic role labelling.
In Proceedings of CoNLL’05, Ann Arbor, Michigan .

