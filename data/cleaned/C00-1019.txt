Automated Generalization of Translation Examples Ralf D.
Brown ()a.lnc'g'c Mellon Univclsity, l~a.nguage Technologies 111stil;ul:('.
l)ittsl)urgh, I)A \]5213-3890 ralf+ ~,/cs.
(;1 n u.
cdu Abstract l~t:ovious work has shown thai adding genera.liza.tion of the exa.ml)les in the corpus of a.n exa.ml)le-1)ased machine tra.nsla.tion (I'31LMT) system ea, n reduce 1;he re(ltfire.d amount o\[' pretra.nsla.ted exa.ml)le text l)y as \[iltl(;\]l }is a.ii order o\[' magnitude for Spa.nish-l';nglish and l,'renchl~;nglish I+',I~Mrl '.
Using word clusto.t:itlg to a.tttoma.ticaJly generalize the example eorl>uS ca.n provide the majority o\[' this inlprovement for l,'rench-l'hlglish wil;h no nlanuaI illtervelltioll; the prior work required a.
la.rge I)iliugual diclionary ta.gged wil;}l 1)a.rls of speech aud the manual crea.tion of gl'.%llllll.:ll" rules.
/~y seeding the clustering with a.
small a.mou nt of manuallycrea.ted iM'orma.tion, even t)el;ter t)erl'ornla.nce ea.n be a.chieved.
This pa.l)ev descril)es a.
method whereby bilingual word clustering ca.n 1)e per\[brined using sta.nda.rd 'nto,zoli'n.qttal document cl ustering techniques, a, nd its e\[l'ectiveness at red ucing the.
size of the exam l)le corpus,'eq u ire(I.
1 hltroduction I';xanq)le-Iased Machine 'l'ranslaLion (I';IM'I') relies on a.
collection of textual units (usually sentences) and their tra, nsla, l,ions.
New text |,o 1)e tra, nsla, ted is nla,tched a,ga, inst the sourcelangua.ge ha.If of the colh'x;tion, and the corresponding tra.nsla.tions from the ta.rget-langua.ge half axe used to generate a.
l;ra.nsh~tion of the new text.
l~xperience with several language pairs has shown that producing a.n EBMT system which provides reasomt.ble t, ra.nsla.tion coverage of unrestricted texts using simple textual matching requires on the order of two million words of pre-translated texts (one million words in each l;mguage); if either la.nguage is highly in\[letting, polysynthetic, or (worse yet) a.gglu tina.tive, even llloro text will be required.
It ma.y I)e difficult, time-consuming, and expensive to obtain tha.t much pa.rallel text, pa.rtieula.rly for lesser-used la.nguage pairs.
Thus, it' one' wishes to develop a.
new tr,~nslator ra.pidly a.nd a.t low cost, techniques are needed which permit the 131~MT sys~ tom to 1)erform just as well using substantia.lly less example text.
lk~th the C,a.ijin Ii;I~MT system 1)y Veale and ~ " r \Va.y (\]997) and 1,he a.uthor's l~\]~h/l I sySteln (I999) COllVel't {;he examples in the corpus into teml)la.tes against which the new texts ea.n I)e ma.tched.
(la.ijin va.ria.I)lizes the well-formed segment mappings between source a.nd ta.rget sentences 1;}ta.t it is able to find, using a.
closed set o\[' markers to segment 1.he input into l)hrasos.
q'he a.utllor'.~ syslem i)er\['orms its generaliza.tioll using equix,a.lence classes (both syntactic a.nd sema.ntic) a.nd a.
production-rule grammar.
First, any occurrences of terms conta,ined in a,n equivalence class are replaced l)y a.
token giving 1.he name of the equiwdence (:lass, a.nd then the gramma.r rules a~re used to replace l)a.tterns of words a.nd tokens I)y more genera.l tokens (such as <NI'> for noun phrases).
(\]{town, 1999) showed t\]la.t one ca.n reduce the corpus size by as much as a.\]l order o\[' ma.gnitude in this way.
(liven l;ha.t, explicit, ma.llua.lly-gom~ra.ted equiva.lence classes red uce the need for exam l)le text, an obvious extelmion would l)e I;o ;~tte\]nl)t lo gelleral.e tll(~se classes a.ul;olna.tica.l\[y frolll the corpus of pre-tra.nslated exanlples.
This pa.1)or describes ()lie ~q)l)roa,ch to a.utoma.ted ex1;racl;ioll of equiva.lence classes, using clustering teclmiques.
The rema.inder of this l)aper describes how to 1)erform bilingua.1 word clustering using standard monoh;ngual document clustering techniques 1)y converting the problem space; the va.rious clustering algorithms which were investiga.ted; mid the effectiveness of generaliza.tion using the derived clusters a.t reducing the required amount of example text.
2 Converting
the Problem The task of clustering words a.ccording to their occurrence pa, tterns ca, n 1)e testa,ted as a, sta, ndard document-clustering task by converting the l)rol)lem sl)a.ce.
For each unique word to be clllstered, crea.te a.
l)seudo-doculnent conta.ining the words of the contexts in which theft word N)125 pears, and use the word itself as tile document identifier.
After the pseudo-documents are clustered, retrieving the identitier for each document in a particular cluster l)roduces tile list of words occurring in su\[\[iciently similar contexts to be considered equivalent \['or the l)urposes of generalizing an EBM(1 ~ system.
By itself, this approach only produces a monolingual clustering, but we require a, bilingum clustering fox" proper generalization since different senses of a word will appear in differing contexts.
The method of Barrachina and Vilar (1999) provides the means for injecting bilingual information into the clustering process.
Using a bilingual dictionary -which may be created fl'om the corl)us using statistical meth()<Is, such as those of Peter \]~rown el al (71990) or the author's own l)r(~viotls• work (Brown, 11997) and the parallel text, create a rough ma.pping 1)etween the words in the source-language half of each translation example in tile corpus and tile target-language half el'that example.
Whenever there is exactly one l)ossible translation candidate listed for a word by the mapping, generate a bilingual word pair consisting of the word and its translation.
This word pair will be treated as an indivisible token in further processing, adding bilingual information to the clustering process.
\]eorming 1)airs in this manner causes each distinct translation of a.
word to be treated as a separate sense; although translation pairs do not exactly correspond to word senses, pairs can be formed without any additional knowledge sonrces and are what tile EBM:I' systern requires for its equivalence classes.
1,'or every unique word pair found in the 1)revious step, we a.ccurnulate counts for each word in the surrounding context of its occurrences.
The context of ~n occurrence is defined to be tile N words immediately prior to and the N words immediately following the occurrence; N currently is set to 3.
Because word order is important, counts are accumulated separately for each position within the context, i.e. for N = 3, a particular context word may contribute to any of six different counts, depending on its location relative to the occurrence.
Further, as the distance ffoln the occurrence increases, the surrounding words become less likely to be a true part of the word-pair's context, so tile counts are weighted to give the greatest importance to the words immediately adjacent to the word pair being examined.
Currently, a silnple linear decay fl'om 1.0 to -~ is used, but other decay functions such as the reciprocal of the distance are also possible.
Tile resulting weighted set of word counts tbrms the above-mentioned I)seudodocument which is converted into a term vector Ibr cosine similarity computations (a standaM measure in information retrieval, defined as the dot product of two term vectors normalized to unit length), If the clustering is seeded with a.
set of initial equivalence classes (which will be discussed below), then the equivalences will be used to generalize the contexts as they are added to tile overall counts \['or tile word pair.
Any words in the context for which a unique correspondence can be found (and f'or which the word and its corresponding translation are one of the pah:s in an equivalence class) will be counted as if the name of the equivMence class had been l)resent in the text rather than the original word.
For example, if days of the week are an equivalence class, then ':(lid he come on Fridas:' and "did he leave on Mends3:' will yield identical context vectors for "come" and "leave", maldng it easier \['or those two terms to chlster together.
To illustrate the conversion process, consider tile li'rench word "('inq" in two examl)les where it translates into English as ::five" (thus forming tile word pair "cinq_fi ve") : <NUt> <NI/L> Le ci,zq jours dcpuis la <NUL> <NUL> 73e five dags si~zce lhe ellcs com'me~,cc~w~,t c~z cinq jours.<NUL> they will begin i~), five days .<NUL> where <NUt> is used as a placeholder when the word pair is too near the beginning or end of the sentence for the flfll context to be present.
Note that the word order on the target-language side \]s not considered when building the term vector, so it need llOt be the same as on the source-language side; the examples were chosen with the same word order merely for clarity.
The resulting ternl vector for "cinqJive" is a.s follows, where the numbers in parentheses indicate the context word's position relative to the word pair under consideration: Word Occur Weight <NWl.>(-3) 1 0.333 elles(-3) 1 0.333 1 0.667 commenceront(-2) 1 0.667 Le(q) 1 1.ooo en(-1) 1 1.000 jours(J) 2 2.000 depuis(2) 1 0.667 .(2) 1 0.667 la(3) 1 0.333 <NUL>(3) 1 0.333 Term vectors such as tile above are then clustered to determine equivalent usages among words.
126 3 Clustering Approaches A tota.l of six clustering a.lgoHthms ha.v(~ I)oen 1.ested; th roe variants of grout)-a.vora.go.
('\]tlsl.('.,'ins a.nd i, hree of agglomera.tive clustering.
Incl'omental group-a.vera.ge clustering was ilnplemented tirst, to provide a.
proof of concopt, borore the COml)uta.tiona.lly more expensive a.gglomerative (bottom-up) clusteril~g was i lnplemented.
The incremental groul)-a.vera.ge a.lgoril;hms all exa.mine each word pair in turn, computing a similsu:ity measure to evory existing clustor.
If th(; 1)(;st siinila.rity measur(; is a l)ov(~ a.
l)r(;del;er nfin('d threshold, the new word pair is i)laced in tile corresponding cluster; otherwis% a now (;\]usi;er is crea.ted.
The th roe varianl;s diltT, r only in tile simila.rity moasure eml)loyed: :1.
cosin(; similarity 1)(;1;w(~(;n 1,h(~ i)s(;u(lo<locumonl, a.nd the centroid o1" the oxisting cluster (standard grOUl)-a.vera.ge clusto.rillg;) 2.
a.verage of' i;\]lo cosine similaril;ies l)otwe(;n the l)seudo-docuni(;nl; a.nd all nl(;nll)ers o\[' the 0xisting (:lust(;,' (a.voragc-link clustoring) 3.
square root of' 1;h(; a.vcrag(; of 1;lie S(luared cosine simila.r\]l;io.s I)ctweon l;he l)seudo(locuinent an(\] all molnl)(~,'s or l he existing ('hlster (rool.-nloa.n-sqllar(, nlo(lifical.ion of average-liNl¢ clustering) Thoso i;hro(~ vnria.tiol,S give hlc,'eas\]ngly IIl()l'(': weight to 1,ho nea.rer mcml)ers of' tho oxist.ing cl ust;cr.
Tim t)o(;1;oin-u 1) a.gglomera.tive algoril;hms all funcl;ion I)y (;tea.tills a.
clustor For each I)Seudo(\[o(:unlenl,, t;hon r(;i)(;a.1;(;(lly ln(u:ging l:li(; two clusl;ors witli the \]iighesl; siinila.ril,y score unl,il 110 (,WO C\]tlS|,orH \]lSt,vo,%,q\]iilila,ril;y .~(:Ol'(~ (~x('.(~(;ding a l)re(Iol;ornlino(\] 1;hl:eshold.
The three vari-;/,IIi;S }/,ga, ill differ ()lily ill 1;lio S\]liiilaril,y lllO}lStll'O O llll)loyc(l: \].
cosine simila.rity between clustor centroids (st~ul(la.rd agglomei:a.tivo clustering) 2.
a.vera.ge of cosine sitnilariLy 1)etween men> l)ers of the two clusters (a.vera.ge-tink) 3.
nia.xilnal cosino similarity betweon a.ny pair Of ni('.nll)oi:s of l,\]ie i;wo clusl;(',rs (single-lin\]{) l"oi: (;acli of the va.i:ia.tions a.bovc, the l)r(~(l(;1,er niincd (;hreshol(I is a.
funci;ion of word \['r(xluoncy.
Two words wliich each a.l)l)ea.r only onc(Y in the entire tra.ining text a.nd ha.re a.
high simila.rib, score a.ro more likely to ha.re a.l)l)ea.red in siniila.r contexl;s I)y cohicide.nce l:ha.n 1;wo wor(ls which each a,1)pea.r ill 1;he traJliil/g 1;(;xi; lifty tin-its.
l,'ro( t UO I/cy 5 (J 7 810 \]2 \] 5 >16 Threshold1 \] .00 2 0.85 3 0.80 4 0.75 0.70 0.65 0.60 9 0.55 1 \] 0.50 0.45 0.40 I,'igure \] : Chtsleling 'l'hro.shold t unction I~br exa.ml)le ~ when using threo words on eithor side as context, a.nd a.
linca.r dcca.y in t;erm weights, two singleton words achievo a.
sinlitarit; 5, scor(', of ().321 (1.000 is the ma.ximum t)ossiblc) if just one o\[" the immodia,tely a(lja,ccnt words is the sa.mc for 1)oth, evon if none of' 1;ho other five context words axe the sa, mc'.
/ks the number o\[' occul'renc('s increases, l;ho contri\])ul,ion t,o the simila.rit,y score o\[' hidividua.l words decreases, ma.king it less likely 1;o encounter a high score by chance.
Ilencc, we wish to set a.
si;ricl;er 1;hres\],ol(l \['or clustering low-frequollcy words i;hati higho,'-l'roquelmy words.
The thr(~shold Function is exI)ressc(l in 1,(~rms of tim fr('(lU(mcy o1" occurrence in th(~ 1,ra.il,ing 1.exl.s.
I"or si,,gle, ull('lus(;ere(\[ \vord pairs, I, ho t'requollcy is sinll)ly 1,11o numb(~r ol' 1;hnos I, he wor(I 1)a.ir was (m(:ounl,(u'(,d.
When I)e,'\['orn> ing groul)-a.\,erag(; (;lu.qlx;ring, the l'requoncy assignod l;() a.
('\]/ml;('.r is tim sum o\[' (;h(; frequencios of a.ll the members; for agglomera.l.ive (:lust('.ri)lg, the \['re(ltten(;y is the sum when using cent;roids and 1,he lnaximunl fre(lucn('y <tnlong the m(;mI)oJ'S wllen using l;he average or lmarest-,,(;ighl)or,~imila.rity.
The va.lu(~ of' the (;hr(>shold \['or a.
given pair of ('lusi,('ms is the va.lue of tim thr(~,~hold I'unction a.t the lower word frequency.
\]:igure 1 sl,ows l,h(', threshold tunction used in the (,Xl)Criments whose results a, rc rel)ortcd here; clusterins is only allowed if the simila, rity measure is a.1)ove the indicated threshold vahm.
On its own, clustering is quite suc(:essfill for generalizing EBMT ('Xaml)les, I)ut the fullya.utomated t)roducl;ion of clusters is not comt)a.tible with adding a, l)roduction-rule gra.mma.r as (lcscril)od in (l~rown, \]999).
Therel'ore, the clustering process may 1)e seeded with a.
set of m an u a.lly-gc'nera.ted clusters.
VVhell seed clusters m'e a.va.ilablo., the clusterins process is moditied in two ways.
First, l;he grOUl)-avera.ge a.pl)roa.clms a.dd an initiaJ clusl;er for o.a,('h soed cluslcr and the a.gglolnera.tive a p127 proaches add an initial cluster for each word pair; these initial clusters are tagged with the name of the seed cluster.
Second, whenever a tagged chister is merged with an untagged one or another cluster with the same tag, the combination inherits the tag; further, merging two clusters with different tags is disallowed.
As a result, the initial seed chlsters are expanded by adding additional word pairs while preventing any of the seed clusters from themselves inerging with each other.
One special case is handled sepa.rately, namely numeric strings.
If both the sourcelanguage and target-l~mguage words of a word pair are numeric strings, the word pair is treated as if it had been specified ill the seed class <number>.
Word pairs not containing a digit in either word can optionally be prevented fi'om being added to the <number> chlster unless explicitly seeded in that cluster.
The former feature eusures that nunibers will apl)ear in a.
single cluster, rather than in multiple chlsters.
The latter avoids the inclusion of the many nonnumeric word pairs (primarily adjectives) which would otherwise tend to cluster with numbers, because both they and numbers are used as modifiers.
Once clustering is completed, any clusters which have inherited the same tag (which is possible when using agglomerative clustering) are merged.
Those clusters which contain more than one pseudo-document are output, together with any inherited label, a.nd can be used as a set of equivalence classes for EBMT.
Agglomerative chlstering using the maximal cosine sinfila.rity (single-link) produced the subjectively best clusters, and was used for the experiments described here.
4 Experiment
The Inethod described in the previous two sections was tested on French-English EBMT.
The training corpus was a subset of the 1BM Ilansard corpns of Canadian parliamentary proceedings (Linguistic Data Consortium, 1997), containing a total of slightly more than one million words, approximately half in each language.
Word-level alignment between French and English was pertbrmed using a dictionary containing entries derived statistically from the full Hansard corpus, auglnented by the ARTH, French-English did;iona.ry (ARTFL Project, 1998).
This dictionary was used for all EBMT and chlstering runs.
The efl'ects of varying the amount of training texts were determined by further sl)litting the training corl)us into smaller seglnents aM using differing numbers of segments.
For each Clust I 238 260 348 522 535 1375 1386 1528 ;1563 ;1.652 2008 21.82 2472 3539 M elnbers lJ.IS'l'OIlE HISTOIW ECONOMIE ECONOMY CERTAINI!~MENT CEITAI NLY CERTAINEMENT SURELY CERTES SURELY JAMAIS NEVER PAS NOT I~EUT-F, TRE MAY H~OI~ABLEMENT PROBAI~LY QUE ONLY l.{lfl';N NOTItING S\[JREMENT CERTAINLY SUREMENT SURELY VRAIMENT REALLY CONSERVATEUR CONSEIWATI \q~J CQNSERVATEUII TORY I)EMOCIi,NI.'IQUE DEMOCtl, ATIC I)I~IVl OCRATIQUIE NDP LIBI~RAL LII3ERA L l)l ANII, A%LS LAS \] \])ERNIIjEIES PAST I)ERNIIERIDS Ih;CENT PIOCI\]A INF, S NEXT Q UELQUES FEW QUF, LQUh;S SOME AVONS HA\q'; SOMMES ARI'~ p p tr, p 1,LLC 10RALL CAMPAIGN EM~2CTOIi,A ILF~ EIAECTION FI~I)I~RAM:,S-I)I {OXq N C IAL1,;S FEI)ERA L-PllOVINCIAI, INDUS'FRIEM3~S INI)US'I'IIIAI, OUVRIERES LA BOUR FA(,J()N h;VENT P • 17' I~VIDLNCL CLEARLY EVIDh;NC\]'; OBVIO USIN HOMMF, S POIATICIANS PRISONNIFJS PR/SONEIS RETOUR, BA.CII(, REVENIR BACK CONVENU AGREED SIGNE SIGNEI) VU SEEN AGRJCOLE AGR1C UL'I'URE ENT'IER AROUN\]) E N T I ER T Ill RO U G I\] O U T OCCIDENTAL WESTERN AVIDUGLI~S BI,IND CIIA.USSURI'2S SI-IOES CONSTRUC;I'EURS BUILDh;RS PENSIONN, F,S PENSIONERS RISTRAITES PENSIONERS VETEMENTS CLOTHING POISSON FISI\] PORC IK)RK Figure 2: Sanli)le Chlsters 128 run using clustering, the first K segments of the corl)uS a.re cones.Lena.ted into a.
single file, which is used as inl)ut \['or both the clustering l)t:ogra, m a.nd the EIM:I'.
system. The clust;erltlg 1)rogranl is rtltt (;o deternfine a.
set o1" equivalence classes, a.nd these classes a.re then provkled to tile I';IM:I' systetn a Jest with the tra.ining exa, mples to be indexed, lleld-out lla.nsa.rd text (a,1)I)roxima.lsely d5,0()O words)is then tra.nslaLed, +tnd tile l)ercenta.ge of tile words in the test text for which the I~;I~M~.I ' system could lind ma,tches a.nd generate a.
tl'a.lasla.tion is determined.
To test the efl'ects of adding seed ('lttsters+ a set of' initia.1 clusters was generated with the \]te.lp of the A I:I'I"I, dict;iona.ry.
First, the 500 most frequ(:nt words in the milliou-word \]\]~msa.rd sul)se.t (excluding pun('.\[;uation) were extracted.
These terms were then nmtched a.gMnst the AI~.TFI, dictionary, removing those words which had multi-word transla.tions as well a.s severaJ which listed multil)le parts el" sl)eech For the same tra,nslation (multil>le l>a, rts of speech can only 1)e used i\[' the corresi>on(Iing tra.tlsla.tiolls are distinct f'rom each <)ther).
The remaining d20 tra.nslal.ion pairs, tagged for l)a.rt o\[' speech, were then convert:e(l inl,o se(~(I clusters a.nd l)rovided to the clustering t)rogra.nl.
To fa.cilita.te experiments using the t)re-existing l)roduction-rule grammar, tire a.d(litiona,I tra.nsla£ion I)a,h's from the lna,nually-gelmra, ix~(1 equivaJe.n(:e ('la.sses were a.dded t;o l)rovide seeds for five equiva.\]ence classes which a.re not, l)resent in the dictiona.ry. 5 Results The nlethod (les('ril>ed i,I this l)a, per does (Sttl) jectively) a, very good jol> of clustering like words toget\]wx, a lid using the clusters to getlera.lize EIMT gives a.
(;onsidera.I)le boost, to the.
l)etTVol'ltl~-Lt,ce+ of' the l<\]\]\]\]\/l~\[ ' SySl;(':lll.
l"igure 2 shows a, sa.ml)ling of tile sma.ller clusters generated from 1.\] million words o\[' Hansard text.
While the nmmbers of a, cluster are often semantica,lly linked (a,s in cluster 848, which cotltains types of politica.1 paxl;ies, or cluster stag), they need not be.
Those clusters whose members a.re not semantically linked generaJly contain words which a.17e all the sa.me l)a.rt of sl)eech, numl)er, a.nd gender (a.s in (:luster 2472, which costa.ins exclusively plural nouns) 1)ut a.s will be discussed in t;he next section, even those chlsters whose,neml)ers a.re tota.lly unrela.ted may 1)e useful a.nd correct..
One J'a.h:ly cotlltl\]Otl occurreltce a, lllOllg the smaller clusters is that various synonymous 1;ra.nslnt;ions o\[ a word (from either source or target language) will chlster together, as in cluster \]652.
This is pa.rticula.rly useful when tile ta.rget-language word is the sa.me, a.s this a.llows va.rious wa.ys of expressing t.he same thing to be tra.nsla.ted when ~l.lly Of" those \['OFtlIS ~/l'e present in the tra.ining ('orpl.t s.
Figure 3 shows how adding a.utoma.ticallygenerated equiva.lence classes sul)sta.ntially increases the covers,we of the EI3MT system.
A1terna.tively, lnuch less text is required to rea.ch a.
specific level of coverage.
The lowest curve in the.
graph is tile percentage of the d5,000-word test text for which the EIM:J' system was able to genera.te tra.nsla.tions when using strict lexic+d matching against the trahling corpus.
The lop-most curve shows the best performa.nce, previously achieved using 1)oth a, la.rge set of eqttivalento classes (in t;he fornt of tagged entries from the \]\ItYI'II+'I, dicl;iona.rv) a.nd a.
production-rule gra.nlntar (\]{rows, J999).
Of the two center curves, the lower is the performs.nee when genera.lizing the tra.ining corl)us using the equivalence classes which were autolna.tica.lly gonerated from that same text, a.nd tim upper shows the t)erforma.tlce using ('lustering with the d25 seed pairs.
/ks can b~, seen in Figure 3, 80% coverage of the test text is achieved with less than 300,000 words using nta.ntta.lly-crea.te(l generalizat, ion information a.nd with approxima.tely 300,000 words wllen using a.utonmticallycreaJ;ed genera.liza.tion informa.tion, but requires 1.2 million words when not using genera.liza.ties.
90% covers.we is reached with less than 500,000 words using lna.nua.lly-ereat.ed informa.
lion a.nd should I>e reached with less t.ha.n 1.2 tnillion words using a.utonm.tically-crealed genera.lization informa.tk)n, versus T million words without genera.liza.tion.
Tiffs reduction I)y a.
tim(or off our to live in tile amount of text is accom1)lishe(I with lit;tie o)' no degradation in the quality of the tra.nsla.tions.
Adding a.
small amount of kt,owle(lge in the f'ornt o1" 425 seed pairs re(lutes the required trahling text; even further; this ca.n la.rgely be attril)uted to the merging of clusters which would otherwise have rema.ined distinct, thus increasing the level of generaliza.ties.
Adding the production-rule gratnma.r to the seeded clustering had little effect.
When usirtg more than 50,000 words of tra.ining text, the increase in coverage from adding the gram m a,r was negligible, and even with the sma.llest training corl)ora, (,he+ increase wa.s very modest.
Using the sa.me thresltolds tha.t were used in tile fully-~mtonla.tic case, clustering on 1.\] million words expands the initial 425 word pairs in 37 clusters to a200 word pairs, a.nd adds a.n additions.1 555 word pairs in \]d() further non(;t:ivia,1 clusters.
This (:Oral)ares very fa.vorably 129 cO o (D C1) CD O O 90 80 70 60 50 40 30 20 ' s i --x.... : ...... x ........... x .... i ..... -x ..........
* .... .....
~.~+", zz V3 .... i ~, (a" /, // / / i I 0.2 0.4 0.6 Corpus Size (millions of words) lexical matching only -~-with automatic clustering -4-clustering w/425 seeds -D--, full manual g~neralization --x--0.8 l i'igure 3: BI3MT \]~el'formance with and without Generalization to the 3506 word 1)airs in 221 clusters tbund without seeding.
'l'he 1)rogram also runs reasonably quickly.
The step of creating context term vectors converts approximately 500,000 words of raw text per minute on a 300 MHz processor.
1,'or agglomerative clustering, the processing time is roughly quadratic in the number of word \])airs, with a theoretical cubic worst case; the 17,527 distinct word pairs found from the million-word training corpus require about 25 minutes to cluster.
6 Discussion
One statement made earlier deserves cla.rification: l;he members of ~ cluster need not be related to each other in any way, either syntactically or semantically, for a cluster to be useful and correct.
This is because (absent a grammar) we do not care about the features of tile words in the cluster, only wh, cthc~" their tr(mslalion,s Jbllow the same pattcrT~.
An illustration based on actual experience is useful here.
In early testing of the groupaverage clustering algorithm with seeding, the <conjunction> seed class of "and" and "or" was used.
Clustering augmented this seed class with "," (comma.), "in", and %y".
One can easily see tha.t the comma is a valid member of the class, since it takes the place of "and" in lists of items.
13ut wllat about ':in" and "135;", wlfich are prepositions rather than conjunctions2 11' one considers the tra.nsbttion t)attern __ 7~ C \[ • __ Fr'eNl~ I'>cNP2 --+ EW/A 1 1 F-~:/NI):2 it becomes clear that all of the terms in the expanded class give a correct translation when placed in the blank in this pattern, lndeed, one could imagine a production-rule grammar geared toward taking advmltage of such common translation patterns regardless of conventional linguistic features.
7 Conclusion
and Future Work Using word clustering to automatically generalize the example corpus of an I;BM'I? system can provide the majority of tile improvement which can be achieved using both ~ manuallygenerated set of equivalence ('lasses and a product;ion rule grammar.
The use of a set of small initial equivalence classes produces a substantial further reduction in training text at a very low cost (a few hours) in lal)or.
130 An obvious {'~xtension to using st,.e\] clusl;ors iS (;(} 1+180 (,110 I'Osllll; ()\[' a, ClU,'ql;{':l'illg 1"I+111 ;IS l;tl? i\]lil;ia\] seed \['or a second it;ra,l;io\]t o1' chlsl,ering, sin('{', th; additional g,neralization of lo{;a.i COlll;{!xl;s cnabl(;d 1)y the la.rgcr s,e(1 clusl,(,J's will l)ormit a.(l(litional ex\])allSiOll O\['LIlo clusl,(Brs.
l:or such itera.tivo {:lustoring, a.II but the last rou n(1 shouI(1 l)l'(2Slllllal)ly USe sl;ri(;Ler 1,hreshol(Is, to avoi(1 adding goo many irr;l,A,ant inonlt)ers Lo tim clusLers.
I)rdiminary OXl)erinmnts hay B been inconclusive --although ihc result o\[' a second itwation {'onta.ins more {,{'.rms ill the {;lusl;ers, IBBMT l}erforma.nce {toes not seem to lint)rove.
More sophistica.ted {;hlsl;o.l'illg; a.lg(}rithms such as k-lneans and (l('+terlninLqtic a.nnealing l\]lay' 1)rovi(lo \])etter-qua.lity clustws for bcl, ter t)ei't"of lllall;e} :-1+,{; the (~xi)ens(; of illCl'Oas(;(\] t)ro'eHsill~ tim'..
This a.i)l}Z:oach to gelWXa.l,ing e(luival('Jw(~ cla.sses should worl( j usl; as well \['or l)h rases as I'or single words, simply hy mo(lil~qng {;he converSi()ll SLOp 1;O el'oat;(; C(}lltOXt VeCl;ors l"or phrases.
This enhancenmnt would elimi,lal;{'~ i;he current limitation t, hat trat,slal;ion \]):q,il:S l,O 1)O.
clust(;red \]\]lUSt t)O single words in 1)oth languages.
\Vot:k or, this n\]o(lifi;al;ion is {:urP(~ll|;ly ttn(ler way.
An inleresting \['ui, ur~ (;xI)eriment would 1)(~ tbr'going gratnnlar rules based {)n standa.rd gl:allllll:-/,l;ical \['{'.:-1+,l;tll'(~s Sll:h as \]).~l,rl, o\[' st){"(':{:\]l, and inst,ad crea,tinp; a gran~ma, r guid(,{I I} 3, {;1~{; ('lusters I'oun(l fully aul,o~tati'ally (wil, houl, sceliug) fronl th~ exa.nll}lc re\l,.
'File r,{:(;nt woH I)y +\(lcTait and 'l¥..iillo (I 999) {}, OXtl':dcl.,ing tra1~slal, ion t}al;l;{'+rn,q woul(l a.t)poa.r t,o 1}o.
a l){;rfe:l; {;oml)lc'nmnt, as 1;h'5 are it, e\[t'ect lindi,g {:ont;ext strings wit\], (}l}e.
slots, while the work descril)ed h('.re lit,(ls {,he fillers I'(}1' tJ~{)s(' slots.
(liv;n the al)ility to learn such +1+.
gra.mmar without l\]\]a.nual intervmtion, it would \])e(:onl'.
I)ossil)l~ to ere'at; an I!'I:~MT 8ystm\] usillg g:qlera, liz,(l e,:aml)les from nol, hi\]~g ~n)r; than l)arallel l;ext~ which for n~any hulguag(, pairs could also 1)c acquired a hnost fully a, utom~tically 1)y crawling the World Wide VVel) (Resnil, :1.998).

