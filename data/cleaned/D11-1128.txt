Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1384‚Äì1393,
Edinburgh, Scotland, UK, July 27‚Äì31, 2011. c 2011 Association for Computational Linguistics
Improved Transliteration Mining Using Graph Reinforcement 
Ali El-Kahky1, Kareem Darwish1, Ahmed Saad Aldein2, Mohamed Abd El-Wahab3, 
Ahmed Hefny2, Waleed Ammar4 
 
1 Qatar
Computing Research Institute, Qatar Foundation, Doha, Qatar 
2 Computer
Engineering Department, Cairo University, Cairo, Egypt 
3 Microsoft
Research, Microsoft, Cairo, Egypt 
4 Microsoft
Research, Microsoft, Redmond, WA, US 
{aelkahky,kdarwish}@qf.org.qa1, asaadaldien@hotmail.com2, 
ahmed.s.hefny@gmail.com2, t-momah@microsoft.com3, 
i-waamma@microsoft.com4 
  
 
 
Abstract 
Mining of transliterations from comparable 
or parallel text can enhance natural 
language processing applications such as 
machine translation and cross language 
information retrieval. This paper presents 
an enhanced transliteration mining 
technique that uses a generative graph 
reinforcement model to infer mappings 
between source and target character 
sequences. An initial set of mappings are 
learned through automatic alignment of 
transliteration pairs at character sequence 
level. Then, these mappings are modeled 
using a bipartite graph. A graph 
reinforcement algorithm is then used to 
enrich the graph by inferring additional 
mappings. During graph reinforcement, 
appropriate link reweighting is used to 
promote good mappings and to demote bad 
ones. The enhanced transliteration mining 
technique is tested in the context of mining 
transliterations from parallel Wikipedia 
titles in 4 alphabet-based languages pairs, 
namely English-Arabic, English-Russian, 
English-Hindi, and English-Tamil. The 
improvements in F1-measure over the 
baseline system were 18.7, 1.0, 4.5, and 
32.5 basis points for the four language 
pairs respectively. The results herein 
outperform the best reported results in the 
literature by 2.6, 4.8, 0.8, and 4.1 basis 
points for the four language pairs 
respectively. 
Introduction 
Transliteration Mining (TM) is the process of 
finding transliterated word pairs in parallel or 
comparable corpora. TM has many potential 
applications such as mining training data for 
transliteration, improving lexical coverage for 
machine translation, and cross language retrieval 
via translation resource expansion. TM has been 
gaining some attention lately with a shared task in 
the ACL 2010 NEWS workshop (Kumaran, et al. 
2010). 
One popular statistical TM approach is performed 
in two stages. First, a generative model is trained 
by performing automatic character level alignment 
of parallel transliterated word pairs to find 
character segment mappings between source and 
target languages. Second, given comparable or 
parallel text, the trained generative model is used 
to generate possible transliterations of a word in 
the source language while constraining the 
transliterations to words that exist in the target 
language. 
However, two problems arise in this approach: 
1. Many possible character sequence mappings 
between source and target languages may not be 
observed in training data, particularly when limited 
training data is available ‚Äì hurting recall. 
2. Conditional probability estimates of obtained 
mappings may be inaccurate, because some 
mappings and some character sequences may not 
1384
appear a sufficient number of times in training to 
properly estimate their probabilities ‚Äì hurting 
precision. 
In this paper we focus on overcoming these two 
problems to improve overall TM. To address the 
first problem, we modeled the automatically 
obtained character sequence mappings (from 
alignment) as a bipartite graph and then we 
performed graph reinforcement to enrich the graph 
and predict possible mappings that were not 
directly obtained from training data. The example 
in Figure 1 motivates graph reinforcement. In the 
example, the Arabic letter ‚ÄïŸÇ ‚Äñ (pronounced as 
‚Äïqa‚Äñ) was not aligned to the English letter ‚Äïc‚Äñ in 
training data. Such a mapping seems probable 
given that another Arabic letter, ‚ÄïŸÉ ‚Äñ (pronounced 
as ‚Äïka‚Äñ), maps to two English letters, ‚Äïq‚Äñ and ‚Äïk‚Äñ, 
to which ‚ÄïŸÇ ‚Äñ also maps. In this case, there are 
multiple paths that would lead to a mapping 
between the Arabic letter ‚ÄïŸÇ ‚Äñ and the English letter 
‚Äïc‚Äñ, namely ŸÇ  ÔÉ® q ÔÉ® ŸÉ  ÔÉ® c and ŸÇ  ÔÉ® k ÔÉ® ŸÉ  ÔÉ® 
c. By using multiple paths as sources of evidence, 
we can infer the new mapping and estimate its 
probability.   
Another method for overcoming the missing 
mappings problem entails assigning small 
smoothing probabilities to unseen mappings. 
However, from looking at the graph, it is evident 
that some mappings could be inferred and should 
be assigned probabilities that are higher than a 
small smoothing probability. 
The second problem has to do primarily with some 
characters in one language, typically vowels, 
mapping to many character sequences in the other 
language, with some of these mappings assuming 
very high probabilities (due to limited training 
data). To overcome this problem, we used link 
reweighting in graph reinforcement to scale down 
the likelihood of mappings to target character 
sequences in proportion to how many source 
sequences map to them. 
We tested the proposed method using the ACL 
2010 NEWS workshop data for English-Arabic, 
English-Russian, English-Hindi, and English-
Tamil (Kumaran et al., 2010). For each language 
pair, the standard ACL 2010 NEWS workshop data 
contained a base set of 1,000 transliteration pairs 
for training, and set of 1,000 parallel Wikipedia 
titles for testing. 
The contributions of the paper are: 
1. Employing graph reinforcement to improve the 
coverage of automatically aligned data ‚Äì as they 
apply to transliteration mining. This positively 
affects recall. 
2. Applying link reweighting to overcome 
situations where certain tokens ‚Äì character 
sequences in the case of transliteration ‚Äì tend to 
have many mappings, which are often erroneous. 
This positively affects precision. 
The rest of the paper is organized as follows: 
Section 2 surveys prior work on transliteration 
mining; Section 3 describes the baseline TM 
approach and reports on its effectiveness; Section 4 
describes the proposed graph reinforcement along 
with link reweighting and reports on the observed 
improvements; and Section 5 concludes the paper. 
 
Figure 1:  Example mappings seen in training 
 
Background 
Much work has been done on TM for different 
language pairs such as English-Chinese (Kuo et al., 
2006; Kuo et al., 2007; Kuo et al., 2008; Jin et al. 
2008;), English-Tamil (Saravanan and Kumaran, 
2008; Udupa and Khapra, 2010), English-Korean 
(Oh and Isahara, 2006; Oh and Choi, 2006), 
English-Japanese (Qu et al., 2000; Brill et al., 
2001; Oh and Isahara, 2006), English-Hindi (Fei et 
al., 2003; Mahesh and Sinha, 2009), and English-
Russian (Klementiev and Roth, 2006). 
TM typically involves two main tasks, namely: 
finding character mappings between two 
languages, and given the mappings ascertaining 
whether two words are transliterations or not. 
When training with a limited number of 
transliteration pairs, two additional problems 
appear: many possible character sequence 
mappings between source and target languages 
may not be observed in training data, and 
conditional probability estimates of obtained 
1385
mappings may be inaccurate. These two problems 
affect recall and precision respectively. 
1.1 Finding
Character Mappings 
To find character sequence mappings between two 
languages, the most common approach entails 
using automatic letter alignment of transliteration 
pairs. Akin to phrasal alignment in machine 
translation, character sequence alignment is treated 
as a word alignment problem between parallel 
sentences, where transliteration pairs are treated as 
if they are parallel sentences and the characters 
from which they are composed are treated as if 
they are words. Automatic alignment can be 
performed using different algorithms such as the 
EM algorithm (Kuo et al., 2008; Lee and Chang, 
2003) or HMM based alignment (Udupa et al., 
2009a; Udupa et al., 2009b). In this paper, we use 
automatic character alignment between 
transliteration pairs using an HMM aligner. 
Another method is to use automatic speech 
recognition confusion tables to extract phonetically 
equivalent character sequences to discover 
monolingual and cross lingual pronunciation 
variations (Kuo and Yang, 2005). Alternatively, 
letters can be mapped into a common character set 
using a predefined transliteration scheme (Oh and 
Choi, 2006). 
1.2 Transliteration
Mining 
For the problem of ascertaining if two words can 
be transliterations of each other, a common 
approach involves using a generative model that 
attempts to generate all possible transliterations of 
a source word, given the character mappings 
between two languages, and restricting the output 
to words in the target language (Fei et al., 2003; 
Lee and Chang, 2003, Udupa et al., 2009a). This is 
similar to the baseline approach that we used in 
this paper. Noeman and Madkour (2010) 
implemented this technique using a finite state 
automaton by generating all possible 
transliterations along with weighted edit distance 
and then filtered them using appropriate thresholds 
and target language words. They reported the best 
TM results between English and Arabic with F1-
measure of 0.915 on the ACL-2010 NEWS 
workshop standard TM dataset. A related 
alternative is to use back-transliteration to 
determine if one sequence could have been 
generated by successively mapping character 
sequences from one language into another (Brill et 
al., 2001; Bilac and Tanaka, 2005; Oh and Isahara, 
2006). 
Udupa and Khapra (2010) proposed a method in 
which transliteration candidates are mapped into a 
‚Äïlow-dimensional common representation space‚Äñ. 
Then, similarity between the resultant feature 
vectors for both candidates can be computed. 
Udupa and Kumar (2010) suggested that mapping 
to a common space can be performed using context 
sensitive hashing. They applied their technique to 
find variant spellings of names. 
Jiampojamarn et al. (2010) used classification to 
determine if a source language word and target 
language word are valid transliterations. They used 
a variety of features including edit distance 
between an English token and the Romanized 
versions of the foreign token, forward and 
backward transliteration probabilities, and 
character n-gram similarity. They reported the best 
results for Russian, Tamil, and Hindi with F1-
measure of 0.875, 0.924, and 0.914 respectively on 
the ACL-2010 NEWS workshop standard TM 
datasets. 
1.3 Training
with Limited Training Data 
When only limited training data is available to 
train a character mapping model, the resultant 
mappings are typically incomplete (due to 
sparseness in the training data). Further, resultant 
mappings may not be observed a sufficient of 
times and hence their mapping probabilities may 
be inaccurate. 
Different methods were proposed to solve these 
two problems. These methods focused on making 
training data less sparse by performing some kind 
of letter conflation. Oh and Choi (2006) used a 
SOUNDEX like scheme. SOUNDEX is used to 
convert English words into a simplified phonetic 
representation, in which vowels are removed and 
phonetically similar characters are conflated. A 
variant of SOUNDEX along with iterative training 
was proposed by Darwish (2010). Darwish (2010) 
reported significant improvements in TM recall at 
the cost of limited drop in precision. Another 
method involved expanding character sequence 
maps by automatically mining transliteration pairs 
and then aligning these pairs to generate an 
expanded set of character sequence maps (Fei et 
al., 2003). In this work we proposed graph 
1386
reinforcement with link reweighting to address this 
problem. Graph reinforcement was used in the 
context of different problems such as mining 
paraphrases (Zhao et al., 2008; Kok and Brockett, 
2010; Bannard and  Callison-Burch 2005) and 
named entity translation extraction (You et al., 
2010). 
Baseline Transliteration Mining 
1.4 Description
of Baseline System 
The basic TM setup that we employed in this 
work used a generative transliteration model, 
which was trained on a set of transliteration pairs. 
The training involved automatically aligning 
character sequences. The alignment was performed 
using a Bayesian learner that was trained on word 
dependent transition models for HMM based word 
alignment (He, 2007). Alignment produced 
mappings of source character sequences to target 
character sequences along with the probability of 
source given target and vice versa. Source 
character sequences were restricted to be 1 to 3 
characters long. 
For all the work reported herein, given an 
English-foreign language transliteration candidate 
pair, English was treated as the target language and 
the foreign language as the source.  Given a 
foreign source language word sequence     and an 
English target word sequence    ,         could 
be a potential transliteration of       .  An 
example of word sequences pair is the Tamil-
English pair:  (‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Ææ‡ÆÆ‡Øç ‡Æπ‡Øà‡Æ≤‡Æø ‡Æö‡ØÜ‡Æ≤‡Ææ‡ØÜ‡Æø, 
Haile Selassie I of Ethiopia), where ‚Äï‡ÆÆ‡ØÅ‡Æ§‡Æ≤‡Ææ‡ÆÆ‡Øç‚Äù  
could be transliteration for any or none of the 
English words {‚ÄïHaile‚Äñ, ‚ÄïSelassie‚Äñ, ‚ÄïI‚Äñ, ‚Äïof‚Äñ, 
‚ÄïEthiopia‚Äñ}.  The pseudo code below describes 
how transliteration mining generates candidates. 
Basically, given a source language word, all 
possible segmentations, where each segment has a 
maximum length of 3 characters, are produced 
along with their associated mappings into the 
target language. Given all mapping combinations, 
combinations producing valid target words are 
retained and sorted according to the product of 
their mapping probabilities. If the product of the 
mapping probabilities for the top combination is 
above a certain threshold, then it is chosen as the 
transliteration candidate. Otherwise, no candidate 
is chosen. To illustrate how TM works, consider 
the following example: Given the Arabic word 
‚ÄïŸÜŸÖ ‚Äñ, all possible segmentations are (ŸÜ ÿå ŸÖ ) and (ŸÜŸÖ ). 
Given the target words {the, best, man} and the 
possible mappings for the segments and their 
probabilities: 
ŸÖŸÄ  = {(m, 0.7), (me, 0.25), (ma, 0.05)} 
ŸÜ  = {n, 0.7), (nu, 0.2), (an, 0.1)} 
ŸÜŸÖ  = {(men, 0.4), (man, 0.3), (mn, 0.3)} 
The only combinations leading valid target 
words would be: 
(ŸÜŸÖ ) ÔÉ® {(man: 0.3)} 
( ŸÖŸÄ ŸÜ ÿå ) ÔÉ® {(m,an: 0.07), (ma, n: 0.035)} 
Consequently, the algorithm would produce the 
tuple with the highest probability: (ŸÜŸÖ  , man, 0.3). 
As the pseudo code suggests, the actual 
implementation is optimized via: incremental left 
to right processing of source words; the use of a 
Patricia trie to prune mapping combinations that 
don‚Äôt lead to valid words; and the use of a priority 
queue to insure that the best candidate is always at 
the top of the queue. 
1.5 Smoothing
and Thresholding  
We implemented the baseline system with and 
without assigning small smoothing probabilities 
for unseen source character to target character 
mappings. Subsequent to training, the smoothing 
probability was selected as the smallest observed 
mapping probability in training.   
We used a threshold on the minimum acceptable 
transliteration score to filter out unreliable 
transliterations. We couldn‚Äôt fix a minimum score 
for reliable transliterations to a uniform value for 
all words, because this would have caused the 
model to filter out long transliterations. Thus, we 
tied the threshold to the length of transliterated 
words. We assumed a threshold d for single 
character mappings and the transliteration 
threshold for a target word of length l was 
computed as    . We selected d by sorting the 
mapping probabilities, removing the lowest 10% of 
mapping probabilities (which we assumed to be 
outliers), and then selecting the smallest observed 
probability to be the character threshold d. The 
choice of removing the lowest ranking 10% of 
mapping probabilities was based on intuition, 
because we did not have a validation set. The 
threshold was then applied to filter out 
transliteration with    ùëö          ùëö         . 
1387
1.6 Effectiveness
of Baseline System 
To test the effectiveness of the baseline system, we 
used the standard TM training and test datasets 
from the ACL-2010 NEWS workshop shared task. 
The datasets are henceforth collectively referred to 
as the NEWS dataset. The dataset included 4 
alphabet-based language pairs, namely English-
Arabic, English-Russian, English-Hindi, and 
English-Tamil. For each pair, a dataset included a 
list of 1,000 parallel transliteration word pairs to 
train a transliteration model, and a list of 1,000 
parallel word sequences to test TM. The parallel 
sequences in the test sets were extracted titles from 
Wikipedia article for which cross language links 
exist between both languages. 
We preprocessed the different languages as 
follows: 
ÔÇ∑ Russian: characters were case-folded 
ÔÇ∑ Arabic: the different forms of alef (alef, alef 
maad, alef with hamza on top, and alef with 
hamza below it) were normalized to alef, ya 
and alef maqsoura were normalized to ya, and 
ta marbouta was mapped to ha. 
ÔÇ∑ English: letters were case-folded and the 
following letter conflations were performed: 
≈æ, ≈º ÔÉ† z  √°, √†, √¢, √°, √§, ƒÅ, ƒÖ, √¶ ÔÉ† a 
√ß, ƒô, √© ÔÉ† e  ƒá, ƒç, √ß ÔÉ†c 
≈Ç ÔÉ†l  √Ø, √≠, √¨, √Æ ÔÉ† i 
√±, ≈ç, √∂, √∂ ÔÉ† o ≈Ñ, √±, ·πÖ  ÔÉ† n 
≈ü, ≈õ, √ü, ≈° ÔÉ† s ≈ô ÔÉ† r 
√Ω ÔÉ† y  ≈´, √º, √µ, √ª ÔÉ† u 
ÔÇ∑ Tamil and Hindi: no preprocessing was 
performed.  
 
English/ P R F 
Arabic 0.988 0.983 0.583 0.603 0.733 0.748 
Russian 0.975 0.967 0.831 0.862 0.897 0.912 
Hindi 0.986 0.981 0.693 0.796 0.814 0.879 
Tamil 0.984 0.981 0.274 0.460 0.429 0.626 
 
Table 1:  Baseline results for all language pairs.  
Results with smoothing are shaded. 
 
Table 1 reports the precision, recall, and F1-
measure results for using the baseline system in 
TM between English and each of the 4 other 
languages in the NEWS dataset with and without 
smoothing.  As is apparent in the results, without 
smoothing, precision is consistently high for all 
languages, but recall is generally poor, particularly 
for Tamil. When smoothing is applied, we 
observed a slight drop in precision for Arabic, 
Hindi, and Tamil and a significant drop of 5.6 
1:  I n p u t :  M a p p i ng s ,  s e t  of  s our ce  giv e n t a rg e t  m a p p i ngs wit h  a s s oci a t e d  P r ob .  
2:  I n p u t :  Sou r c e Wo r d  (  
 
  
1
ùëö
) ,  Sou rce  l a ngu a ge  wor d  
3:  I n p u t :  Ta r ge tWor d s ,  P a t rici a  t r ie  con t a i ni ng  a l l  t a rge t  l a ngu a ge  word s  (  
1
 
)  
4:  Dat a S t r u ct u r es :  D F S ,  P rio rit y  qu e u e  t o s t ore  ca nd i d a t e  t ra nsl itera t io ns p a ir or d e re d  by  t h e ir t ra nsl itera t io n 
s core  ‚Äì  Ea ch  ca n d id a t e  t ra nsl ite ra t io n t u p l e  =  ( S our c e F r a gme nt , Ta r ge tTr a nslite r a tio n , Tr a nslite r a tio nSc or e ).  
5:  Sta r tSy mbol =  (‚Äú‚Äù, ‚Äú‚Äù ,  1 . 0 )  
6:  D F S ={ Sta r tSy mbol }  
7:  Whi le ( D F S i s  not e m p ty )  
8:   Sou r c e F r a gme nt=  D F S . T op ( ) . S our ce F ra gm e nt  
9:   Ta r ge tF r a gme nt=  D F S . T op ( ) . T a rge t T ra nsl itera t io n  
10:   F r a gme ntP r ob= D F S . T op ( ) . T ra ns l itera t io nScore  
11:   I f  ( Sou r c e Wo r d  ==  Sou r c e F r a gme nt  )  
12:    If ( F r a gme ntSc or e  >  Thr e s h old )  
13:     R etur n  ( Sou r c e Wo r d , Ta r ge tTr a nslite r a tio n , Tr a nslite r a tio nS c o r e )  
14:    E ls e  
15:     R etur n  N ull  
16:   D F S . R e m ov e T op ( )  
17:   F o r  Sub F r a gme ntL e ng th = 1  t o  3  
18:    Sou r c e Sub Str i ng =  Su bS t rin g(  So ur c e Wo r d , Sou r c e F r a gme nt . L e n gt h  ,  Sub F r a gme ntL e ng th )  
19:    F o r each  ma p p i ng  in M a p p i ng s [ Sou r c e Sub Str i ng ]   
20:     If ( ( Ta r ge tF r a gme nt +  ma p p i ng )   is  a  s u b s t rin g  in Ta r ge tWor d s )  
21:      D F S . A d d ( Sou r c e F r a gme nt +  Sou r c e Sub Str i ng , M a p p i ng .Sc or e  * F r a gme ntSc or e )  
22 :   D F S . R e m ov e ( Sou rce F ra gm e nt)  
2 3 :  E n d  Whi le  
2 4 :  R etur n  N ull  
Figure 2:  Pseudo code for transliteration mining 
 
1388
basis points for Russian. However, the application 
of smoothing increased recall dramatically for all 
languages, particularly Tamil. For the remainder of 
the paper, the results with smoothing are used as 
the baseline results. 
Background 
1.7 Description
of Graph Reinforcement 
In graph reinforcement, the mappings deduced 
from the alignment process were represented using 
a bipartite graph G = (S, T, M), where S was the 
set of source language character sequences, T was 
the set of target language character sequences, and 
M was the set of mappings (links or edges) 
between S and T. The score of each mapping 
m(v1|v2), where m(v1|v2) ÔÉé M, was initially set to 
the conditional probability of target given source 
p(v1|v2). Graph reinforcement was performed by 
traversing the graph from S ÔÉ® T ÔÉ® S ÔÉ® T in 
order to deduce new mappings. Given a source 
sequence s' ÔÉé S and a target sequence t' ÔÉéÔÄ†T, the 
deduced mapping probabilities were computed as 
follows (Eq.1):  
 (   |   )    ‚àè (    (   |  )  (  |  )  (  |   ) )
        
 
where the term (    (   |  )  (  |  )  (  |   ) )  
computed the probability that a mapping is not 
correct. Hence, the probability of an inferred 
mapping would be boosted if it was obtained from 
multiple paths. Given the example in Figure 1, 
m(c|ŸÇ ) would be computed as follows:  
  (    (  | ŸÉ )  ( ŸÉ |  )  (  | ŸÇ ) )   
(    (  | ŸÉ )  ( ŸÉ |  )  (  | ŸÇ ) )  
We were able to apply reinforcement iteratively on 
all mappings from S to T to deduce previously 
unseen mappings (graph edges) and to update the 
probabilities of existing mappings. 
1.8 Link
Reweighting  
The basic graph reinforcement algorithm is prone 
to producing irrelevant mappings by using 
character sequences with many different possible 
mappings as a bridge. Vowels were the most 
obvious examples of such character sequences. For 
example, automatic alignment produced 26 Hindi 
character sequences that map to the English letter 
‚Äïa‚Äñ, most of which were erroneous such as the 
mapping between ‚Äïa‚Äñ and ‚Äï‡§µ‚Äù  (pronounced va). 
Graph reinforcement resulted in many more such 
mappings. After successive iterations, such 
character sequences would cause the graph to be 
fully connected and eventually the link weights 
will tend to be uniform in their values. To illustrate 
this effect, we experimented with basic graph 
reinforcement on the NEWS dataset. The figures of 
merit were precision, recall, and F1-measure. 
Figures 3, 4, 5, and 6 show reinforcement results 
for Arabic, Russian, Hindi, and Tamil respectively. 
The figures show that: recall increased quickly and 
nearly saturated after several iterations; precision 
continued to drop with more iterations; and F1-
measure peaked after a few iterations and began to 
drop afterwards. This behavior was undesirable 
because overall F1-measure values did not 
converge with iterations, necessitating the need to 
find clear stopping conditions. 
To avoid this effect and to improve precision, we 
applied link reweighting after each iteration. Link 
reweighting had the effect of decreasing the 
weights of target character sequences that have 
many source character sequences mapping to them 
and hence reducing the effect of incorrectly 
inducing mappings. Link reweighting was 
performed as follows (Eq. 2): 
    (  |  )   
 (  |  )
‚àë  (   |  )     
Where si ÔÉé S is a source character sequence that 
maps to t. So in the case of ‚Äïa‚Äñ mapping to the ‚Äï‡§µ‚Äñ 
character in Hindi, the link weight from ‚Äïa‚Äñ to ‚Äï‡§µ‚Äñ 
is divided by the sum of link weights from ‚Äïa‚Äñ to 
all 26 characters to which ‚Äïa‚Äñ maps. 
We performed multiple experiments on the NEWS 
dataset to test the effect of graph reinforcement 
with link reweighting with varying number of 
reinforcement iterations. Figures 7, 8, 9, and 10 
compare baseline results with smoothing to results 
with graph reinforcement at different iterations. 
As can be seen in the figures, the F1-measure 
values stabilized as we performed multiple graph 
reinforcement iterations. Except for Russian, the 
results across different languages behaved in a 
similar manner. 
For Russian, graph reinforcement marginally 
affected TM F1-measure, as precision and recall 
1389
marginally changed. The net improvement was 1.1 
basis points. English and Russian do not share the 
same alphabet, and the number of initial mappings 
was bigger compared to the other language pairs.  
Careful inspection of the English-Russian test set, 
with the help of a Russian speaker, suggests that:  
1) the test set reference contained many false 
negatives;  
2) Russian names often have multiple phonetic 
forms (or spellings) in Russian with a single 
standard transliteration in English. For example, 
the Russian name ‚ÄïOlga‚Äñ is often written and 
pronounced as ‚ÄïOla‚Äñ and ‚ÄïOlga‚Äñ in Russian; and  
3) certain English phones do not exist in Russian, 
leading to inconsistent character mappings in 
Russian.  For example, the English phone for ‚Äïg‚Äñ, 
as in ‚ÄïGeorge‚Äñ, does not exist in Russian. 
 
For the other languages, graph reinforcement 
yielded steadily improving recall and consequently 
steadily improving F1-measure. Most 
improvements were achieved within the first 5 
iterations, and improvements beyond 10 iterations 
were generally small (less than 0.5 basis points in 
F1-measure). After 15 iterations, the improvements 
in overall F1-measure above the baseline with 
smoothing were 19.3, 5.3, and 32.8 basis points for 
Arabic, Tamil, and Hindi respectively. The F1-
measure values seemed to stabilize with successive 
iterations. The least improvements were observed 
for Hindi. This could be attributed to the fact that 
Hindi spelling is largely phonetic, making letters in 
words pronounceable in only one way. This fact 
makes transliteration between Hindi and English 
easier than Arabic and Tamil. In the case of Tamil, 
the phonetics of letters change depending on the 
position of letters in words. As for Arabic, multiple 
letters sequences in English can map to single 
letters in Arabic and vice versa. Also, Arabic has 
diacritics which are typically omitted, but 
commonly transliterate to English vowels. Thus, 
the greater the difference in phonetics between two 
languages and the greater the phonetic complexity 
of either, the more TM can gain from the proposed 
technique. 
1.9 When
Graph Reinforcement Worked  
An example where reinforcement worked entails 
the English-Arabic transliteration pair (Seljuq, 
ŸáŸÇÿ¨ŸÑÿßÿ≥ ). In the baseline runs with 1,000 training 
examples, both were not mapped to each other 
because there were no mappings between the letter 
‚Äïq‚Äñ and the Arabic letter sequence ‚ÄïŸáŸÇ ‚Äñ 
(pronounced as ‚Äïqah‚Äñ). The only mappings that 
were available for ‚Äïq‚Äñ were ‚ÄïŸáŸÉ ‚Äñ (pronounced as 
‚Äïkah‚Äñ), ‚ÄïŸÇ ‚Äñ (pronounced as ‚Äïq‚Äñ), and ‚ÄïŸÉ ‚Äñ 
(pronounced as ‚Äïk‚Äñ) with probabilities 54.0, 0.10, 
and 5452 respectively. Intuitively, the third 
mapping is more likely than the second. After 3 
graph reinforcement iterations, the top 5 mappings 
for ‚Äïq‚Äñ were ‚ÄïŸÇ ‚Äñ (pronounced as ‚Äïq‚Äñ), ‚ÄïŸáŸÇ ‚Äñ 
(pronounced as ‚Äïqah‚Äñ), ‚ÄïŸáŸÉ ‚Äñ (pronounced as 
‚Äïkah‚Äñ), ‚ÄïŸÉ ‚Äñ (pronounced as ‚Äïk‚Äñ), and ‚ÄïŸÇŸÑÿß ‚Äñ 
(pronounced as ‚Äïalq‚Äñ) with mapping probabilities 
of 0.22, 0.19, 0.15, 0.05, and 0.05 respectively. In 
this case, graph reinforcement was able to find the 
missing mapping and properly reorder the 
mappings.  Performing 10 iterations with link 
reweighting for Arabic led to 17 false positives. 
Upon examining them, we found that: 9 were 
actually correct, but erroneously labeled as false in 
the test set; 6 were phonetically similar like ‚ÄïÿßŸäŸÜÿßÿ®ÿ≥ÿß ‚Äñ 
(pronounced espanya) and ‚ÄïSpain‚Äñ and ‚ÄïÿßŸäÿ¨ŸàŸÑŸàŸÜŸÉÿ™ŸÑÿß ‚Äñ 
(pronounced alteknologya) and ‚Äïtechnology‚Äñ; and 
the remaining 2 were actually wrong, which were 
‚ÄïŸäÿ¥ÿ™Ÿäÿ® ‚Äñ (pronounced beatchi) and ‚Äïmedici‚Äñ and 
‚Äï Ÿäÿ≥ŸäÿØ ‚Äñ (pronounced sidi) and ‚Äïtaya‚Äñ. This seems to 
indicate that graph reinforcement generally 
introduced more proper mappings than improper 
ones. 
1.10 Comparing to the State-of-the-Art  
Table 2 compares the best reported results in ACL-
2010 NEWS TM shared task for Arabic (Noeman 
and Madkour, 2010) and for the other languages 
(Jiampojamarn et al. 2010) and the results obtained 
by the proposed technique using 10 iterations, with 
link reweighting. The comparison shows that the 
proposed algorithm yielded better results than the 
best reported results in the literature by 2.6, 4.8, 
0.8 and 4.1 F1-measure points in Arabic, Russian, 
Hindi and Tamil respectively. For Arabic, the 
improvement over the previously reported result 
was due to improvement in precision, while for the 
other languages the improvements were due to 
improvements in both recall and precision. 
 
 
1390
 
 
Fig u r e 3:  Gr ap h  r ein f o r ce m en t  w /o  li n k  r e w ei g h ti n g  
f o r  A r ab ic  
 
Fig u r e 4 :  Gr ap h  r ein f o r ce m en t  w /o  li n k  r e w ei g h ti n g  
f o r  R u s s ia n  
 
Fig u r e 5 :  Gr ap h  r ein f o r ce m en t  w /o  li n k  r e w ei g h ti n g  
f o r  Hin d i  
 
Fig u r e 6 :  Gr ap h  r ein f o r ce m en t  w /o  li n k  r e w ei g h ti n g  
f o r  T am il  
 
Fig u r e 7:   Gr ap h  r ein f o r ce m en t r esu lt s  f o r  A r ab ic  
 
Fig u r e 8:  Gr ap h  r ein f o r ce m en t  r esu lts  f o r  R u s s ian  
 
Fig u r e 9:   Gr ap h  r ein f o r ce m en t r esu lt s  f o r  Hin d i  
 
Fig u r e 10:   Gr ap h  r ein f o r ce m e n t r es u lts  f o r  T am il  
 
0.4 00
0.5 00
0.6 00
0.7 00
0.8 00
0.9 00
1.0 00
ba
se
l
i
ne
1 2 3 4 5 6 7 8 9
10
Iter at i ons
F
R
P
0.4 00
0.5 00
0.6 00
0.7 00
0.8 00
0.9 00
1.0 00
ba
se
l
i
ne
1 2 3 4 5 6 7 8 9
10
Iter at i ons
F
R
P
0.4 00
0.5 00
0.6 00
0.7 00
0.8 00
0.9 00
1.0 00
ba
se
l
i
ne
1 2 3 4 5 6 7 8 9
10
Iter at i ons
F
R
P
0.4 00
0.5 00
0.6 00
0.7 00
0.8 00
0.9 00
1.0 00
ba
se
l
i
ne
1 2 3 4 5 6 7 8 9
10
Iter at i ons
F
R
P
0.8 2
0.8 4
0.8 6
0.8 8
0.9 0
0.9 2
0.9 4
0.9 6
0.9 8
1.0 0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Num be r  of  It e r at i ons
F
R
P
ba se l i ne
F  =  0.748
R  =  0.60 3
P  =  0.983
0.8 2
0.8 4
0.8 6
0.8 8
0.9 0
0.9 2
0.9 4
0.9 6
0.9 8
1.0 0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Num be r  of  It e r at i ons
F
R
P
ba se l i ne
F  =  0.912
R  =  0.86 2
P  =  0.967
0.8 2
0.8 4
0.8 6
0.8 8
0.9 0
0.9 2
0.9 4
0.9 6
0.9 8
1.0 0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Num be r  of  It e r at i ons
F
R
P
ba se l i ne
F  =  0.879
R  =  0.79 6
P  =  0.981
0.8 2
0.8 4
0.8 6
0.8 8
0.9 0
0.9 2
0.9 4
0.9 6
0.9 8
1.0 0
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Num be r  of  It e r at i ons
F
R
P
ba se l i ne
F  =  0.626
R  =  0.46 0
P  =  0.981
1391
 Shared Task Proposed Algorithm 
English/ P R F P R F 
Arabic 0.887 0.945 0.915 0.979 0.905 0.941 
Russian 0.880 0.869 0.875 0.921 0.925 0.923 
Hindi 0.954 0.895 0.924 0.972 0.895 0.932 
Tamil 0.923 0.906 0.914 0.964 0.945 0.955 
Table 2: Best results obtained in ACL-2010 NEWS TM 
shared task compared to graph reinforcement with link 
reweighting after 10 iterations 
Conclusion 
In this paper, we presented a graph reinforcement 
algorithm with link reweighting to improve 
transliteration mining recall and precision by 
systematically inferring mappings that were unseen 
in training. We used the improved technique to 
extract transliteration pairs from parallel Wikipedia 
titles. The proposed technique solves two problems 
in transliteration mining, namely: some mappings 
may not be seen in training data ‚Äì hurting recall, 
and certain mappings may not be seen a sufficient 
number of times to appropriate estimate mapping 
probabilities ‚Äì hurting precision. The results 
showed that graph reinforcement yielded improved 
transliteration mining from parallel Wikipedia 
titles for all four languages on which the technique 
was tested. 
Generally iterative graph reinforcement was able to 
induce unseen mappings in training data ‚Äì 
improving recall. Link reweighting favored 
precision over recall counterbalancing the effect of 
graph reinforcement. The proposed system 
outperformed the best reported results in the 
literature for the ACL-2010 NEWS workshop 
shared task for Arabic, Russian, Hindi and Tamil.  
To extend the work, we would like to try 
transliteration mining from large comparable texts. 
The test parts of the NEWS dataset only contained 
short parallel fragments. For future work, graph 
reinforcement could be extended to MT to improve 
the coverage of aligned phrase tables. In doing so, 
it is reasonable to assume that there are multiple 
ways of expressing a singular concept and hence 
multiple translations are possible. Using graph 
reinforcement can help discover such translation 
though they may never be seen in training data. 
Using link reweighting in graph reinforcement can 
help demote unlikely translations while promoting 
likely ones. This could help clean MT phrase 
tables. Further, when dealing with transliteration, 
graph reinforcement can help find phonetic 
variations within a single language, which can 
have interesting applications in spelling correction 
and information retrieval. Applying the same to 
machine translation phrase tables can help identify 
paraphrases automatically. 
References  
Colin Bannard, Chris Callison-Burch. 2005. 
Paraphrasing with Bilingual Parallel Corpora. ACL-
2005, pages 597‚Äî604. 
Slaven Bilac, Hozumi Tanaka. 2005. Extracting 
transliteration pairs from comparable corpora. NLP-
2005. 
Eric Brill, Gary Kacmarcik, Chris Brockett. 2001. 
Automatically harvesting Katakana-English term 
pairs from search engine query logs. NLPRS 2001, 
pages 393‚Äì399. 
Kareem Darwish. 2010. Transliteration Mining with 
Phonetic Conflation and Iterative Training. ACL 
NEWS workshop 2010. 
Huang Fei, Stephan Vogel, and Alex Waibel. 2003. 
Extracting Named Entity Translingual Equivalence 
with Limited Resources. TALIP, 2(2):124‚Äì129. 
Xiaodong He. 2007. Using Word-Dependent Transition 
Models in HMM based Word Alignment for 
Statistical Machine Translation. ACL-07 2nd SMT 
workshop. 
Sittichai Jiampojamarn, Kenneth Dwyer, Shane 
Bergsma, Aditya Bhargava, Qing Dou, Mi-Young 
Kim and Grzegorz Kondrak. 2010. Transliteration 
Generation and Mining with Limited Training 
Resources. ACL NEWS workshop 2010. 
Chengguo Jin, Dong-Il Kim, Seung-Hoon Na, Jong-
Hyeok Lee. 2008. Automatic Extraction of English-
Chinese Transliteration Pairs using Dynamic 
Window and Tokenizer. Sixth SIGHAN Workshop 
on Chinese Language Processing, 2008. 
Alexandre Klementiev and Dan Roth. 2006. Named 
Entity Transliteration and Discovery from 
Multilingual Comparable Corpora. HLT Conf. of the 
North American Chapter of the ACL, pages 82‚Äì88. 
Stanley Kok, Chris Brockett.. 2010. Hitting the Right 
Paraphrases in Good Time. Human Language 
Technologies: The 2010 Annual Conference of the 
North American Chapter of the ACL, June 2010 
A. Kumaran, Mitesh M. Khapra, Haizhou Li. 2010. 
Report of NEWS 2010 Transliteration Mining Shared 
Task. Proceedings of the 2010 Named Entities 
1392
Workshop, ACL 2010, pages 21‚Äì28, Uppsala, 
Sweden, 16 July 2010. 
Jin-Shea Kuo, Haizhou Li, Ying-Kuei Yang. 2006. 
Learning Transliteration Lexicons from the Web. 
COLING-ACL2006, Sydney, Australia, 1129 ‚Äì 1136. 
Jin-shea Kuo, Haizhou Li, Ying-kuei Yang. 2007. A 
phonetic similarity model for automatic extraction of 
transliteration pairs. TALIP, 2007 
Jin-Shea Kuo, Haizhou Li, Chih-Lung Lin. 2008. 
Mining Transliterations from Web Query Results: An 
Incremental Approach. Sixth SIGHAN Workshop on 
Chinese Language Processing, 2008. 
Jin-shea Kuo, Ying-kuei Yang. 2005. Incorporating 
Pronunciation Variation into Extraction of 
Transliterated-term Pairs from Web Corpora. Journal 
of Chinese Language and Computing, 15 (1): (33-
44). 
Chun-Jen Lee, Jason S. Chang. 2003. Acquisition of 
English-Chinese transliterated word pairs from 
parallel-aligned texts using a statistical machine 
transliteration model. Workshop on Building and 
Using Parallel Texts, HLT-NAACL-2003, 2003. 
Sara Noeman and Amgad Madkour. 2010. Language 
Independent Transliteration Mining System Using 
Finite State Automata Framework. ACL NEWS 
workshop 2010. 
R. Mahesh, K. Sinha. 2009. Automated Mining Of 
Names Using Parallel Hindi-English Corpus. 7th 
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, pages 48‚Äì54, 2009. 
Jong-Hoon Oh, Key-Sun Choi. 2006. Recognizing 
transliteration equivalents for enriching domain 
specific thesauri. 3rd Intl. WordNet Conf. (GWC-
06), pages 231‚Äì237, 2006. 
Jong-Hoon Oh, Hitoshi Isahara. 2006. Mining the Web 
for Transliteration Lexicons: Joint-Validation 
Approach. pp.254-261, 2006 IEEE/WIC/ACM Intl. 
Conf. on Web Intelligence (WI'06), 2006. 
Yan Qu, Gregory Grefenstette, David A. Evans. 2003. 
Automatic transliteration for Japanese-to-English text 
retrieval. SIGIR 2003:353-360 
Robert Russell. 1918. Specifications of Letters. US 
patent number 1,261,167. 
K Saravanan, A Kumaran. 2008. Some Experiments in 
Mining Named Entity Transliteration Pairs from 
Comparable Corpora. The 2nd Intl. Workshop on 
Cross Lingual Information Access: Addressing the 
Need of Multilingual Societies, 2008. 
Raghavendra Udupa, K. Saravanan, Anton Bakalov, 
Abhijit Bhole. 2009a. "They Are Out There, If You 
Know Where to Look": Mining Transliterations of 
OOV Query Terms for Cross-Language Information 
Retrieval. ECIR-2009, Toulouse, France, 2009. 
Raghavendra Udupa, K. Saravanan, A. Kumaran, and 
Jagadeesh Jagarlamudi. 2009b. MINT: A Method for 
Effective and Scalable Mining of Named Entity 
Transliterations from Large Comparable Corpora. 
EACL 2009. 
Raghavendra Udupa and Mitesh Khapra. 2010a. 
Transliteration Equivalence using Canonical 
Correlation Analysis. ECIR-2010, 2010. 
Raghavendra Udupa, Shaishav Kumar. 2010b. Hashing-
based Approaches to Spelling Correction of Personal 
Names. EMNLP 2010. 
Gae-won You, Seung-won Hwang, Young-In Song, 
Long Jiang, Zaiqing Nie. 2010. Mining Name 
Translations from Entity Graph Mapping. 
Proceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
430‚Äì439. 
Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li. 2008. 
Pivot Approach for Extracting Paraphrase Patterns 
from Bilingual Corpora. Proceedings of ACL-08: 
HLT, pages 780‚Äì788. 
1393

