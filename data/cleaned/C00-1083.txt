Taking Account of the User's View in 3D Multimodal Instruction Dialogue Yukiko I.
Nakano and Kenji hnamura and Hisashi Ohara 1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan {yukiko, ilnamura, ohara}@ntl;nly.isl.ntt.co.jp Abstract While recent advancements in virtual reality technology have created a rich communication interface linking humans and computers, there has beefl little work on building dialogue systems for 3D virtual worlds.
This paper proposes a method for altering the instruction dialogue to match the user's view in a virtual enviromnent.
-\~re illustrate the method with the system MID-aD, which interactively instructs the user on dismantling some parts of a car.
First, in order to change the content of ~he instruction dialogue to match the user's view, we extend the refinement-driven plmming algorithm by using the user's view as a l)lan constraint.
Second, to manage the dialogue smoothly, the systeln keeps track of the user's viewpoint as part of the dialogue skate and uses this information for coping with interruptive sul)dialogues.
These mechanisms enable MID-3D to set instruction dialogues in an incremental way; it takes account of the user's view even when it changes frequently.
1 Introduction
In a aD virtual enviromnent, we can freely walk through the virtual space and view three dimensional objects from various angles.
A inultilnodal dialogue system for such a virtual environment should ainl to realize conversations which are performed in the real world.
It would also be very useflll for education, where it is necessary to learn in near real-life situations.
One of the most significant characteristics of 3D virtual environments is that the user can select her/his own view from whidi to observe the virtual world.
Thus, the nmltimodal instruction dialogue system should be able to set the course of the dialogue by considering the user's current view.
However, previous works on nmltilnodal presentation generation and instruction dialogue generation (Wahlster et al., 1993; Moore, 1995; Cawsey, 1992) do not achieve this goal because they were not designed to hail(lie dialogues pertbrmed in 3D virtual environments.
This paper proposes a method that ensures that the course of the dialogue matches the user's view in the virtual environment.
More specificall> we focus on (1) how to select the contents of the dialogue since it is essential that the instruction dialogue system form a sequence of dialogue contents that is coherent and comprehensible, and (2) how to control mixed-initiative instruction dialogues snloothly, especially how to manage interruptive subdialogues.
These two problelns basically determine the course of the dialogue.
First, in order to decide the appropriate content, we propose a content selection mechanism based on plan-based multilnodal presentation generation (Andrd and Rist, 1993; Wahlster et al., 1993).
We extend this algorithm by using the user's view as a constraint in expanding the plan.
In addition, by employing tilt incremental planning algorithm, the syst;em can adjust the content to match the user's view during ongoing conversations.
Second, ill order to nlanage interruptive subdialogues, we propose a dialogue management mechanism that takes account of the user's view.
This mechanism maintains the user's viewpoint as a dialogue state in addition to intentional and linguistic context (Rich and Sidher, 1998).
It maintains the dialogue state as a focus stack of discourse segments and updates it at each turn.
Tlms, it Call track the viewpoint information in an on-going dialogue.
By using this viewpoint inibrlnation in restarting the dialogue after an interruptive subdialogue, the dialogue Inai~agement medmnism returns the user's viewpoint to that of the interrupted segment.
These two mechanisms work as a core dialogue engine in MID-3D (Multimodal Instruction Dialogue system for 3D virtual environments).
They make it possible to set the instruction dialogue in an increnlental ww while 572 Figure 1: Right angle Figure 2: l,efl; angle considering the user's view.
They also (mal)h'~ MID-a1) to (:re~te coherent and mixe, d-initiative (liah)gues in virtual enviromuents.
This paper is organized as lbllows.
In Secti(m 2, we define the 1)rol)h;ms spc(:ifi(: 1;o 313 multimoda\] (tiah)gne genera.tion.
Section 3 describes rclat;ed works.
\]n S('x:l;ion 4, we propose the MID-a1) architecture.
Sections 5 ;rod 6 des(:ril)e the contenl; plmming meclm.nism a.nd the dialogue manngement meclm.nism, a.nd show they dynami(:ally decide coherent insl;rn(:t;ions, and control mixed-initial;ire diah)guc.s considering the user's view.
V~/e also show a smnt)le di:dogue in Section 7.
2 Problems
In a virtual emdromnent, the user can freely move a.round the world and select her/his own view.
r\['he systelll C&llllOt; predict where the user will stand and what; s/he observes in the virtual environment.
This section describes two types of 1)roblems in generating instru(:tion dialogues ibr such virtual enviromnents.
They arc caused l)y mismatches b(~,twe(;ll tile llSel'~S vi0,wl)oint ;m(1 the sta.te of th(; dialogue.
First, the syStelll shouM check whether the user's view matches the tbcns of the next exchange when the systen~ tries to ('hange COllllllllni('ative goals.
\]if a mismatch occurs, the system shouhl choose the instru(:tion (li~dogue content according to the user's view.
Figure 1 a,n(1 2 m:e examl)les of observing a car's front suspension from (liff(',r(mt, points of view.
In Figm'(', 1, the right; side of the steering system can 1)e seen, while Figure 2 shows the left side.
If the system is not aware of the user's view, I;he system may talk about the left; tie rod end even though the user's view remains the right side (Figure 1).
In such n (:ase, the system shouM chang(: its d(> scril)tion or ask the user to change her/his view to |;11('.
left; side.
view (Figure 2) and r('.(-Olmnen(:e its instruction hi)out this part.
Therefore, the system should be al)le to change the content of the dialogue according 1;o the user's view.
In order to ac(:omplish this, the system shoul(1 lmve ;1.
content selection nlechan.ism whi(:h incrementally (let:ides i;h('~ content while ('he(:king the llSef~s (;llrrellt vi(!w.
Second, t;here could 1)e a case in which 1;21(; user chang(~s 1;he, topi(: as well as the vie\vl)oillt as interrupl;ing the.
system's instru('t;ion, i n such a case, the (tia.h)gue~ system shouhl kee l) track of the user's viewpoint as ~ 1)art of the dialogue state nnd return to that viewpoint when resmning the (lia.logu(? after the interrupl;ing sul)(lialogue.
Sul)l)ose that while the sys|;em is (',xl)lnining tlm right; t)i(; rod end, th('.
user initially looks a,t the right side, (l"igure 1) hut then shifts her/his view to the left (Figure 2) and asks about the \]eft knu(-kle arm.
After finishing a sub(lialogue about this arm, the syst(;nl tries to return to the dialogue al)out the interrupted topic.
At this time, if the sysl;em resumed the dialogue using the current view (Figure 2), the view and the instruction would \])e(;olne mismatched.
When resmning the interrupted dialogue, it would be less (:onfllsing to the user if the system retm:ned to the user's prior viewl)oint rather than selecting n new o11o.
'\].'he user may be (:onfilsed if the dialogue is resulned but the observed state looks different.
\,Ve address the ~fl)ove problems.
In order to (:ope wit;h the first; problem, we present a content selection mechanism that incrementally expands the content plan of a multimodal dialogue while checking the user's view.
To solve the second 1)roblem, we present a.
dialogue nmnagemerit me(:\]mnism l;hat keel)s t:ra(-k of the user's viewpoint as a part of the diah)gue context and 573............................................................................................................................................................................ -: ~;'~ Operation l,uttons Figure 3: The system architecture uses this intbrmation in resuming the dialogue after interruptive subdialogues.
3 Related
work There are many multimodal systems, such as nmltimedia presentation systems and animated agents (Mwbury, 1993; Lester et al., 1997; Bares and Lester, 1997; Stone and Lester, 1996; Towns et al., 1998)~ all of which use 3D graphics and 3D animations.
In some of them (Maybury, 1993; Wahlster et al., 1993; Towns et al., 1998), planning is used in generating multimodal presentations including graphics and animations.
They are similar to MID-aD in that they use planning mechanisms in content planning.
However, in presentation systems, unlike dialogue systems, the user just watches the presentation without changing her/his view.
Therefore, these studies are not concerned with dlanging the content of the discourse to match the user's view.
In some studies of dialogue management (Rich and Sidner, 1998; Stent et M., 1999), the state of the dialogue is represented using Grosz and Sidner's framework (Grosz and Sidner, 1986).
We also adopt this theory in our dialogue management mechanism.
However, they do not keep track of the user's viewpoint information as a part of the dialogue state because they were not concerned with dialogue management in virtual environments.
Studies on pedagogical agents have goals closer to ours.
In (Rickel and .\]ohnson, 1999), a pedagogical agent demonstrates the sequential operation of complex machiuery and answers some follow up questions fl'on~ the student.
Lester et al.(1999) proposes a lifelike pedagogical agent that supports problemsolving activities.
Although these studies are concerned with building interactive learning environments using natural language, they do not discuss how to decide the course of on-going instruction dialogues in an incremental and coherent way.
4 Overview
of the System Architecture This section describes the architecture of MID3D.
This system instructs users how to dismantle the steering system of a cal'.
Tile system steps through the procedure and the user can interrupt the system's instructions at any time.
Figme 3 shows the architecture and a snapshot of the system.
The 3D virtual environment is viewed through an application window.
A 3D model of a part of the car is provided and a frog574 like character is used as the pedagogical agent (Johnson et al., 2000).
The user herself/himself Call also al)l)ear in the virtual enviromn(mt as an avatar.
The buttons to the right of the 3D scre(m are operation 1)uttons tbr changillg the viewpoint.
By using these buttons, the user can freely change her/his viewt)oint at any time.
This system consists of five main modules: hll)Ut Analyzer, Domain Plan Reasoner, Content Planner (CP), Sentence Planner, Dialogue Manager (DM), and Virtual Environment Controller.
First of all, the user's inputs are interpreted through the Input Analyzer.
It receives strings of characters from the voice recognizer and the user's inputs ti'om the Virtual Environment Controller.
It interl)rets these inputs, transforms them into a semantic reprcsentation~ and sends them to the DM.
The DM, working as a dialogue management mechanism, keeI)s track of the dialogue (:ontext including the user:s view and decides the, next goal (or a(:tion) of the system.
Ut)on receiving an intmt from the user through the Input Analyzer, the DM sends it to the l)omaill Plan Reasoner (DPR) to get discourse goals for rest)onding to the inlmt.
For example, if th(: user requests some instruction, the DI'I decides the sequence of steps that realizes the l)rocedure 1)y refi~rring to domain knowh~dge.
Th(: 1354 (;hen adds (;he discourse goals to the goal agenda.
If the user does not sulmlit a ~lew (;ot)ie, the DM (:ontilmes to expand the, instruction plan 1)y sending a goal in the goal agenda to (:lie CP.
Details of the I)M are given in Section 6.
After the goal is sent to the CP, it decides the apl)ropriate contents of instruction dialogue by eml)loying a refinement-driven hierar(:hi(:al linear 1)lamfing technique.
When it; receives a goal fl'om the DM, it exl)ands the goal and returns its sul)goal to the DM.
13y ret)eating this process, the dialogue contents are, gradually specified.
Theretbre, the CP provides the scenario tbr the instruction 1)ased on the control 1)rovided by the DM.
Details of the CP are provided in Section 5.
The Sentence Plalmer generates surface, linguisti(: expressions coordinated with action (Kato et al., 1996).
The linguistic exl)ressions arc.
output through a voice synthesizer.
Actions ;/re realized through the Virtual Enviromnent Controller as 3D animation.
For the Virtual Environment Controller, we use HyCLASS (Kawanol)e et al., 1998), which <Operator 1> (:tleader :Iiftbcl :Constraints :Main-Acts :Subskliary-Acts <Operator 2> (:lleader :Effect :Conslraints :Main-Acts :Subsidiary-Acts (Inshuct-act N l l ?act MM) (BMB S 11 (Goal II (Done 11 ?act))) ((KB (Obj ?act ?object)) (Visible-p (Visible ?ol~iect t))) ((Look S II) (Request S I I (Try It (action ?act)) NO-SYNC MM)) ((Describeact S II ?act MM) (Reset S (actioll ?act)))) (Instruct-act S 11 ?act MM) (BMB S 11 (Goal I1 (Done 11 '?act))) ((KB (Obj ?act ?object)) (Visiblc-p (Visible ?object oil))) ((Look S ll) (Make-recognize S 11 (Object ?object) MM) (Rcqucst S 11 (Try I1 (action ?act)) NO-SYNC M M)) ((l)escribc-act S 11 ?act MM) (Reset S (action '?act)))) Figure 4: Exanlt)les of Content Plan Operators is a 3D simulation-1)ased environment tbr edu(:ational activities.
Several APls are provided tbr controlling HyCLASS.
By using these interfaces, the CP and the DM can discern the liser~s view and issue an action command in ()l'der to challge the virtual (;nvironnmllt.
\Â¥h(m HyCLASS receives an action command, it interprets the command and renders the 31) animation corresponding to the action in real time.
5 Selecting
the Conten(; of Instruction Dialogue Ill this section, we introduce the CP and show how the instruction dialogue is (leeided in all in(:renl(:ntal way to ma, tch the user's view.
5.1 Content
Planner In MID-3D, the CP is (:ailed by the DM.
Wheal a goal is put to the CP fl'(nn the DM, it; selects a plan operator fi)r achieving the goal, applies the ol)erator to lind new subgoals, and returns them to l;he \])M.
The sul)goals are then added to the goal agenda maintained by the DM.
Theretbre, the CP provides the seenm:io tbr the instruction dialogue to the DM and enables MID-3D to output coherent instructions.
Moreover, the Content Planer emt)loys depth-first search with a retinement-drivell hierarchical linear plmming algorithm as in (Cmvsey, 1992).
The advantage of this method is that the t)lan is de, veloped increnmntally, and can be changed while the conversation is in progress.
Thus, by aI)plying this algorithm to 3D dialogues, it be(-omes lmssible to set instruction dialogue strategies that are contingent on the user's view.
575 5.2 Considering the User's View in Content Selection In order to decide the dialogue content according to tile user's view, we extend the description of the content plan operator (Andrd and Rist, 1993) by using the user's view as a constraint in plan operator selection.
We also modify the constraint checking flmctions of |;lie previous planning algorithm such that HyCLASS is queried about the state of the virtual environment.
Figure 4 shows examples of content plan operators.
Each operator consists of the name of the operator (Header), the etfcct resulting from plan execution (Effect), the constraints for executing the plan (Constraints), the essential subgoals (Main-acts), and the optional subgoals (Subsidiary-acts).
As shown in {Operator 1).
in Figure 4, we use the constraint (gisible-p (Visible ?object t)) to check whether the object is visible fl'om tile user's viewpoint.
Actually, the CP asks HyCLASS to examine whether the object is in the student's field of view.
If an object is bound to the ?object variable by rel~rring to the knowledge base, and the object is visible to the user, (Operator 1) is selected.
As a result, two Main-Acts (looking at the, user and requesting to try to do the action) and two Subsidiary-Acts (showing how to do the action, then resetting the state) are set as subgoals and returned to the DM.
In contrast, if l;he object is not visible to the user, {Operator 2} is selected.
In this case, a goal for making the user i(tenti(y the object is added to the Main-Acts; (Hake-recognize S H (Object ?object) MM).
As shown al)ove, the user's view is considered in deciding the instruction strategy.
In addition to the above example, the distance between the target object and the user as well as three dimensional overlapping of objects, can also be considered as constraims related to the user's view.
Although the user's view is also considered in selecting locative expressions of objects in the Sentence Planner in MID-3D, we do not discuss this issue here becanse surface generation is not the tbcus of this paper.
6 Managing
Interruptive Subdialogue The DM controls the other components of MID3D based on a discourse model that represents the state of tile dialogue.
This section describes the DM and shows how the user's view is used in managing the instruction dialogue.
6.1 Maintaining
the Discourse Model The DM maintains a discourse model for tracking the state of the dialogue.
The discourse model consists of the discourse goal agenda (agenda), focus stack, and dialogue history.
The agenda is a list of goals that should be achieved through a dialogue between the user and the system.
If all the goals in the agenda are accomplished, the instruction (tialogue finishes successflflly.
The focus stack is a sta& of discourse segment frames (DSF).
Each DSF is a frmne structure that stores the tbllowing inlbrmation as slot vMues: utterance content (UC): A list of utterance contents constructing a discourse segment.
Physical actions are also regarded as uttcra.nce contents (D;rguson and Allen, 1998).
discourse purpose (1)19: The purt)ose of a discourse segment.
9oal state (GS): A state (or states) whi('h shouhl 1)e accomplished to achieve the discourse lmrpose of the segment.
In addition to these, we add the user's viewpoint slot to the DSF description in order to track the user's viewl)oint information: user's vic.'wpoint (UV): Current user's viewpoint, which is represented as the position and orientation of the camera.
The position consists of x-, y-, and z-coordinates.
The orientation consists of x-, y-, and z-angles of the ('amera.
The basic algorithm of the DM is to repeat (a) th(; peribnning actions step and (1)) updating the discourse model, until there is no unsatisfied goal in the agenda (~IYaum, 1994).
In 1)ertbrming actions step, the DM decides what to do next ill the current dialogue state, an(1 then pertbnns the action.
When continuing the system explanation, the DM posts the first goal in the agenda to the CP.
If the user's response is needed in the current state, the 1)M waits tbr the nser's input.
The other step in the DM algorith.m is to update the discourse model according to the state that results from the actions pertbrmed by the user as well as the actions peribrmed by the system.
Although we do not detail this step here, the tbllowing operations could be executed depending on the case.
if the current discourse purpose is accomplished, the top level DSF is popped and added to the dialogue history, q_/he 576 l I)SFI21 DSFI2 DSFI Jf J J UV: ((18, -20, -263) (0, 0.3 I, 0)) UC: ((IJseJ~act (Ask where heal_r)) I)P: (Response-to-user-act (Uscr-act (ask where bootr))) GS: ((Know 11 (About (l'lace_of boot_r))))...
UV: ((-38, -22, -259) (0, -0.33, 0)) UC: ((System-act (lnl'(~rm S 11 (Show S (Action rcmovc-tiemd end.I)) NO-SYNC I'R)) DI': (I)cscribe-acl S l I rcmove-licrod end I)) GS: ((Know 1I (llove-lo-do 11 (action remove-tiered eml I))))...
Figure 5: Example of the state of a dialogue system then assunms that the user understands the instruction and adds the assumption to the user model.
If a new discourse 1)urpose is introduced from the CP, the I)M creates a new DSF by setting the header of the selected plan operator in the discourse lmrpose slot mM the effi~ct of the operator in the goal state slot.
The DSF is then trashed to the tbcus stack.
If the current discourse purpose is contimmd, the DM updates the information of the top level DSF.
6.2 Considering
the User's View in Coping with Interruptive Subdialogues The main ditlbxence of the Dialogue Manager of our system from the i)revious one is to maintain the user's viewpoint information and use this in managing the dialogue.
When the DM updates the information of the current DSt i', it observes the user's viewi~oint at that petal; and renews the UV slot and it also adds the sema.nl;ic representation of utterance (or action) in the UC slot.
As a result, it becomes possible to update the user's viewpoint information at each turn, and to track the user's viewl)oint in an on-going dialogue.
By using this mechanism, the DM can cope with interruptive subdialognes.
In resmning from a subdialogue, the user may become contimed if the dialogue is resumed but the observed state differs from what the user relllelllhers.
In order to match the view to the resumed dialogtm, the I)M refers the UV slot of the top DSF and puts the users view ha& to that point.
This ensures that the user experiences a smooth transition back to the previous topic.
Figure 5 shows an example of the state of a dialogue.
DSF12 represents a discourse segment that describes how to remove the left tie rod end.
DSF121 represents the user-initiated interrul)tive subdialogue about where the right \[14\]System: The left knuckle arm is removed like this.
(with the anilnation showing the left knuckle arm coming off) \[ 15\]User: (After moving the viewpoint to Figure I and clicking the right knuckle ann) What is this?
\[16\]System: This is the right knuckle arm.
\[I 7\]Uscr: OK.
\[18\]Systeln: Now, let's continue the explanation.
(with changing the view to the one in utterance \[ 14\]) \[19\]System: The left knuckle arm is removed like this.
(with the animation showing the left knuckle arm coming off) \[20\]User: Yes.
\[21\]System: Now, try removing the left knuckle arm.
\[22\]User: (After clicking the left knuckle arm) Remove it.
\[23\]System: You have succcssfitlly completed thc operation.
\[24\]System: The next step is to remove the right tie rod cad.
\[25\]System: Let's move to the right tie rod end.
1126\] User: Ycs.
\[27\]System: (Moving the view to the right) The right tie rod end is in fiont of you.
\[28\] User: Yes, 1 see.
Figure 6: Example of a dialogue with MII)-3D boot is.
hmnediately before starting DSF\]21, the user's viewpoint in l)SF12 is ((-38, -22, -259) (0, -0.33, 0)).
After completing the subdialogue \])y answering the user's question, DSF121 is l)opped and the system resmnes DSF12.
At this time, the \])M gets the viewpoint value of the top DSF (DSF12), alld (;Oltlmands ItyCLASS to change the viewpoint to that view, which is in this case ((-as, -22, -2,59) (0, -0.a3, 0)) ' The systeln then restarts the interrupted dialogue.
7 Exmnple
In order to illustrate the behavior of MID-3D, an example is shown in Figure 6.
This is a part of an instruction dialogue on how to dismantle the steering system of a car.
The current topic is removing the left knuckle arm.
In utterance \[14\], the system describes how to remove this part in conjunction with an animation created by HyCLASS.
In \[15\], the user interrupted the system's instruction and asked "What is this"? by clicking the right knuckle arm.
At this point, the user's speech input was interpreted in the Input An~In the current system, it; is not 1)ossible to move the camera to an arbitrary point because of the limitations of the virtual environment controller employed.
Accordingly, this func|;ion is al)proximated by selecting the nearest of several predetined viewpoints.
577 alyzer and a user initiative subdialogue started by t)ushing another DSF onto the focus stack.
In order to answer the question, the DM asked the Domain Plan Reasoner how to answer the user's question.
As a result, a discourse goal was returned to the DM and added to the agenda.
The DM then sent the goal (Describe-name S H (object knuckle_arm_r)) to the CP.
This goal generated utterance \[16\].
In system utterance \[18\], in order to resume the dialogue, a recta-comment, "Now let's continue the explanation", was generated and the viewpoint returned to the previous one in \[14\] as noted in the DSF.
After returning to the previous view, the interrupted goal was re-planned.
As a result, utterance \[19\] was generated.
After completing this operation in \[23\], the next step, removing the right tie rod end, is started.
At this time, if the user is viewing the left side (Figure 2) and the system has the goal (Instruct-act S H remove-tierod_end_r MR), (Operator 2} in Figure 4 is applied because the target object, right tie rod end, is not visible fi'om the user's viewpoint.
Thus a goal of making the user view the right tie rod end is added as a subgoal and utterances \[24\] and \[25\] are generated.
8 Discussion
This paper proposed a inethod tbr altering instruction dialogues to match the user's view in a virtual enviromnent.
We described the Content Planner which can incrementally decide coherent instruction dialogue content to match changes in the user's view.
We also presented the Dialogue Manager, which can keep track of the user's viewpoint in an on-going dialogue and use this intbrmation in resuming from interruptive subdialogues.
These mechanisms allow to detect mismatches between the user's viewpoint and the topic at any point in the dialogue, and then to choose the instruction content and user's viewpoint appropriately.
MID-3D, an experimental system that uses these mechanisms, shows that the method we proposed is effective in realizing instruction dialogues that suit the user's view in virtual enviromnents.

