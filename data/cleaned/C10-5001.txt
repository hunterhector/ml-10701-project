Coling 2008: Kernel Engineering for Fast and Easy Design of Natural Language Applicationsâ€“Tutorial notes, pages 1â€“91,
Beijing, August 2010
Kernel Engineering for Fast and Easy 
Design of Natural Language Applications 
 Alessandro Moschitti 
Department of Information Engineering and Computer Science 
University of Trento  
Email: moschitti@disi.unitn.it 
The 23rd International Conference on Computational Linguistics 
August 22, 2010  
Beijing, China  
Schedule 
 â€¯ 14:00 15:30 First part 
 â€¯ 15:30 16:00 Coffee break   
 â€¯ 16:00 17:30 Second part 
1
Outline (1) 
 â€¯ Motivation 
 â€¯ Kernel-Based Machines 
 â€¯ Perceptron 
 â€¯ Support Vector Machines 
 â€¯ Kernel Definition 
 â€¯ Kernel Trick 
 â€¯ Mercerâ€™s conditions 
 â€¯ Kernel operators 
 â€¯ Basic Kernels 
 â€¯ Linear Kernel 
 â€¯ Polynomial Kernel 
 â€¯ Lexical Kernel 
Outline (2) 
 â€¯ Structural Kernels 
 â€¯ String and Word Sequence Kernels  
 â€¯  Tree Kernels 
 â€¯ Subtree, Syntactic, Partial Tree Kernels 
 â€¯ Applied Examples of Structural Kernels 
 â€¯ Semantic Role Labeling (SRL) 
 â€¯ Question Classification (QC)  
 â€¯ SVM-Light-TK 
 â€¯ Experiments in classroom with SRL and QC 
 â€¯ Inspection of the input, output, and model files 
2
Outline (3) 
 â€¯ Kernel Engineering 
 â€¯ Structure Transformation 
 â€¯ Syntactic Semantic Tree kernels 
 â€¯ Kernel Combinations 
 â€¯ Kernels on Object Pairs 
 â€¯ Kernels for re-ranking 
 â€¯ Practical Question and Answer Classifier based on     
SVM-Light-TK 
 â€¯ Combining Kernels 
 â€¯ Conclusion and Future Work 
Motivation (1) 
 â€¯ Feature design most difficult aspect in designing a 
learning system 
 â€¯ complex and difficult phase, e.g., structural feature 
representation: 
 â€¯ deep knowledge and intuitions are required 
 â€¯ design problems when the phenomenon is 
described by many features 
3
Motivation (2) 
 â€¯ Kernel methods alleviate such problems 
 â€¯ Structures represented in terms of substructures 
 â€¯ High dimensional feature spaces 
 â€¯ Implicit and abstract feature spaces 
 â€¯ Generate high number of features 
 â€¯ Support Vector Machines â€œselectâ€ the relevant 
features 
 â€¯ Automatic Feature engineering side-effect 
Part I: Kernel Methods Theory 
4
A simple classification problem: 
Text Categorization 
Sport 
  C
n
 
Politic 
    C
1
 
Economic 
            
C
2 
. . . . . . . . . . . 
Bush 
declares 
war 
Wonderful 
Totti 
Yesterday 
match 
Berlusconi 
acquires 
Inzaghi 
before 
elections 
Text Classification Problem 
 â€¯ Given: 
 â€¯ a set of target categories: 
 â€¯ the set T of documents,  
     define 
       f : T  â†’   2
C
 
 â€¯ VSM (Salton89â€™) 
 â€¯ Features are dimensions of a Vector Space. 
 â€¯ Documents and Categories are vectors of feature weights. 
 â€¯ d is assigned to        if  
  
â‚¬ 
ï² 
d â‹…
ï² 
C 
i
>th
â‚¬
C=C
1,..,C
n
{}
i
5
More in detail 
 â€¯ In Text Categorization documents are word 
vectors 
 â€¯ The dot product            counts the number of 
features in common 
 â€¯ This provides a sort of similarity 
  
â‚¬ 
Î¦(d
x
)=
ï² 
x =(0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)
                         buy       acquisition     stocks          sell     market
zx
ï²
â‹…
  
â‚¬ 
Î¦(d
z
)=
ï² 
z =(0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)
                         buy   company            stocks          sell     
Linear Classifier 
  
â‚¬ 
f(
ï² 
x )=
ï² 
x â‹…
ï² 
w +b=0,   
ï² 
x ,
 
w âˆˆâ„œ
n,bâˆˆâ„œ
 â€¯ The equation of a hyperplane is 
 â€¯    is the vector representing the classifying example 
 â€¯    is the gradient of the hyperplane 
 â€¯ The classification function is 
x
ï²
w
()sign()hxfx=
6
 â€¯ Mapping vectors in a space where they are linearly 
separable 
x
x
o
o
The main idea of Kernel Functions 
)(x
ï²
Ï†â†’
)x(Ï†
)(
)(o
)(
oÏ†
A mapping example 
 â€¯ Given two masses m
1 
and m
2 , one is constrained 
 â€¯ Apply a force f
a
 to the mass m
1   
 â€¯ Experiments 
 â€¯ Features m
1 , m
2 
and  f
a 
 â€¯ We want to learn a classifier that tells when a mass m
1
 will get far 
away from m
2 
 
2
1
21
),(
r
Cf=
 â€¯ If we consider the Gravitational Newton Law 
 â€¯ we need to find when f(m
1 , m
2 , r) < f
a 
7
A mapping example (2) 
))(),...,()(,...,(
11
xxx
nn
ï²ï²ï²
Ï†Ï†=â†’=
 â€¯ The gravitational law is not linear so we need to change space 
)ln,ln,(),(),,(
2121
rmfzykrmf
aa
zyxcmCfln,ln
2121
âˆ’+=âˆ’+=
(ln m
1,ln m
2,-2ln r)â‹… (x,y,z)ln f
a
 + ln C = 0, we can decide 
without error if the mass will get far away or not 
 â€¯ As 
0
2
âˆ’rf
a
 â€¯ We need the hyperplane 
A kernel-based Machine 
Perceptron training 
  
â‚¬ 
ï² 
w 
0
â†
ï² 
0 ;b
0
â†0;kâ†0;Râ†max
1â‰¤il
||
ï² 
x 
i
||
do
       for i= 1 to ï¬
         if y
i
(
ï² 
w 
k
â‹…
ï² 
x 
i
+b
k
)â‰¤0 then
                  
ï² 
w 
k+1
=
ï² 
w 
k
+Î·y
i
ï² 
x 
i
                  b
k+1
=b
k
+y
i
R
2
                 k1
        endif
      endfor
while an error is found
return k,(
ï² 
w 
k,b
k
) 
8
9
Novikoffâ€™s Theorem 
Let S be a non-trivial training-set and let 
Let us suppose there is a vector           and 
with Î³ > 0. Then the maximum number of errors of the perceptron 
is: 
*,||1=w
*
(,),.
ii
ybilÎ³+â‰¥x
2
*,
R
t
Î³
ï£«ï£¶
=
ï£¬ï£·
ï£­ï£¸
1
ma|.
i
il
R
â‰¤
=
10
 â€¯ In each step of perceptron only training data is added with 
a certain weight 
 â€¯ So the classification function 
 â€¯ Note that data only appears in the scalar product 
Dual Representation for Classification 
  
â‚¬ 
ï² 
w =Î±
j
j=1..ï¬
âˆ‘
y
j
ï² 
x 
j
  
â‚¬ 
sgn(
ï² 
w â‹…
ï² 
x +b)=sgnÎ±
j
j=1..ï¬
âˆ‘
y
j
ï² 
x 
j
â‹…
ï² 
x +b
ï£« 
ï£­ 
ï£¬ 
ï£¶ 
ï£¸ 
ï£· 
Dual Representation for Learning 
 â€¯ as well as the updating function  
 â€¯ The learning rate      only affects the re-scaling of the 
hyperplane, it does not affect the algorithm, so we can 
fix 1.Î·=
  
â‚¬ 
if y
i
(Î±
j
j=1..ï¬
âˆ‘
y
j
ï² 
x 
j
â‹…
ï² 
x 
i
+b)â‰¤0 then Î±
i
=Î±
i
+Î·
11
 â€¯ We can rewrite the classification function as 
 â€¯ As well as the updating function 
Dual Perceptron algorithm and Kernel 
functions 
  
â‚¬ 
h(x)=sgn(
ï² 
w 
Ï†
â‹…(
ï² 
x )+b
Ï†
)=sgn(Î±
j
j=1..ï¬
âˆ‘
y
j
Ï†(
ï² 
x 
j
)â‹…Ï†(
ï² 
x )+b
Ï†
)=
=sgn(Î±
j
i=1..ï¬
âˆ‘
y
j
k(
ï² 
x 
j,
ï² 
x )+b
Ï†
)
  
â‚¬ 
if y
i
Î±
j
j=1..ï¬
âˆ‘
y
j
k(
ï² 
x 
j,
ï² 
x 
i
)+b
Ï†
ï£« 
ï£­ 
ï£¬ 
ï£¶ 
ï£¸ 
ï£· â‰¤0 allora Î±
i
=Î±
i
+Î·
Support Vector Machines 
 â€¯ Hard-margin SVMs 
 â€¯ Soft-margin SVMs 
12
Which hyperplane do we choose? 
Classifier with a Maximum Margin 
Var
1 
Var
2 
Margin 
Margin 
IDEA 1: Select the 
hyperplane with 
maximum margin 
13
Support Vectors 
Var
1 
Var
2 
Margin 
Support Vectors 
Support Vector Machines 
Var
1 
Var
2 
kbxwâˆ’=+â‹…
ï²
kbxw=+â‹…
ï²
0â‹…x
ï²
The margin is equal to 
2k
w
14
Support Vector Machines 
Var
1 
Var
2 
kbxwâˆ’=+â‹…
ï²
kbxw=+â‹…
ï²
0â‹…x
ï²
The margin is equal to 
2k
w
We need to solve 
  
â‚¬ 
max
2k
||
ï² 
w ||
ï² 
w â‹…
 
x +bâ‰¥+k,  if 
ï² 
x  is positive  
ï² 
w â‹…
ï² 
x bâ‰¤âˆ’k,  if 
ï² 
x  is negative 
Support Vector Machines 
Var
1 
Var
2 
1wxbâ‹…+=âˆ’
ï²
1wxbâ‹…+=
ï²
0â‹…x
ï²
There is a scale for 
which k=1.  
The problem transforms 
in: 
  
â‚¬ 
max
2
||
ï² 
w ||
ï² 
w â‹…
 
x +bâ‰¥+1,  if 
ï² 
x  is positive  
ï² 
w â‹…
 
x bâ‰¤âˆ’1,  if 
ï² 
x  is negative 
15
Final Formulation 
â‚¬ 
â‡’
  
â‚¬ 
max
2
||
ï² 
w ||
ï² 
w â‹…
ï² 
x 
i
+bâ‰¥+1,  y
i
=1
ï² 
w â‹…
ï² 
x 
i
+bâ‰¤âˆ’1,  y
i
 =-1
  
â‚¬ 
max
2
||
ï² 
w ||
y
i
(
ï² 
w â‹…
ï² 
x 
i
+b)â‰¥1
  
â‚¬ 
min
||
ï² 
w ||
2
y
i
(
ï² 
w â‹…
ï² 
x 
i
+b)â‰¥1
  
â‚¬ 
min
||
ï² 
w ||
2
2
y
i
(
ï² 
w â‹…
ï² 
x 
i
+b)â‰¥1
â‚¬ 
â‡’
â‚¬ 
â‡’
â‚¬ 
â‡’
Optimization Problem 
 â€¯ Optimal Hyperplane: 
 â€¯ Minimize 
 â€¯ Subject to 
 â€¯ The dual problem is simpler 
   
libxwy
ii,...,1,)((
2
1
)(
=â‰¥+â‹…
=
ï²
Ï„
16
Lagrangian Definition 
Dual Optimization Problem 
17
Dual Transformation 
 â€¯ To solve the dual problem we need to evaluate: 
 â€¯ Given the Lagrangian associated with our problem 
 â€¯ Let us impose the derivatives to 0, with respect to  w
ï²
Dual Transformation (contâ€™d) 
 â€¯ and wrt b 
 â€¯ Then we substituted them in the objective function 
18
The Final Dual Optimization Problem 
Khun-Tucker Theorem 
 â€¯ Necessary and sufficient conditions to optimality 
19
Properties coming from constraints 
 â€¯ Lagrange constraints: 
 â€¯ Karush-Kuhn-Tucker constraints 
 â€¯ Support Vectors have     not null 
 â€¯ To evaluate b, we can apply the following equation 
  
â‚¬ 
a
i
i=1
l
âˆ‘
y
i
=0,
ï² 
w =Î±
i
i=1
l
âˆ‘
y
i
ï² 
x 
i
lbwx
ii,...,,])([âˆ’+â‹…â‹…
ï²
Î±
Soft Margin SVMs 
Var
1 
Var
2 
1wxbâ‹…+=âˆ’
ï²
1wxbâ‹…+=
ï²
0â‹…x
ï²
i
Î¾
   slack variables are 
added 
Some errors are allowed 
but they should penalize 
the objective function 
i
Î¾
20
Soft Margin SVMs 
Var
1 
Var
2 
1wxbâ‹…+=âˆ’
ï²
1wxbâ‹…+=
ï²
0â‹…x
ï²
i
Î¾
The new constraints are 
The objective function 
penalizes the incorrect 
classified examples 
C is the trade-off 
between margin and the 
error 
  
â‚¬ 
y
i
(
ï² 
w â‹…
ï² 
x 
i
+b)â‰¥1âˆ’Î¾
i
   
âˆ€
ï² 
x 
i
  where  Î¾
i
0
  
â‚¬ 
min
1
2
||
ï² 
w ||
2
+CÎ¾
i
i
âˆ‘
Dual formulation 
 â€¯ By deriving wrt 
  
â‚¬ 
ï² 
w ,
 
Î¾ and b
21
Partial Derivatives 
Substitution in the objective function 
 â€¯      of Kronecker  
ij
Î´
22
Final dual optimization problem 
Soft Margin Support Vector Machines 
 â€¯ The algorithm tries to keep Î¾
i
 low and maximize the margin 
 â€¯ NB: The number of error is not directly minimized (NP-complete 
problem); the distances from the hyperplane are minimized 
 â€¯ If Câ†’âˆ, the solution tends to the one of the hard-margin algorithm 
 â€¯ Attention !!!: if C = 0 we get          = 0, since  
 â€¯ If C increases the number of error decreases. When C tends to 
infinite the number of errors must be 0, i.e. the hard-margin 
formulation 
||w
ï²
  
â‚¬ 
min
1
2
||
ï² 
w ||
2
+CÎ¾
i
i
âˆ‘
  
â‚¬ 
y
i
(
ï² 
w â‹…
ï² 
x 
i
+b)â‰¥1âˆ’Î¾
i
   âˆ€
ï² 
x 
i
Î¾
i
â‰¥0
  
â‚¬ 
y
i
bâ‰¥1âˆ’Î¾
i
   âˆ€
ï² 
x 
i
23
Robusteness of Soft vs. Hard Margin SVMs 
i
Î¾
Var
1 
Var
2 
0=+â‹…bxw
ï²
Î¾
i 
Var
1 
Var
2 
0=+â‹…bxw
ï²
Soft Margin SVM Hard Margin SVM 
Kernels in Support Vector Machines  
 â€¯ In Soft Margin SVMs we maximize: 
 â€¯ By using kernel functions we rewrite the problem as: 
24
Kernel Function Definition 
 â€¯ Kernels are the product of mapping functions 
such as 
  
â‚¬ 
ï² 
x âˆˆâ„œ
n,    
ï² 
Ï† (
ï² 
x )=(Ï†
1
(
ï² 
x ),Ï†
2
(
ï² 
x ),...,Ï†
m
(
ï² 
x ))âˆˆâ„œ
m
The Kernel Gram Matrix 
 â€¯ With KM-based learning, the sole information used 
from the training data set is the Kernel Gram Matrix 
 â€¯ If the kernel is valid, K is symmetric definite-positive . 
25
Valid Kernels 
Valid Kernels contâ€™d 
 â€¯ If the matrix is positive semi-definite then we can 
find a mapping Ï† implementing the kernel function 
26
Mercerâ€™s Theorem (finite space) 
 â€¯ Let us consider 
  
â‚¬ 
K = K(
ï² 
x 
i,
ï² 
x 
j
)
(
i,j=1
n
 â€¯ K symmetric â‡’ âˆƒ V:                      for Takagi factorization of a 
complex-symmetric matrix, where:  
 â€¯ Î› is the diagonal matrix of the eigenvalues Î»
t
 of K  
 â€¯                        are the eigenvectors, i.e. the columns of V 
 â€¯ Let us assume lambda values non-negative 
 
K =VÎ›â€² V 
  
â‚¬ 
ï² 
v 
t
 = v
ti
()
i=1
n
  
â‚¬ 
Ï†:
ï² 
x 
i
 â†’ Î»
t
v
ti( )
t=1
n
âˆˆâ„œ
n,i=1,..,n
Mercerâ€™s Theorem 
(sufficient conditions) 
  
â‚¬ 
Î¦(
ï² 
x 
i
)â‹…Î¦(
ï² 
x 
j
)=Î»
t
v
ti
t=1
n
âˆ‘
tj
=VÎ›â€² V 
( )
ij
=K
ij
=K(
ï² 
x 
i,
ï² 
x 
j
)
     
 â€¯ Therefore 
                                                                    ,  
 â€¯ which implies that K is a kernel function       
27
Mercerâ€™s Theorem 
(necessary conditions) 
  
â‚¬ 
ï² 
z 
2
=
ï² 
z â‹…
ï² 
z =Î›â€² V 
ï²
v 
s
Î›â€² V 
ï²
v 
s
=
ï² 
v 
s
'VÎ›â€² V 
ï²
v 
s
=
 
ï² 
v 
s
'K
ï² 
v 
s
= 
ï² 
v 
s
'Î»
s
ï² 
v 
s
=Î»
s
ï² 
v 
s
2
<0
 â€¯ Suppose we have negative eigenvalues Î»
s 
and 
eigenvectors       the following point 
 â€¯ has the following norm: 
this contradicts the geometry of the space. 
  
â‚¬ 
ï² 
v 
s
  
â‚¬ 
ï² 
z =v
si
Î¦(
ï² 
x 
i
)
i=1
n
âˆ‘
=v
si
Î»
t
v
ti( )
t
=
i=1
n
âˆ‘
Î›â€² V 
ï²
v 
s
   
Is it a valid kernel? 
 â€¯ It may not be a kernel so we can use MÂ´Â·M 
28
Valid Kernel operations 
 â€¯ k(x,z) = k
1
(x,z)+k
2
(x,z) 
 â€¯ k(x,z) = k
1
(x,z)*k
2
(x,z) 
 â€¯ k(x,z) = Î± k
1
(x,z) 
 â€¯ k(x,z) = f(x)f(z) 
 â€¯ k(x,z) = k
1
(Ï†(x),Ï†(z)) 
 â€¯ k(x,z) = x'Bz 
Basic Kernels for unstructured data 
 â€¯ Linear Kernel 
 â€¯ Polynomial Kernel 
 â€¯ Lexical kernel 
 â€¯ String Kernel 
29
Linear Kernel 
 â€¯ In Text Categorization documents are word 
vectors 
 â€¯ The dot product            counts the number of 
features in common 
 â€¯ This provides a sort of similarity 
  
â‚¬ 
Î¦(d
x
)=
ï² 
x =(0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,1)
                         buy       acquisition     stocks          sell     market
zx
ï²
â‹…
  
â‚¬ 
Î¦(d
z
)=
ï² 
z =(0,..,1,..,0,..,1,..,0,..,0,..,0,..,1,..,0,..,0,..,1,..,0,..,0)
                         buy   company            stocks          sell     
Feature Conjunction (polynomial Kernel) 
 â€¯ The initial vectors are mapped in a higher space 
 â€¯ More expressive, as            encodes  
      Stock+Market vs. Downtown+Market features 
 â€¯ We can smartly compute the scalar product as 
)1,2,2,(),(
1121
xxxxâ†’><Î¦
),()1()(        
122
)1,2,,,,,         
)(
1
21
2
2
zxKzxzx
z
z
Poly
ï²ï²
ï²
=+â‹…+=
+
=â‹…
â‹…
)
30
Document Similarity 
industry 
telephone 
 market 
company 
product 
Doc 1 
Doc 2 
Lexical Semantic Kernel [CoNLL 2005] 
 â€¯ The document similarity is the SK function: 
 â€¯ where s is any similarity function between words, 
e.g. WordNet [Basili et al.,2005] similarity or LSA 
[Cristianini et al., 2002] 
 â€¯ Good results when training data is small 
â‚¬ 
SK(d
1,d
2
)= s(w
1,w
2
)
w
1
âˆˆd
1,w
2
âˆˆd
2
âˆ‘
31
Using character sequences 
zx
ï²
â‹…
  
â‚¬ 
Ï†("bank")=
ï² 
x =(0,..,1,..,0,..,1,..,0,......1,..,0,..,1,..,0,..,1,..,0)
 â€¯          counts the number of common substrings 
 bank       ank           bnk          bk          b 
  
â‚¬ 
Ï†("rank")=
ï² 
z =(1,..,0,..,0,..,1,..,0,......0,..,1,..,0,..,1,..,0,..,1)
 rank               ank                  rnk          rk            r 
  
â‚¬ 
ï² 
x â‹…
ï² 
z =Ï†("bank")â‹…Ï†("rank")=k("bank","rank")
String Kernel 
 â€¯ Given two strings, the number of matches 
between their substrings is evaluated 
 â€¯ E.g. Bank and Rank 
 â€¯ B, a, n, k, Ba, Ban, Bank, Bk, an, ank, nk,.. 
 â€¯ R, a , n , k, Ra, Ran, Rank, Rk, an, ank, nk,.. 
 â€¯ String kernel over sentences and texts 
 â€¯ Huge space but there are efficient algorithms 
32
Formal Definition ,  where ,  where 
i
1 
+1 
Kernel between Bank and Rank 
33
An example of string kernel 
computation 
Efficient Evaluation 
 â€¯ Dynamic Programming technique 
 â€¯ Evaluate the spectrum string kernels 
 â€¯ Substrings of size p 
 â€¯ Sum the contribution of the different spectra 
34
Efficient Evaluation 
An example: SK(â€œGattaâ€,â€Cataâ€) 
 â€¯ First, evaluate the SK with size p=1, i.e. â€œaâ€, 
â€œaâ€,â€tâ€,â€tâ€,â€aâ€,â€aâ€ 
 â€¯ Store this in the table 
â‚¬ 
  SK
p=1
  
35
Evaluating DP2 
 â€¯ Evaluate the weight of the string of size p in case 
a character will be matched  
 â€¯ This is done by multiplying the double summation 
by the number of substrings of size p-1 
Evaluating the Predictive DP on 
strings of size 2 (second row) 
 â€¯ Letâ€™s consider substrings of size 2 and suppose that: 
 â€¯ we have matched the first â€œaâ€ 
 â€¯ we will match the next character that we will add to the two strings 
 â€¯ We compute the weights of matches above at different string 
positions with some not-yet known character â€œ?â€ 
 â€¯ If the match occurs immediately after â€œaâ€ the weight will be Î»
1+1 
x Î»
1+1 = 
Î»
4
 and we store just Î»
2
 in the DP entry in [â€œaâ€,â€aâ€] 
36
Evaluating the DP wrt different 
positions (second row) 
 â€¯ If the match for â€œgattaâ€ occurs after â€œtâ€ the weight will be Î»
1+2  
(x Î»
2 = 
Î»
5
) since the substring for it will be with â€œaâ˜?â€ 
 
 â€¯
 
We write such prediction in the entry [â€œaâ€,â€tâ€] 
 â€¯ Same rationale for a match after the second â€œtâ€: we have 
the substring â€œaâ˜â˜?â€  (matching with â€œa?â€ from â€œcattaâ€) for 
a weight of Î»
3+1 
 (x Î»
2
) 
Evaluating the DP wrt different 
positions (third row) 
 â€¯ If the match occurs after â€œtâ€ of â€œcataâ€, the weight will be Î»
2+1 
 
(x Î»
2 = 
Î»
5 
) since it will be with the string â€œaâ˜?â€, with a weight 
of Î»
3  
 â€¯ If the match occurs after â€œtâ€ of both â€œgattaâ€ and â€œcataâ€, there 
are two ways to compose substring of size two:
 
â€œaâ˜?â€ with 
weight Î»
4
 or â€œt?â€ with weight Î»
2
 â‡’ the total is Î»
2
+Î»
4
 
 
37
Evaluating the DP wrt different 
positions (third row) 
 â€¯ The final case is a match after the last â€œtâ€ of both â€œcatâ€ and 
â€œgattaâ€ 
 â€¯  There are three possible substrings of â€œgattaâ€: 
 â€¯ â€œaâ˜â˜?â€, â€œtâ˜?â€, â€œt?â€ for â€œgattaâ€ with weight Î»
3
 , Î»
2
 or Î», respectively. 
 â€¯ There are two possible substrings of â€œcataâ€
 
 â€¯  â€œaâ˜?â€, â€œt?â€ with weight Î»
2
 and Î» 
 â€¯ Their match gives weights: Î»
5 , Î»
3, Î»
2  
â‡’ by summing: Î»
5 
+ Î»
3
 + Î»
2
 
Evaluating SK of size 2 using DP2 
 â€¯ The number (weight) of 
substrings of size 2 between 
â€œgatâ€ and â€œcatâ€ is Î»
4
 = Î»
2
 
([â€œaâ€,â€aâ€] entry of DP) x Î»
2
(cost 
of one character), where a = 
â€œtâ€ and   b = â€œtâ€. 
 â€¯ Between â€œgattaâ€ and â€œcataâ€ is 
Î»
7 
+ Î»
5
 + Î»
4, i.e the matches of 
â€œaâ˜â˜aâ€, â€œtâ˜aâ€, â€œtaâ€ with 
â€œaâ˜aâ€ and â€œtaâ€  
â‚¬ 
  SK
p=2
  
38
Tree kernels 
 â€¯ Subtree, Subset Tree, Partial Tree kernels 
 â€¯ Efficient computation 
Example of a parse tree 
 â€¯ â€œJohn delivers a talk in Romeâ€ 
S â†’ N VP
 
VP â†’ V NP PP
 
PP â†’ IN N
 
N â†’ Rome
 
N
 
Rome
 
S
 
N
 
NP
 
D
 
N
 
VP
 
V
 
John
 
in
 
 delivers 
 
a
 
talk
 
PP
 
IN
 
39
The Syntactic Tree Kernel (STK)  
[Collins and Duffy, 2002] 
NP 
D N 
VP 
V 
delivers 
a    talk 
The overall fragment set 
40
The overall fragment set 
NP
 
D
 
VP
 
a
 
Children are not divided 
Explicit kernel space 
zx
ï²
â‹…
  
â‚¬ 
Ï†(T
x
)=
ï² 
x =(0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,1,..,0)
 â€¯          counts the number of common substructures 
  
â‚¬ 
Ï†(T
z
)=
ï² 
z =(1,..,0,..,0,..,1,..,0,..,1,..,0,..,1,..,0,..,0,..,1,..,0,..,0)
41
Efficient evaluation of the scalar product 
  
â‚¬ 
ï² 
x â‹…
ï² 
z =Ï†(T
x
)â‹…Ï†(T
z
)=K(T
x,
z
)=
                    =
n
x
âˆˆT
x
âˆ‘
Î”(n
x,n
z
)
n
z
âˆˆT
z
Efficient evaluation of the scalar product 
 â€¯ [Collins and Duffy, ACL 2002] evaluate Î” in O(n
2
): 
â‚¬ 
Î”(n
x,n
z
)=0,  if the productions are different else
Î”(n
x,n
z
)=1,   if pre-terminals else
Î”(n
x,n
z
)=(1+Î”(ch(n
x,j),ch(n
z,j)))
j=1
nc(n
x
)
âˆ
  
â‚¬ 
ï² 
x â‹…
ï² 
z =Ï†(T
x
)â‹…Ï†(T
z
)=K(T
x,
z
)=
                    =
n
x
âˆˆT
x
âˆ‘
Î”(n
x,n
z
)
n
z
âˆˆT
z
42
Other Adjustments 
 â€¯ Normalization 
â‚¬ 
Î”(n
x,n
z
)=Î»,   if pre-terminals else
Î”(n
x,n
z
)=Î»(1+Î”(ch(n
x,j),ch(n
z,j)))
j=1
nc(n
x
)
âˆ
â‚¬ 
â€² K (T
x,
z
) =
K(T
x,
z
)
K(T
x,
x
)Ã—K(T
z,
z
)
 
 â€¯ Decay factor 
SubTree (ST) Kernel [Vishwanathan and Smola, 2002] 
 
NP 
D N 
a   talk 
D N 
a   talk 
NP 
D N 
VP 
V 
delivers 
a    talk 
V 
delivers 
43
Evaluation 
â‚¬ 
Î”(n
x,n
z
)=0,  if the productions are different else
Î”(n
x,n
z
)=1,   if pre-terminals else
Î”(n
x,n
z
)=(1+Î”(ch(n
x,j),ch(n
z,j)))
j=1
nc(n
x
)
âˆ
 â€¯ Given the equation for the SST kernel 
Evaluation 
â‚¬ 
Î”(n
x,n
z
)=0,  if the productions are different else
Î”(n
x,n
z
)=1,   if pre-terminals else
Î”(n
x,n
z
)=Î”(ch(n
x,j),ch(n
z,j))
j=1
nc(n
x
)
âˆ
 â€¯ Given the equation for the SST kernel 
44
Fast Evaluation of STK [Moschitti, EACL 2006] 
where P(n
x
) and P(n
z
) are the production rules used 
at nodes n
x
 and n
z
 
â‚¬ 
K(T
x,
z
)= Î”(n
x,n
z
)
n
x,n
z
âˆˆNP
âˆ‘
NP=n
x,n
z
âˆˆT
x
Ã—T
z
:Î”(n
x,n
z
)â‰ 0
{ }
=
      =n
x,n
z
âˆˆT
x
Ã—T
z
:P(n
x
)=P(n
z
)
{ },
Algorithm 
45
Observations 
 â€¯ We order the production rules used in T
x
 and T
z,  
at loading time 
 â€¯ At learning time we may evaluate NP in  
    |T
x
|+|T
z 
| running time 
 â€¯ If T
x
 and T
z
 are generated by only one production 
rule â‡’ O(|T
x
|Ã—|T
z 
| )â€¦ 
Observations 
 â€¯ We order the production rules used in T
x
 and T
z,  
at loading time 
 â€¯ At learning time we may evaluate NP in  
    |T
x
|+|T
z 
| running time 
 â€¯ If T
x
 and T
z
 are generated by only one production 
rule â‡’ O(|T
x
|Ã—|T
z 
| )â€¦Very Unlikely!!!! 
46
Labeled Ordered Tree Kernel 
NP
 
D N
 
VP
 
V
 
   gives
 
a
 
  talk
 
NP
 
D N
 
VP
 
V
 
a
 
   talk
 
NP
 
D N
 
VP
 
a
 
   talk
 
NP
 
D N
 
VP
 
a
 
NP
 
D
VP
 
a
 
NP
 
D
VP
 
NP
 
N
 
VP
 
NP
 
N
 
NP
 
NP
 
D N
 
D
NP
 
â€¦
 
VP
 
 â€¯ SST satisfies the constraint â€œremove 0 or all 
children at a timeâ€. 
 â€¯ If we relax such constraint we get more general 
substructures [Kashima and Koyanagi, 2002] 
Weighting Problems 
 â€¯ Both matched pairs give the 
same contribution. 
 â€¯ Gap based weighting is 
needed. 
 â€¯ A novel efficient evaluation 
has to be defined 
NP
 
D N
 
VP
 
V
 
   gives
 
a
 
  talk
 
NP
 
D N
 
VP
 
V
 
a
 
   talk
 
NP
 
D N
 
VP
 
V
 
   gives
 
a
 
  talk
 
   gives
 
JJ
 
  good
 
NP
 
D N
 
VP
 
V
 
   gives
 
a
 
  talk
 
JJ
 
  bad
 
47
Partial Trees, [Moschitti, ECML 2006] 
NP
 
D N
 
VP
 
V
 
brought
 
a
 
   cat
 
NP
 
D N
 
VP
 
V
 
a
 
   cat
 
NP
 
D N
 
VP
 
a
 
   cat
 
NP
 
D N
 
VP
 
a
 
NP
 
D
VP
 
a
 
NP
 
D
VP
 
NP
 
N
 
VP
 
NP
 
N
 
NP
 
NP
 
D N
 
D
NP
 
â€¦
 
VP
 
 â€¯ SST + String Kernel with weighted gaps on 
Nodesâ€™ children 
Partial Tree Kernel 
 â€¯ By adding two decay factors we obtain: 
48
Efficient Evaluation (1) 
 â€¯ In [Taylor and Cristianini, 2004 book], sequence kernels with 
weighted gaps are factorized with respect to different 
subsequence sizes. 
 â€¯ We treat children as sequences and apply the same theory 
D
p
 
Efficient Evaluation (2) 
 â€¯ The complexity of finding the subsequences is             
 â€¯ Therefore the overall complexity is 
    where Ï  is the maximum branching factor (p = Ï) 
49
Running Time of Tree Kernel Functions 
SVM-light-TK Software 
 â€¯ Encodes ST, SST and combination kernels  
    in SVM-light [Joachims, 1999] 
 â€¯ Available at http://dit.unitn.it/~moschitt/ 
 â€¯ Tree forests, vector sets 
 â€¯ The new SVM-Light-TK toolkit will be released 
asap 
50
Data Format 
 â€¯ â€œWhat does Html stand for?â€ 
 â€¯ 1  |BT| (SBARQ (WHNP (WP What))(SQ (AUX does)(NP (NNP 
S.O.S.))(VP (VB stand)(PP (IN for))))(. ?))  
|BT|    (BOW (What *)(does *)(S.O.S. *)(stand *)(for *)(? *))  
|BT|    (BOP (WP *)(AUX *)(NNP *)(VB *)(IN *)(. *))  
|BT|   (PAS (ARG0 (R-A1 (What *)))(ARG1 (A1 (S.O.S. NNP)))(ARG2 
(rel stand)))  
|ET| 1:1 21:2.742439465642236E-4 23:1 30:1 36:1 39:1 41:1 46:1 49:1 
66:1 152:1 274:1 333:1  
|BV| 2:1 21:1.4421347148614654E-4 23:1 31:1 36:1 39:1 41:1 46:1 49:1 
52:1 66:1 152:1 246:1 333:1 392:1 |EV|  
Basic Commands 
 â€¯ Training and classification 
 â€¯ ./svm_learn -t 5 -C T train.dat model 
 â€¯ ./svm_classify test.dat model 
 â€¯ Learning with a vector sequence 
 â€¯ ./svm_learn -t 5 -C V train.dat model 
 â€¯ Learning with the sum of vector and kernel 
sequences 
 â€¯ ./svm_learn -t 5 -C + train.dat model 
51
Part II: Kernel Methods for 
Practical Applications 
Kernel Engineering approaches 
 â€¯ Basic Combinations 
 â€¯ Canonical Mappings, e.g. object transformations  
 â€¯ Merging of Kernels 
52
Kernel Combinations an example 
 â€¯ Kernel Combinations: 
33
33
          , 
              , 
pTree
PTree
pTree
PTree
pTreeTreepTreeTree
K
K
KÃ—
=+Ã—=
Ã—+
Î³
kernel Tree
featuresflat    of  polynomial  
3
p
K
Object Transformation [Moschitti et al, CLJ 2008] 
 â€¯ Canonical Mapping, Ï†
M
()  
 â€¯ object transformation, 
 â€¯ e. g. a syntactic parse tree into a verb 
subcategorization frame tree. 
 â€¯ Feature Extraction, Ï†
E
() 
 â€¯ maps the canonical structure in all its fragments 
 â€¯ different fragment spaces, e. g. ST, SST and PT. 
                    
),()()(      
))())),(
2121
221
SKS
OOOK
EE
ME
=â‹…=
â‹…Ï†
53
Predicate Argument Classification 
 â€¯ In an event: 
 â€¯ target words describe relation among different entities 
 â€¯ the participants are often seen as predicate's 
arguments. 
 â€¯ Example: 
Paul gives a talk in Rome 
Predicate Argument Classification 
 â€¯ In an event: 
 â€¯ target words describe relation among different entities 
 â€¯ the participants are often seen as predicate's 
arguments. 
 â€¯ Example: 
[ 
Arg0
 Paul] [ 
predicate
 gives ] [ 
Arg1
 a talk] [ 
ArgM
 in Rome] 
54
Predicate-Argument Feature 
Representation 
Given a sentence, a predicate p: 
1.â€¯ Derive the sentence parse tree 
2.â€¯ For each node pair <N
p,N
x
>  
a.â€¯ Extract a feature representation set 
F 
b.â€¯ If N
x 
exactly covers the Arg-i, F is 
one of its positive examples 
c.â€¯ F is a negative example otherwise 
Vector Representation for the linear kernel 
Phrase Type 
Predicate 
Word 
Head Word 
Parse Tree 
Path 
Voice Active 
Position Right 
55
Kernel Engineering: Tree Tailoring 
PAT Kernel [Moschitti, ACL 2004] 
S
N
NP
 
D N
 
VP
 
V
 
Paul 
in
 
delivers 
a
 
   talk
 
PP
 
IN
 
  NP
 
jj
 
F
v,arg.0 
 formal
 
 N
 
      style
 
Arg. 0
 
a)
 S
N
NP
 
D N
 
VP
 
V
 
Paul
 
in
 
delivers
 
a
 
   talk
 
PP
 
IN
 
  NP
 
jj
 
 formal
 
 N
 
      style
 
F
v,arg.1 
b)
 S
N
NP
 
D N
 
VP
 
V
 
Paul
 
in
 
delivers
 
a
 
   talk
 
PP
 
IN
 
  NP
 
jj
 
 formal
 
 N
 
      style
 
Arg. 1
 
F
v,arg.M
 
c)
 
Arg.M
 
 â€¯ These are Semantic Structures 
 â€¯ Given the sentence: 
  [ 
Arg0
 Paul] [ 
predicate
 delivers] [ 
Arg1
 a talk] [ 
ArgM
 in formal Style] 
56
In other words we considerâ€¦ 
NP 
D N 
VP 
V 
delivers 
a    talk 
S 
N 
Paul 
in 
PP 
IN   NP 
jj 
 formal 
 N 
      style 
Arg. 1 
Sub-Categorization Kernel (SCF) 
[Moschitti, ACL 2004] 
S 
N 
NP 
D N 
VP 
V Paul 
in 
delivers 
a    talk 
PP 
IN   NP 
jj 
 formal 
 N 
      style 
Arg. 1 
Arg. M 
Arg. 0 
Predicate 
57
Experiments on Gold Standard Trees 
 â€¯ PropBank and PennTree bank 
 â€¯ about 53,700 sentences 
 â€¯ Sections from 2 to 21 train., 23 test., 1 and 22 dev. 
 â€¯ Arguments from Arg0 to Arg5, ArgA and ArgM for 
    a total of 122,774 and 7,359 
 â€¯ FrameNet and Collinsâ€™ automatic trees 
 â€¯ 24,558 sentences from the 40 frames of Senseval 3 
 â€¯ 18 roles (same names are mapped together) 
 â€¯ Only verbs  
 â€¯ 70% for training and 30% for testing 
Argument Classification with Poly Kernel 
58
PropBank Results 
Argument Classification on PAT using 
different Tree Fragment Extractor 
0.75
0.78
0.80
0.83
0.85
0.88
0 10 20 30 40 50 60 70 80 90 100
% Training Data
Accuracy   --ST SST
Linear PT
59
FrameNet Results 
 â€¯ ProbBank arguments vs. Semantic Roles  
Kernel Engineering: Node marking 
60
Marking Boundary nodes 
Node Marking Effect  
61
Different tailoring and marking 
CMST 
MMST 
Experiments 
 â€¯ PropBank and PennTree bank 
 â€¯ about 53,700 sentences 
 â€¯ Charniak trees from CoNLL 2005 
 â€¯ Boundary detection: 
 â€¯ Section 2 training 
 â€¯ Section 24 testing 
 â€¯ PAF and MPAF 
62
Number of examples/nodes of Section 2 
Predicate Argument Feature (PAF) vs. 
Marked PAF (MPAF) [Moschitti et al, ACL-ws-2005] 
63
Merging of Kernels [ECIR 2007]: 
Question/Answer Classification 
 â€¯ Syntactic/Semantic Tree Kernel 
 â€¯ Kernel Combinations 
 â€¯ Experiments 
Merging of Kernels [Bloehdorn & Moschitti, ECIR 
2007 & CIKM 2007] 
64
Merging of Kernels 
NP
 
D
 
N
 
VP
 
V
 
   gives
 
a
 
  talk
 
N
 
 good 
 
NP
 
D
 
N
 
VP
 
V
 
   gives
 
a
 
  talk
 
N
 
  solid
 
Delta Evaluation is very simple 
65
Question Classification 
 â€¯ Definition: What does HTML stand for?    
 â€¯ Description: What's the final line in the Edgar Allan Poe 
poem "The Raven"?  
 â€¯ Entity: What foods can cause allergic reaction in people? 
 â€¯ Human: Who won the Nobel Peace Prize in 1992?  
 â€¯ Location: Where is the Statue of Liberty?    
 â€¯ Manner: How did Bob Marley die?     
 â€¯ Numeric: When was Martin Luther King Jr. born?  
 â€¯ Organization: What company makes Bentley cars? 
Question Classifier based on Tree Kernels 
 â€¯ Question dataset (http://l2r.cs.uiuc.edu/~cogcomp/Data/QA/QC/)   
[Lin and Roth, 2005]) 
 â€¯ Distributed on 6 categories: Abbreviations, Descriptions, Entity, 
Human, Location, and Numeric. 
 â€¯ Fixed split 5500 training and 500 test questions  
 â€¯ Cross-validation (10-folds) 
 â€¯ Using the whole question parse trees 
 â€¯ Constituent parsing 
 â€¯ Example 
        â€œWhat is an offer of direct stock purchase plan ?â€ 
66
Kernels 
 â€¯ BOW, POS are obtained with a simple tree, e.g. 
 â€¯ PT (parse tree) 
 â€¯ PAS (predicate argument structure) 
â€¦ 
BOX 
is What 
an 
offer an 
* * 
* * * 
67
Question classification 
Similarity based on WordNet 
68
Question Classification with S/STK 
Multiple Kernel Combinations 
69
TASK: Question/Answer 
Classification [Moschitti, CIKM 2008] 
 â€¯ The classifier detects if a pair (question and 
answer) is correct or not 
 â€¯ A representation for the pair is needed 
 â€¯ The classifier can be used to re-rank the output of 
a basic QA system 
Dataset 2: TREC data 
 â€¯ 138 TREC 2001 test questions labeled as 
â€œdescriptionâ€  
 â€¯ 2,256 sentences, extracted from the best ranked 
paragraphs (using a basic QA system based on 
Lucene search engine on TREC dataset) 
 â€¯  216 of which labeled as correct by one annotator 
70
Dataset 2: TREC data 
 â€¯ 138 TREC 2001 test questions labeled as 
â€œdescriptionâ€  
 â€¯ 2,256 sentences, extracted from the best ranked 
paragraphs (using a basic QA system based on 
Lucene search engine on TREC dataset) 
 â€¯  216 of which labeled as correct by one annotator 
A question is linked to many answers: all its derived 
pairs cannot be shared by training and test sets 
Bags of words (BOW) and POS-tags (POS) 
 â€¯ To save time, apply STK to these trees: 
â€¦ 
BOX 
is What an offer of 
* * 
* * * 
â€¦ 
BOX 
VBZ WHNP 
DT 
NN IN 
* * 
* * * 
71
Word and POS Sequences 
 â€¯ What is an offer ofâ€¦? (word sequence, WSK) 
 ïƒ¨ What_is_offer 
 ïƒ¨ What_is 
 â€¯ WHNP VBZ DT NN INâ€¦(POS sequence, POSSK) 
 ïƒ¨ WHNP_VBZ_NN 
 ïƒ¨ WHNP_NN_IN 
Syntactic Parse Trees (PT) 
72
Predicate Argument Structure for Partial 
Tree Kernel (PAS
PTK
) 
 â€¯ [ARG1 Antigens] were [AMâˆ’TMP originally] [rel defined] [ARG2 as non-
self molecules]. 
 â€¯ [ARG0 Researchers] [rel describe] [ARG1 antigens][ARG2 as foreign 
molecules] [ARGMâˆ’LOC in the body] 
Kernels and Combinations 
 â€¯ Exploiting the property: k(x,z) = k
1
(x,z)+k
2
(x,z) 
 â€¯ BOW, POS, WSK, POSSK, PT, PAS
PTK
 
â‡’ BOW+POS, BOW+PT, PT+POS, â€¦ 
73
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
74
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
75
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
76
Results on TREC Data 
(5 folds cross validation) 
20 
22 
24 
26 
28 
30 
32 
34 
36 
38 
40 
BOW 
POS 
POS_SK 
WSK 
PT
 
P
AS_SSTK P
AS_PTK 
BOW+POS 
BOW+PT
 
POS_SK+PT
 
WSK+PT
 
POS_SK+PT+P
AS_SSTK 
POS_SK+PT+P
AS_PTK 
F1-measure 
Kernel Type 
BOW â‰ˆ 24 
POSSK+STK+PAS_PTKâ‰ˆ 39 
â‡’62 % of improvement 
Kernels for Re-ranking 
77
Re-ranking Framework 
 â€¯ Local classifier generates the most likely set of 
hypotheses. 
 â€¯  These are used to build annotation pairs,           . 
 â€¯ positive instances if h
i
 more correct than h
j, 
 â€¯ A binary classifier decides if h
i
 is more accurate 
than h
j
.  
 â€¯ Each candidate annotation h
i
 is described by a 
structural representation 
â‚¬ 
h
i,h
j
Re-ranking framework 
Local Model 
78
Syntactic Parsing Re-ranking 
 â€¯ Pairs of parse trees (Collins and Duffy, 2002) 
Re-ranking concept labeling 
[Dinarelli et al, 2009] 
 â€¯ I have a problem with my monitor 
h
i
: I NULL have NULL a NULL problem PROBLEM-
B with NULL my NULL monitor HW-B 
h
j
: I NULL have NULL a NULL problem HW-B 
with NULL my NULL monitor 
79
Flat tree representation  
(cross-language structure) 
Multilevel Tree 
80
Enriched Multilevel Tree 
 â€¯ FST CER from 23.2 to 16.01 
Re-ranking for Named-Entity 
Recognition [Vien et al, 2010] 
 â€¯ CRF F1 from 84.86 to 88.16 
81
Re-ranking Predicate Argument Structures 
[Moschitti et al, CoNLL 2006] 
 â€¯ SVMs F1 from 75.89 to 77.25 
Conclusions 
 â€¯ Kernel methods and SVMs are useful tools to design 
language applications 
 â€¯ Kernel design still requires some level of expertise 
 â€¯ Engineering approaches to tree kernels 
 â€¯ Basic Combinations 
 â€¯ Canonical Mappings, e.g. 
 â€¯ Node Marking 
 â€¯ Merging of kernels in more complex kernels 
 â€¯ Easy modeling produces state-of-the-art accuracy in many 
tasks, RTE, SRL, QC, NER, RE 
 â€¯ SVM-Light-TK efficient tool to use them 
82
Future (on going work) 
 â€¯ Once we have found the right kernel, are we satisfied? 
 â€¯ What about knowing the most relevant features? 
 â€¯ Can we speed up learning/classification at real-application 
scenario level? 
 â€¯ The answer is reverse kernel engineering: 
 â€¯ [Pighin&Moschitti, CoNLL2009, EMNLP2009, CoNLL2010] 
 â€¯ Mine the most relevant fragments according to SVMs gradient 
 â€¯ Use the linear space 
 â€¯ Software for reverse kernel engineering available in the 
next  months 
Thank you 
83
References 
 â€¯ Alessandro Moschitti and Silvia Quarteroni, Linguistic Kernels for Answer Re-ranking in 
Question Answering Systems, Information and Processing Management, ELSEVIER,
2010. 
 â€¯ Yashar Mehdad, Alessandro Moschitti and Fabio Massimo Zanzotto. Syntactic/
Semantic Structures for Textual Entailment Recognition. Human Language Technology 
North American chapter of the Association for Computational Linguistics (HLT-
NAACL), 2010, Los Angeles, Calfornia. 
 â€¯ Daniele Pighin and Alessandro Moschitti. On Reverse Feature Engineering of Syntactic 
Tree Kernels. In Proceedings of the 2010 Conference on Natural Language Learning, 
Upsala, Sweden, July 2010. Association for Computational Linguistics.  
 â€¯ Thi Truc Vien Nguyen, Alessandro Moschitti and Giuseppe Riccardi. Kernel-based 
Reranking for Entity Extraction. In proceedings of the 23
rd
 International Conference on 
Computational Linguistics (COLING), August 2010, Beijing, China. 
References 
 â€¯ Alessandro Moschitti. Syntactic and semantic kernels for short text pair categorization. 
In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 
2009), pages 576â€“584, Athens, Greece, March 2009. Association for Computational 
Linguistics.  
 â€¯ Truc-Vien Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. Convolution kernels 
on constituent, dependency and sequential structures for relation extraction. In 
Proceedings of the Conference on Empirical Methods in Natural Language Processing, 
pages 1378â€“1387, Singapore, August 2009. Association for Computational Linguistics.  
 â€¯ Marco Dinarelli, Alessandro Moschitti, and Giuseppe Riccardi. Re-ranking models 
based-on small training data for spoken language understanding. In Proceedings of the 
Conference on Empirical Methods in Natural Language Processing, pages 1076â€“1085, 
Singapore, August 2009. Association for Computational Linguistics.  
 â€¯ Alessandra Giordani and Alessandro Moschitti. Syntactic Structural Kernels for Natural 
Language Interfaces to Databases. In ECML/PKDD, pages 391â€“406, Bled, Slovenia, 
2009. 
84
References 
 â€¯ Alessandro Moschitti, Daniele Pighin and Roberto Basili. Tree Kernels for Semantic 
Role Labeling, Special Issue on Semantic Role Labeling, Computational Linguistics 
Journal. March 2008. 
 â€¯ Fabio Massimo Zanzotto, Marco Pennacchiotti and Alessandro Moschitti, A Machine 
Learning Approach to Textual Entailment Recognition, Special Issue on Textual 
Entailment Recognition, Natural Language Engineering, Cambridge University Press., 
2008 
 â€¯ Mona Diab, Alessandro Moschitti, Daniele Pighin, Semantic Role Labeling Systems for 
Arabic Language using Kernel Methods. In proceedings of the 46th Conference of the 
Association for Computational Linguistics (ACL'08). Main Paper Section. Columbus, 
OH, USA, June 2008.  
 â€¯ Alessandro Moschitti, Silvia Quarteroni, Kernels on Linguistic Structures for Answer 
Extraction. In proceedings of the 46th Conference of the Association for Computational 
Linguistics (ACL'08). Short Paper Section. Columbus, OH, USA, June 2008.  
References 
 â€¯ Yannick Versley, Simone Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, 
Jason Smith, Xiaofeng Yang and Alessandro Moschitti, BART: A Modular Toolkit for 
Coreference Resolution, In Proceedings of the Conference on Language Resources 
and Evaluation, Marrakech, Marocco, 2008.  
 â€¯ Alessandro Moschitti, Kernel Methods, Syntax and Semantics for Relational Text 
Categorization. In proceeding of ACM 17th Conference on Information and Knowledge 
Management (CIKM). Napa Valley, California, 2008.  
 â€¯ Bonaventura Coppola, Alessandro Moschitti, and Giuseppe Riccardi. Shallow semantic 
parsing for spoken language understanding. In Proceedings of HLT-NAACL Short 
Papers, pages 85â€“88, Boulder, Colorado, June 2009. Association for Computational 
Linguistics.  
 â€¯ Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for 
Relational Learning from Texts, Proceedings of The 24th Annual International 
Conference on Machine Learning  (ICML 2007). 
85
References 
 â€¯ Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, 
Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, 
Proceedings of the 45th Conference of the Association for Computational Linguistics 
(ACL), Prague, June 2007. 
 â€¯ Alessandro Moschitti and Fabio Massimo Zanzotto, Fast and Effective Kernels for 
Relational Learning from Texts, Proceedings of The 24th Annual International 
Conference on Machine Learning  (ICML 2007), Corvallis, OR, USA. 
 â€¯ Daniele Pighin, Alessandro Moschitti and Roberto Basili, RTV: Tree Kernels for 
Thematic Role Classification, Proceedings of the 4th International Workshop on 
Semantic Evaluation (SemEval-4), English Semantic Labeling, Prague, June 2007. 
 â€¯ Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semanitc 
Kernels for Text Classification, to appear in the 29th European Conference on 
Information Retrieval (ECIR), April 2007, Rome, Italy. 
 â€¯ Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, 
Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on 
Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007 
References 
 â€¯ Alessandro Moschitti, Silvia Quarteroni, Roberto Basili and Suresh Manandhar, 
Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification, 
Proceedings of the 45th Conference of the Association for Computational Linguistics 
(ACL), Prague, June 2007. 
 â€¯ Alessandro Moschitti, Giuseppe Riccardi, Christian Raymond, Spoken Language 
Understanding with Kernels for Syntactic/Semantic Structures, Proceedings of IEEE 
Automatic Speech Recognition and Understanding Workshop (ASRU2007), Kyoto, 
Japan, December 2007  
 â€¯ Stephan Bloehdorn and Alessandro Moschitti, Combined Syntactic and Semantic 
Kernels for Text Classification, to appear in the 29th European Conference on 
Information Retrieval (ECIR), April 2007, Rome, Italy.  
 â€¯ Stephan Bloehdorn, Alessandro Moschitti: Structure and semantics for expressive text 
kernels. In proceeding of ACM 16th Conference on Information and   Knowledge 
Management (CIKM-short paper) 2007: 861-864, Portugal.  
86
References 
 â€¯ Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, 
Efficient Kernel-based Learning for Trees, to appear in the IEEE Symposium on 
Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii, 2007.  
 â€¯ Alessandro Moschitti, Efficient Convolution Kernels for Dependency and Constituent 
Syntactic Trees. In Proceedings of the 17th European Conference on Machine 
Learning, Berlin, Germany, 2006. 
 â€¯ Fabio Aiolli, Giovanni Da San Martino, Alessandro Sperduti, and Alessandro Moschitti, 
Fast On-line Kernel Learning for Trees, International Conference on Data Mining 
(ICDM) 2006 (short paper).  
 â€¯ Stephan Bloehdorn, Roberto Basili, Marco Cammisa, Alessandro Moschitti, Semantic 
Kernels for Text Classification based on Topological Measures of Feature Similarity. In 
Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 06), Hong 
Kong, 18-22 December 2006. (short paper). 
References 
 â€¯ Roberto Basili, Marco Cammisa and Alessandro Moschitti, A Semantic Kernel to 
classify texts with very few training examples, in Informatica, an international journal of 
Computing and Informatics, 2006.  
 â€¯ Fabio Massimo Zanzotto and Alessandro Moschitti, Automatic learning of textual 
entailments with cross-pair similarities. In Proceedings of COLING-ACL, Sydney, 
Australia, 2006.  
 â€¯ Ana-Maria Giuglea and Alessandro Moschitti, Semantic Role Labeling via FrameNet, 
VerbNet and PropBank. In Proceedings of COLING-ACL, Sydney, Australia, 2006.  
 â€¯ Alessandro Moschitti, Making tree kernels practical for natural language learning. In 
Proceedings of the Eleventh International Conference on European Association for 
Computational Linguistics, Trento, Italy, 2006. 
 â€¯ Alessandro Moschitti, Daniele Pighin and Roberto Basili. Semantic Role Labeling via 
Tree Kernel joint inference. In Proceedings of the 10th Conference on Computational 
Natural Language Learning, New York, USA, 2006.  
87
References 
 â€¯ Roberto Basili, Marco Cammisa and Alessandro Moschitti, Effective use of Wordnet 
semantics via kernel-based learning. In Proceedings of the 9th Conference on 
Computational Natural Language Learning (CoNLL 2005), Ann Arbor (MI), USA, 2005  
 â€¯ Alessandro Moschitti, A study on Convolution Kernel for Shallow Semantic Parsing. In 
proceedings of the 42-th Conference on Association for Computational Linguistic 
(ACL-2004), Barcelona, Spain, 2004. 
 â€¯ Alessandro Moschitti and Cosmin Adrian Bejan, A Semantic Kernel for Predicate 
Argument Classification. In proceedings of the Eighth Conference on Computational 
Natural Language Learning (CoNLL-2004), Boston, MA, USA, 2004.  
An introductory book on SVMs, Kernel 
methods and Text Categorization 
88
Non-exhaustive reference list from other 
authors 
 â€¯ V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995. 
 â€¯ P. Bartlett and J. Shawe-Taylor, 1998. Advances in Kernel Methods -
Support Vector Learning, chapter Generalization Performance of 
Support Vector Machines and other Pattern Classifiers. MIT Press. 
 â€¯ David Haussler. 1999. Convolution kernels on discrete structures. 
Technical report, Dept. of Computer Science, University of California at 
Santa Cruz. 
 â€¯ Lodhi, Huma, Craig Saunders, John Shawe Taylor, Nello Cristianini, 
and Chris Watkins. Text classification using string kernels. JMLR,2000 
 â€¯ SchÃ¶lkopf, Bernhard and Alexander J. Smola. 2001. Learning with 
Kernels: Support Vector Machines, Regularization, Optimization, and 
Beyond. MIT Press, Cambridge, MA, USA. 
Non-exhaustive reference list from other 
authors 
 â€¯ N. Cristianini and J. Shawe-Taylor, An introduction to support vector 
machines (and other kernel-based learning methods) Cambridge 
University Press, 2002 
 â€¯ M. Collins and N. Duffy, New ranking algorithms for parsing and 
tagging: Kernels over discrete structures, and the voted perceptron. In 
ACL02, 2002. 
 â€¯ Hisashi Kashima and Teruo Koyanagi. 2002. Kernels for semi-
structured data. In Proceedings of ICMLâ€™02. 
 â€¯ S.V.N. Vishwanathan and A.J. Smola. Fast kernels on strings and 
trees. In Proceedings of NIPS, 2002. 
 â€¯ Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean Michel 
Renders. 2003. Word sequence kernels. Journal of Machine Learning 
Research, 3:1059â€“1082. D. Zelenko, C. Aone, and A. Richardella. 
Kernel methods for relation extraction. JMLR, 3:1083â€“1106, 2003. 
89
Non-exhaustive reference list from other 
authors 
 â€¯ Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based 
text analysis. In Proceedings of ACLâ€™03. 
 â€¯ Dell Zhang and Wee Sun Lee. 2003. Question classification using 
support vector machines. In Proceedings of SIGIRâ€™03, pages 26â€“32. 
 â€¯ Libin Shen, Anoop Sarkar, and Aravind k. Joshi. Using LTAG Based 
Features in Parse Reranking. In Proceedings of EMNLPâ€™03, 2003 
 â€¯ C. Cumby and D. Roth. Kernel Methods for Relational Learning. In 
Proceedings of ICML 2003, pages 107â€“114, Washington, DC, USA, 
2003. 
 â€¯ J. Shawe-Taylor and N. Cristianini. Kernel Methods for Pattern 
Analysis. Cambridge University Press, 2004. 
 â€¯ A. Culotta and J. Sorensen. Dependency tree kernels for relation 
extraction. In Proceedings of the 42
nd
 Annual Meeting on ACL, 
Barcelona, Spain, 2004. 
Non-exhaustive reference list from other 
authors 
 â€¯ Kristina Toutanova, Penka Markova, and Christopher Manning. The 
Leaf Path Projection View of Parse Trees: Exploring String Kernels for 
HPSG Parse Selection. In Proceedings of EMNLP 2004. 
 â€¯ Jun Suzuki and Hideki Isozaki. 2005. Sequence and Tree Kernels with 
Statistical Feature Mining. In Proceedings of NIPSâ€™05. 
 â€¯ Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting based 
parse reranking with subtree features. In Proceedings of ACLâ€™05. 
 â€¯ R. C. Bunescu and R. J. Mooney. Subsequence kernels for relation 
extraction. In Proceedings of NIPS, 2005. 
 â€¯ R. C. Bunescu and R. J. Mooney. A shortest path dependency kernel 
for relation extraction. In Proceedings of EMNLP, pages 724â€“731, 
2005. 
 â€¯ S. Zhao and R. Grishman. Extracting relations with integrated 
information using kernel methods. In Proceedings of the 43rd Meeting 
of the ACL, pages 419â€“426, Ann Arbor, Michigan, USA, 2005. 
90
Non-exhaustive reference list from other 
authors 
 â€¯ J. Kazama and K. Torisawa. Speeding up Training with Tree Kernels for 
Node Relation Labeling. In Proceedings of EMNLP 2005, pages 137â€“
144, Toronto, Canada, 2005. 
 â€¯ M. Zhang, J. Zhang, J. Su, , and G. Zhou. A composite kernel to extract 
relations between entities with both flat and structured features. In 
Proceedings of COLING-ACL 2006, pages 825â€“832, 2006. 
 â€¯ M. Zhang, G. Zhou, and A. Aw. Exploring syntactic structured features 
over parse trees for relation extraction using kernel methods. 
Information Processing and Management, 44(2):825â€“832, 2006. 
 â€¯ G. Zhou, M. Zhang, D. Ji, and Q. Zhu. Tree kernel-based relation 
extraction with context-sensitive structured parse tree information. In 
Proceedings of EMNLP-CoNLL 2007, pages 728â€“736, 2007. 
Non-exhaustive reference list from other 
authors 
 â€¯ Ivan Titov and James Henderson. Porting statistical parsers with data-
defined kernels. In Proceedings of CoNLL-X, 2006 
 â€¯ Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring Syntactic Features 
for Relation Extraction using a Convolution tree kernel. In Proceedings 
of NAACL. 
 â€¯ M. Wang. A re-examination of dependency path kernels for relation 
extraction. In Proceedings of the 3rd International Joint Conference on 
Natural Language Processing-IJCNLP, 2008. 
91

