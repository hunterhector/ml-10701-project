ments m languages other than Enghsh for summanzatlon either into then" native language or into Enghsh 6 Acknowledgments The author is grateful to Donna Harman and Beth Sundhelm for then" support and assmance m designing the evaluauon The views expressed m this paper are those of the author and do not necessarily reflect the views • of the Department of Defense or any of its agencies References Chmatsu Aone, Mary Ellen Okurowska, James Gorhnsky, and Bjomar Larsen 1997 A Scalable Summarization System Usmg Robust NLP In Proceedings of ACL-97, Madrid, Spmn, July To appear " Ronald Brandow, Karl Mltze, and Lisa F Rau 1995 Automatic Condensation of Electromc Pubhcations by Sentence Selection Information Processmg and Management, 31 (5) 675-685 Kenneth W Church and Lisa F Ran 1995 Commercial Apphcations of Natural Language Processmg Commumcattons of the ACM, 38(11) 71-79 H P Edmundson 1969 New Methods m Automattc Abstracting Journal of the ACM, 16(2) 264-285 Bngette Endres-Nlggemeyer, Jerry Hobbs, and Karen Sparck Jones 1993 Summarizing Text for Intelhgent Commumcation In Dagstuhl Seminar Report, IBFI GmbH, Schloss Dagstuhl, Wadem, Germany J R GaUlers and Karen Sparck Jones.
W97-0711
A Proposal for Task-based Evaluation of Text Summarization Systems Th~rtse Firmin Hand Department of Defense 9800 Savage Rd Ft Meade, MD 20755-6000, USA t fimlnOromulus, ncsc• mil Abstract Evaluauon is a key part of any research and development effort, but the goals and focus of evaluat:ons are often narrow m scope, addressing a specific algonthm or technique, or analyzing a single result All of the evaluation work clone to date on text summarization systems has been by the developers of mdlvldual systems, usually to study and improve sentence selection cntena Under TIPSTER III, DARPA ~s sponsoring a task-based evaluauon of multiple text summarization systems This focus of this evaluation wall be on user needs, and the feaslbdlty of applying summarization technology to a variety of tasks 1 Introduction The explosion of on-lme textual matenal and the advances m text.processing technology have prowded an important opportunity for broad apphcatJon of text summanzauon systems Numerous techmques for denwng summaries from full text documents have already been implemented, and there are several commercial summarization products available The summaries generated by these systems are potenually useful m a variety of settmgs In 1997, the US Government wall begin a Defense Advanced Research Projects Agency (DARPA)-sponsored program under the TIPSTER umbrella to evaluate full text summanzaruon sys-terns to prowde feedback to researchers and commercial msututtons on the utdtty of various approaches to spec~ffic summanzauon tasks TIPSTER, discussed m more detml later, is a DARPA lmtlattve wxth participation from multiple US government agencies.and research and commercial msmut~ons to pushthe stareof the art m text processing technologies 2 Concepts of Text Summarization Automatic summaries are usually descnbed m terms of certain key features which relate to the concepts of intent, focus, and Coverage • Intent describes the potential use of the summary, either mdlcattve or reformative Indlcattve summaries, used m this context, provide just enough mformatlon to judge the relevancy of the full text Informattve or substantttve summanes serve as substztutes for the full documents, retaining all tmportant detads • Focus refers to the scope of the summary, either generic or user-directed A generic summary Is based on the mmn concept(s) of a document, whereas a useror goal-directed summary Is based on the topic of interest indicated by the recipient of the summary • Coverage tn&cates whether the summary is based on a single document or multiple docu" ments Much of the historical work m automatic text summarization has been geared towards the creation of indicative, generic summaries of single documents For example, the work of Luhn (1958), Edmundson (1969), Johnson et al (1993) and Brandow et al (1995) all generated this type of summary, although their approaches have included different combmauons of staustJcal and hngmsttc techmques Luhn (1958) considered frequency of word occurrence within a document 31 and the posmon of the word m a sentence, Edmundson (1969) looked at cue words, taOe and beading words, and structural indicators, Johnsonet al (1993) used md~tbr phrases, and Brandow et al (1995) apphed sentence welghung using signature word selection Most of these approaches claim some degree of domain independence, however they have been tested only on a specific type of data, such as newspaper arucles (Brandow et al 1995) or techmcal hterature (Edmundson 1969) More recently, the scope of research has expanded to include reformative, user-directed, and multi-document summaries Reamer and Hahn (1988), Maybury (1993), and McKeown and Radev (1995) used knowledge-based approaches to generate mformauve surmnanes that can serve as subsututes for the original document The expansion m focus to include user-dtrected summaries has been influenced by research m reformation retrieval cormnumty on passagebased retrieval, as m the work of KhanS et al (1996) Also, advances m stalasUcal learning algorithms, such as those maplemented by Kup~ee et al (1995) and Aone et al (1997) have combined generic surnmartes and user-customtzatlon, allowmg the userto affect the content of the summaries by mampulaung sentence extractaon features The potenual for multt-document summarization as proposed by the work of Strzalkowska (1996) and Mare and Bloedom (1997) is based m part on advances m mformauon retrteval and mformauon extracuon performance 3 Previous Evaluations During the course of their development, most of the above systems were subject to some form of evaluation Many of these evalualaons rehed on the presence of a human-generated target abstract, or the notion of a single 'best' abstract, although there is fairly uniform acceptance of the behef that any number of acceptable abstracts could effectwely represent the content of a single document Human-generated abstracts attempt to capture the central concept(s) of a document using the terminology of the document, along the lines of a generic summary The comparisons made between the human-generated versus machmegenerated summaries were mtended pnmanly for the developers' own benefit, and evaluate the technology ~tself, rather than the utahty of the technology for a given task.Other evaluauons did focus on specific tasks and potenual uses of automatac summaries, but only w~th respect to a single sys• tern and a hrmted document set Many dttferent techmques were attempted m the area of mtrmslc or developer-oriented evaluauons, wluch judge the quahty of summaries Edmundson (1969) compared sentence selection m the automauc abstracts to the target abstracts, and also performed a subjective evaluation of the content Johnson et al (1993) proposed matching a template of manually generated key concepts with the concepts included m the abstract, and performed one sample abstract evaluation Pa~ce and Jones (1993) used a set of statlslacs to deteranne ff the summary effeclavely captured the focal concepts, the non-focal concepts, and conclusions Using a smctly statastlcal measure, Kuplec et al (i995) calculated the percentage of sentence matches and parual matches between thetr automatlc summary and a manually generated abstract The mare problem with this type of evaluation is ~ts rehance on the nouon of a single 'correct' abstract Smce many different representations of a document can form an effecave summary, this is an inappropriate measure In extrinsic or task-oriented evaluations, the mformat~on retrieval notlon of relevancy of a document to a specific topic ~s the common measure for summarization testing Make et al (1994) analyzed key sentence coverage and'also recorded t~rmng and precision/recall stattstles to make relevance decisions based on summaries for a domain-specific summarizer Brandow et al (1995) had news analysts compare the summaries generated using stat~stteal and natural language processing (NLP) techniques to suramanes using the mmal sentences (called the "lead summaries") of the document Brandow et al (1995) discovered that m general, expenenced news analysts felt that the lead summaries were more acceptable than the summartes created using sophisticated NLP teehmques Mare and Bloedom (1997) generated smnlar precision/recall and tmung measures for an mformalaon retrieval experiment usmg a graph search and matching techmque and 32 I I I I I I I I I I I I I I I I I I I Evaluation Quanfitabve Decision Task CatcgonzaUon Adhoc TABLE 1.
W97-0711
