<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization</booktitle>
<pages>65--73</pages>
<contexts>
<context>ch as BiLingual Evaluation Understudy (BLEU) (Papineni et al., 2002), Translation Edit Rate (TER) (Snover et al., 2006), and Metric for Evaluation of Translation with Explicit word Ordering (METEOR) (Banerjee &amp; Lavie, 2005) have been developed and widely used for translations of text and broadcast material, which have very different properties than dialog. The TRANSTAC evaluations provide an opportunity to explore the </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Banerjee, S. and A. Lavie. (2005). METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pp. 65-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Belvin</author>
<author>S Riehemann</author>
<author>K Precoda</author>
</authors>
<title>A Fine-Grained Evaluation Method for Speech-to-Speech Machine Translation Using Concept Annotations</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1427--1430</pages>
<marker>Belvin, Riehemann, Precoda, 2004</marker>
<rawString>Belvin, R., Riehemann, S., and K. Precoda. (2004). A Fine-Grained Evaluation Method for Speech-to-Speech Machine Translation Using Concept Annotations. In Proceedings of LREC 2004, pp. 1427-1430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<date>2006</date>
<contexts>
<context>lated with human judgments for statistical machine translation systems, especially those that have been optimized using BLEU, compared to systems developed using approaches that are not n-gram based (Callison-Burch, Osborne, &amp; Koehn, 2006; Coughlin, 2003; Doddington, 2002). The system that uses rule-based translation for English to Arabic also employs a statistical translation component as a fallback, which made it possible to compute</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Callison-Burch, C., Osborne, M., and P.,Koehn. (2006).</rawString>
</citation>
<citation valid="true">
<title>Re-evaluating the role of BLEU in Machine Translation Research</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>249--256</pages>
<marker>2006</marker>
<rawString>Re-evaluating the role of BLEU in Machine Translation Research. In Proceedings of EACL 2006, pp. 249-256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chatterjee</author>
<author>A Johnson</author>
<author>M Krishna</author>
</authors>
<date>2007</date>
<marker>Chatterjee, Johnson, Krishna, 2007</marker>
<rawString>Chatterjee, N., Johnson, A., and M. Krishna. (2007).</rawString>
</citation>
<citation valid="true">
<title>Some Improvements over the BLEU Metric for Measuring Translation Quality for Hindi</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Computing: Theory and Applications</booktitle>
<pages>485--90</pages>
<marker>2007</marker>
<rawString>Some Improvements over the BLEU Metric for Measuring Translation Quality for Hindi. In Proceedings of the International Conference on Computing: Theory and Applications 2007, pp. 485-90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Coughlin</author>
</authors>
<title>Correlating automated and human assessments of machine translation quality</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX</booktitle>
<pages>63--70</pages>
<contexts>
<context>al machine translation systems, especially those that have been optimized using BLEU, compared to systems developed using approaches that are not n-gram based (Callison-Burch, Osborne, &amp; Koehn, 2006; Coughlin, 2003; Doddington, 2002). The system that uses rule-based translation for English to Arabic also employs a statistical translation component as a fallback, which made it possible to compute scores using bo</context>
</contexts>
<marker>Coughlin, 2003</marker>
<rawString>Coughlin, D. (2003). Correlating automated and human assessments of machine translation quality. In Proceedings of MT Summit IX, pp. 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Culy</author>
<author>S Riehemann</author>
</authors>
<title>The Limits of N-gram Translation Evaluation Metrics</title>
<date>2003</date>
<booktitle>In Proceedings of the MT Summit IX, AMTA</booktitle>
<pages>71--18</pages>
<contexts>
<context>f the limitations that have been identified for BLEU are very general, such as the fact that its precision-based scoring fails to measure recall, rendering it more like a document similarity measure (Culy &amp; Riehemann, 2003; Lavie, Sagae, &amp; Jayaraman, 2004; Owczarzak, van Genabith, &amp; Way, 2007). In addition to BLEU, the TRANSTAC program uses METEOR to score translations of the recorded scenarios with a measure that inco</context>
</contexts>
<marker>Culy, Riehemann, 2003</marker>
<rawString>Culy, C. and S. Riehemann. (2003). The Limits of N-gram Translation Evaluation Metrics. In Proceedings of the MT Summit IX, AMTA, pp. 71-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-Gram Co-Occurrence Statistics</title>
<date>2002</date>
<booktitle>In Proceeding of the Second International Conference on Human Language Technology</booktitle>
<pages>138--145</pages>
<contexts>
<context>lation systems, especially those that have been optimized using BLEU, compared to systems developed using approaches that are not n-gram based (Callison-Burch, Osborne, &amp; Koehn, 2006; Coughlin, 2003; Doddington, 2002). The system that uses rule-based translation for English to Arabic also employs a statistical translation component as a fallback, which made it possible to compute scores using both types of transl</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>Doddington, G.. (2002). Automatic Evaluation of Machine Translation Quality Using N-Gram Co-Occurrence Statistics. In Proceeding of the Second International Conference on Human Language Technology, pp. 138-145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gates</author>
<author>A Lavie</author>
<author>L Levin</author>
<author>A Waibel</author>
<author>M Mayfield Gavalda</author>
<author>L Woszczyna</author>
<author>M</author>
<author>P Zhan</author>
</authors>
<title>End-to-End Evaluation in JANUS: a Speech-to-speech Translation System</title>
<date>1996</date>
<booktitle>In Proceedings of the ECAI Workshop on Dialogue Processing in Spoken Language Systems</booktitle>
<pages>195--206</pages>
<contexts>
<context> involve comparisons to one or more reference translations. In contrast, evaluations of speech translation have relied on human judgments such as the binary or ternary classifications adopted by CMU (Gates et al., 1996) and Verbmobil (Nübel, 1997) researchers, which combine assessments of accuracy and fluency. Other methods use abstract semantic representations of the source utterances and require human judges to s</context>
</contexts>
<marker>Gates, Lavie, Levin, Waibel, Gavalda, Woszczyna, M, Zhan, 1996</marker>
<rawString>Gates, D., Lavie, A., Levin, L., Waibel, A., Gavalda, M. Mayfield, L., Woszczyna, M., and P. Zhan. (1996). End-to-End Evaluation in JANUS: a Speech-to-speech Translation System. In Proceedings of the ECAI Workshop on Dialogue Processing in Spoken Language Systems, pp. 195-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M King</author>
</authors>
<title>Evaluating natural language processing systems</title>
<date>1996</date>
<journal>Communications of the ACM</journal>
<volume>39</volume>
<pages>73--79</pages>
<contexts>
<context>. 2. Previous Work Researchers have recognized that translation quality is multi-faceted and that human judgments of even more specific qualities such as fluency and fidelity are not always reliable (King, 1996; Turian, Shen &amp; Melamed, 2003). Given the unevenness and cost of human judgments, researchers have welcomed automated measures such as BLEU and have proposed a plethora of alternative methods, all of</context>
</contexts>
<marker>King, 1996</marker>
<rawString>King, M. (1996). Evaluating natural language processing systems. Communications of the ACM, 39, pp. 73-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context> Fortunately, some recent work suggests that samples as small as 300 sentences can be sufficient to correctly detect significant differences between systems, though bootstrap sampling is recommended (Koehn, 2004; Zhang &amp; Vogel, 2004). A related concern is the length of the inputs, which has particular importance for TRANSTAC data because spoken utterances tend to be shorter than written ones. For example Tur</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Koehn, P. (2004). Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of EMNLP 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>S Sagae</author>
<author>S Jayaraman</author>
</authors>
<title>The Significance of Recall in Automatic Metrics for MT Evaluation</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004</booktitle>
<pages>134--143</pages>
<contexts>
<context>ave been identified for BLEU are very general, such as the fact that its precision-based scoring fails to measure recall, rendering it more like a document similarity measure (Culy &amp; Riehemann, 2003; Lavie, Sagae, &amp; Jayaraman, 2004; Owczarzak, van Genabith, &amp; Way, 2007). In addition to BLEU, the TRANSTAC program uses METEOR to score translations of the recorded scenarios with a measure that incorporates recall on the unigram le</context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Lavie, A., Sagae, S., and S. Jayaraman. (2004). The Significance of Recall in Automatic Metrics for MT Evaluation. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA-2004), pp. 134–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gates Levin</author>
<author>D Lavie</author>
<author>A Pianesi</author>
<author>F Wallace</author>
<author>D Watanabe</author>
<author>T</author>
<author>M Woszczyna</author>
</authors>
<title>Evaluation of a practical interlingua for task-oriented dialogue</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL-ANLP</booktitle>
<volume>2</volume>
<pages>18--23</pages>
<contexts>
<context>utterances and require human judges to score structural elements of those representations separately. CMU researchers use the interlingua Interchange Format to represent utterance intent and content (Levin et al., 2000), and Belvin, Rieheman, &amp; Precoda (2004) use predicate-argument structures. The TRANSTAC program has experimented with several types of human scoring, and these are described in Sanders et al. (2008)</context>
</contexts>
<marker>Levin, Lavie, Pianesi, Wallace, Watanabe, T, Woszczyna, 2000</marker>
<rawString>Levin, L. Gates, D., Lavie, A., Pianesi, F., Wallace, D., Watanabe, T., and M. Woszczyna. (2000). Evaluation of a practical interlingua for task-oriented dialogue. In Proceedings of the NAACL-ANLP 2000 Workshop on Applied interlinguas: practical applications of interlingual approaches to NLP, Volume 2, pp. 18-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L V Lita</author>
<author>M Rogati</author>
<author>A Lavie</author>
</authors>
<title>BLANC: Learning Evaluation Metrics for MT</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<pages>740--747</pages>
<marker>Lita, Rogati, Lavie, 2005</marker>
<rawString>Lita, L.V., Rogati, M., and A. Lavie. (2005). BLANC: Learning Evaluation Metrics for MT. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pp. 740–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nübel</author>
</authors>
<title>End-to-End Evaluation in Verbmobil I</title>
<date>1997</date>
<booktitle>In Proceedings of MT Summit VI</booktitle>
<pages>232--239</pages>
<contexts>
<context> reference translations. In contrast, evaluations of speech translation have relied on human judgments such as the binary or ternary classifications adopted by CMU (Gates et al., 1996) and Verbmobil (Nübel, 1997) researchers, which combine assessments of accuracy and fluency. Other methods use abstract semantic representations of the source utterances and require human judges to score structural elements of </context>
</contexts>
<marker>Nübel, 1997</marker>
<rawString>Nübel, R. (1997). &amp;quot;End-to-End Evaluation in Verbmobil I,&amp;quot; In Proceedings of MT Summit VI, pp. 232-239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<date>2007</date>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Owczarzak, K., van Genabith, J., and A. Way. (2007).</rawString>
</citation>
<citation valid="true">
<title>Dependency-Based Automatic Evaluation for Machine Translation</title>
<booktitle>In Proceedings of HLT-NAACL 2007 AMTA Workshop on Syntax and Structure in Statistical Translation</booktitle>
<pages>80--87</pages>
<marker></marker>
<rawString>Dependency-Based Automatic Evaluation for Machine Translation. In Proceedings of HLT-NAACL 2007 AMTA Workshop on Syntax and Structure in Statistical Translation, pp. 80-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<date>2002</date>
<contexts>
<context> of interactions. Because the inputs in the offline evaluation are the same for each system, we analyzed translations using automated metrics. Measures such as BiLingual Evaluation Understudy (BLEU) (Papineni et al., 2002), Translation Edit Rate (TER) (Snover et al., 2006), and Metric for Evaluation of Translation with Explicit word Ordering (METEOR) (Banerjee &amp; Lavie, 2005) have been developed and widely used for tra</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and W-J. Zhu. (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bleu</author>
</authors>
<title>A method for automatic evaluation of machine translation</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>311--318</pages>
<marker>Bleu, 2002</marker>
<rawString>Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sanders</author>
<author>S Bronsart</author>
<author>S Condon</author>
<author>C Schlenoff</author>
</authors>
<title>Odds of successful transfer of low-level concepts: A key metric for bidirectional speech-to-speech machine translation in DARPA’s TRANSTAC program</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context>t to assess the progress of system development and evaluate the systems’ readiness for fielding. This report is one of several that describe the evaluation methods developed for the TRANSTAC program (Sanders et al., 2008; Weiss et al., 2008). In the initial stages of development, the focus of evaluations has been on the basic functionality of speech recognition and machine translation, and a major goal has been tests</context>
</contexts>
<marker>Sanders, Bronsart, Condon, Schlenoff, 2008</marker>
<rawString>Sanders, G., Bronsart, S., Condon, S., and C. Schlenoff, (2008).   Odds of successful transfer of low-level concepts:  A key metric for bidirectional speech-to-speech machine translation in DARPA’s TRANSTAC program.  In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
<author>L</author>
</authors>
<marker>Snover, Dorr, Schwartz, Makhoul, L, </marker>
<rawString>Snover, M., Dorr, B., Schwartz, R., Makhoul, J., and L.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micciula</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA</booktitle>
<pages>223--231</pages>
<marker>Micciula, 2006</marker>
<rawString>Micciula. (2006). A Study of Translation Error Rate with Targeted Human Annotation. In Proceedings of AMTA 2006, pp. 223-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Turian</author>
<author>L Shen</author>
<author>I D Melamed</author>
</authors>
<date>2003</date>
<booktitle>Evaluation of Machine Translation and Its Evaluation</booktitle>
<contexts>
<context>riants that may not be represented in reference translations, especially for languages that have relatively free word order (Chatterjee, Johnson &amp; Krishna, 2007; Owczarzak, van Genabith, &amp; Way, 2007; Turian, Shen, &amp; Melamed, 2003). The TRANSTAC program also uses TER to measure translations of the recorded scenarios, which allows for some syntactic variation because any number of contiguous words can shift positions in a singl</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Turian, J.P., Shen, L. and I. D. Melamed. (2003). Evaluation of Machine Translation and Its Evaluation.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<booktitle>In Proceedings of MT</booktitle>
<pages>386--393</pages>
<location>Summit</location>
<contexts>
<context>). A related concern is the length of the inputs, which has particular importance for TRANSTAC data because spoken utterances tend to be shorter than written ones. For example Turian, Shen, &amp; Melamed (2003) report that samples of reference translations from TIDES corpora averaged about 31 words per sentence, whereas 30 words is considered a maximum for inputs to the TRANSTAC speech translation systems. </context>
</contexts>
<marker>2003</marker>
<rawString>In Proceedings of MT Summit 2003, pp. 386-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Weiss</author>
<author>C Schlenoff</author>
<author>G Sanders</author>
<author>M Steves</author>
<author>S Condon</author>
<author>J Phillips</author>
<author>D Parvaz</author>
</authors>
<title>Performance Evaluation of Speech Translation Systems</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context>ss of system development and evaluate the systems’ readiness for fielding. This report is one of several that describe the evaluation methods developed for the TRANSTAC program (Sanders et al., 2008; Weiss et al., 2008). In the initial stages of development, the focus of evaluations has been on the basic functionality of speech recognition and machine translation, and a major goal has been tests incorporating users</context>
<context>formation using each system. Scores were based on a binary human judgment of translation adequacy for inputs produced in 20 ten-minute dialogs. The live evaluations are described in greater detail in Weiss et al., 2008. Figure 1 presents the results of the automated scores for the rerecorded data pooled with the reserved data from the July 2007 evaluation. The five systems that were evaluated are consistently label</context>
</contexts>
<marker>Weiss, Schlenoff, Sanders, Steves, Condon, Phillips, Parvaz, 2008</marker>
<rawString>Weiss, B., Schlenoff, C., Sanders, G., Steves, M., Condon, S., Phillips, J., and Parvaz, D. (2008).  Performance Evaluation of Speech Translation Systems.  In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vo gel</author>
</authors>
<title>Measuring confidence intervals for the machine translation evaluation metrics</title>
<date>2004</date>
<marker>Zhang, gel, 2004</marker>
<rawString>Zhang, Y. and S. Vo gel. (2004). Measuring confidence intervals for the machine translation evaluation metrics.</rawString>
</citation>
</citationList>
</algorithm>

