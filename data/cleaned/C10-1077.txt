Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 680‚Äì688,
Beijing, August 2010
Filtered Ranking for Botstrapping in Event Extraction 
Shasha Liao 
Dept. of Computer Science 
New York University 
liaoss@cs.nyu.edu 
Ralph Grishman 
Dept. of Computer Science 
New York University 
 grishman@cs.nyu.edu 
 
Abstract 
Several researchers have proposed 
semi-supervised learning methods for 
adapting event extraction systems to new 
event types. This paper investigates two 
kinds of bootstrapping methods used for 
event extraction: the document-centric 
and similarity-centric approaches, and 
proposes a filtered ranking method that 
combines the advantages of the two. We 
use a range of extraction tasks to 
compare the generality of this method to 
previous work. We analyze the results 
using two evaluation metrics and 
observe the efect of diferent training 
corpora. Experiments show that our new 
ranking method not only achieves higher 
performance on diferent evaluation 
metrics, but also is more stable acros 
diferent bootstrapping corpora. 
1 Introduction

The goal of event extraction is to identify 
instances of a class of events in text, along with 
the arguments of the event (the participants, 
place, and time). In this paper we shal focus on 
the sub-problem of identifying the events 
themselves. 
Event extraction systems from the early and 
mid 90s relied primarily on hand-coded rules, 
which must be writen anew for every task. 
Since then, supervised and semi-supervised 
methods have ben developed in order to build 
systems for new scenarios more easily. 
Supervised methods can perform quite wel with 
enough training data, but annotating suficient 
data may require months of labor. 
Semi-supervised methods aim to reduce the 
annotated data required, ideally to a small set of 
seeds. 
Most semi-supervised event extractors seek to 
learn sets of patterns consiting of a predicate 
and some lexical or semantic constraints on its 
arguments. The semi-supervised learning was 
based primarily on one of two asumptions: the 
document-centric approach, which assumes that 
relevant paterns should apear more frequently 
in relevant documents (Rilof 196; Yangarber 
et al. 200; Yangarber 203; Surdeanu et al.2006); and the similarity-centric approach, 
which asumes that relevant paterns should 
have lexicaly related terms (Stevenson and 
Grenwood 205, Grenwod and Stevenson 
2006). 
An efective semi-supervised extractor wil 
have good performance over a range of 
extraction tasks and corpora. However, many of 
the learning procedures just cited have ben 
tested on only one or two extraction tasks, so 
their generality is uncertain. To remedy this, we 
have tested learners based on both asumptions, 
targeting both a MUC (Mesage Understanding 
Conference) scenario and several ACE 
(Automatic Content Extraction) event types. We 
identify shortcomings of the prior botstraping 
methods, propose a more efective and stable 
ranking method, and consider the efect of 
diferent corpora and evaluation metrics. 
2 Related
Work 
The basic asumption of the document-centric 
approach is that documents containing a large 
number of paterns already identified as relevant 
to a particular IE scenario are likely to contain 
further relevant paterns. Rilof (196) initiated 
680
this aproach and claimed that if a corpus can be 
divided into documents involving a certain event 
type and those not involving that type, patterns 
can be evaluated based on their frequency in 
relevant and irelevant documents. Yangarber et 
al. (200) incorporated Rilof‚Äôs metric into a 
bootstrapping procedure, which started with 
several seed paterns but required no manual 
document clasification or corpus anotation.  
The sed paterns were used to identify some 
relevant documents, and the top-ranked paterns 
(based on their distribution in relevant and 
irrelevant documents) were aded to the sed 
set. This process was repeated, assigning a 
relevance score to each document based on the 
relevance of the paterns it contains and 
gradualy growing the set of relevant paterns. 
This aproach was further refined by Surdeanu 
et al. (206), who used a co-training strategy in 
which two clasifiers sek to clasify documents 
as relevant to a particular scenario. Patwardhan 
and Rilof (207) prented an informtio 
extraction system that find relvant regions of 
text and aplies extraction paterns within those 
regions. They cretd a slf-trained relevant 
sentence classifier to identify relevant regions, 
and use a semantic affinity measure to 
automatically learn domain-relevant extraction 
paterns. They also distinguish primary paterns 
from secondary paterns and aply the paterns 
selectively in the relevant regions. 
Stevenson and Grenwod (205) (henceforth 
‚ÄòS&G‚Äô) sugested an alternative method for 
ranking the candidate paterns. Their aproach 
relied on the asumption that useful paterns wil 
have similar lexical items to the paterns that 
have already been acepted. They used WordNet 
to calculate word similarity. They chose to 
represent each patern as a vector consisting of 
the lexical items and used a version of the cosine 
metric to determine the similarity betwen pairs 
of paterns. Later, Grenwood and Stevenson 
(206) introduced a structural similarity measure 
that could be aplied to extraction patterns 
consisting of linked dependency chains. 
3 Ranking
Methods in Bootstrapping 
Most semi-supervised event extraction systems 
are based on patterns with variables which have 
semantic type constraints. A simple example is 
‚Äúorganization apoints person as position‚Äù; if 
this pattern matches a pasage in a test 
document, a hiring event wil be instantiated 
with the items matching the variables being the 
arguments of the event. So training an event 
extractor becomes primarily a task of acquiring 
these patterns. In a semi-supervised seting, this 
involves ranking candidate patterns and 
accepting the top-ranked paterns at each 
iteration.  Our goal was to create a more robust 
learner through improved pattern ranking. 
3.1 Problems
of Document-centric 
Bootstraping 
Document-centric botstrapping tries to find 
paterns with high frequency in relevant 
documents and low frequency in irelevant 
documents. The asumption is that descriptions 
of the same event or the same type of event may 
occur multiple times in a document, and so a 
document containing a relevant patern is more 
likely to contain more such patterns. This 
approach may end up extracting patterns for 
related events; for example, start-position often 
comes with end-position events. This efect may 
be salutary if the extraction scenario includes 
these related events (as in MUC-6), but wil pose 
a problem if the goal is to extract individual 
event types. Also, because an extra corpus for 
bootstrapping is needed, diferent corpora might 
perform quite diferently (se Figure 2). 
3.2 Problems
of Similarity-centric 
Bootstraping 
Similarity-centric botstrapping tries to find 
paterns with high lexical similarities. The most 
crucial issue is how to evaluate the similarity of 
two patterns, which is based on the similarity of 
two words. In this strategy, no extra corpus is 
needed, which eliminates the efort to find a 
good bootstrapping corpus, but a semantic 
dictionary that can provide word similarity is 
required. S&G used WordNet
1
 to provide word 
similarity information. However, in the 
similarity-centric approach, lexical polysemy 
can lead the botstrapping down false paths. For 
example, for start-position (hire) events, ‚Äúname‚Äù 
and ‚Äúcharge‚Äù are in the same Synset as appoint, 
but including thes words is quite dangerous 
because they contain other common senses 
                                                             
1
http:/wordnet.princeton.edu/ 
681
unrelated to start-position events. For die events, 
we might have words like ‚Äúgo‚Äù and ‚Äúpass‚Äù, 
which are also used in very specific contexts 
when they refer to ‚Äúdie‚Äù. If similarity-centric 
ranking extracts paterns including these words, 
performance wil deteriorate very quickly, 
because most of the time, these words do not 
predicate the proper event, and more and more 
wrong paterns wil be extracted. 
3.3 Our
Aproach 
We propose a new ranking method, which 
constrains the document-centric and 
similarity-centric assumptions, and makes a 
more restricted asumption: paterns that apear 
in relevant documents and are lexicaly similar 
are most likely to be relevant. This method 
limits the efect of ambiguous patterns by 
narowing the search to relevant documents, and 
limits irrelevant patterns in relevant documents 
by word similarity restriction.  For example, 
although ‚Äúcharge‚Äù has high word similarity to 
‚Äúapoint‚Äù, its document relevance score is very 
low, and we will not include this word in 
bootstrapping starting from ‚Äúappoint‚Äù. 
Many diferent combinations are posible; we 
propose one that uses the word similarity as a 
filter. The document relvance score is first 
applied to rank the patterns in relevant 
documents, then the paterns with lexical 
similarity scores below a similarity threshold 
wil be removed from the ranking; only paterns 
above threshold wil be added to the seeds. 
However, if in the curent iteration, no patern 
meets the threshold, the threshold wil be 
lowered until new patterns can be found. We call 
this ranking method filtered ranking
2
: 
‚Ç¨ 
Filter(p)=
Yangarber(p)Stevnson(p)>=t
0 otherwise
‚éßÔ£± 
‚é®Ô£≤ 
‚é©Ô£≥ 
 
where t is the threshold, which is initialized to 
0.9 in our experiments. 
4 System
Description 
Our aproach is similar to that for 
document-centric botstrapping, but the ranking 
                                                             
2
 We also tried using the product of the document 
relevance score and word similarity score, and found the 
results to be quite similar. Due to space limitations, we do 
not report these results here.  
function is changed to incorporate lexical 
similarity information. For our experiments 
bootstrapping was terminated after a fixed 
number of iterations; in practice, we would 
monitor performance on a held-out (dev-test) 
sample and stop when it declines for k iterations. 
4.1 Pre-procesing 
Instead of limiting ourselves to surface syntactic 
relations, we want to get more general and 
meaningful paterns. To this end, we used 
semantic role labeling (Gildea and Jurafsky, 
2002) to generate the logical gramatical and 
predicate-argument representation automatically 
from a parse tre (Meyers et al. 209). The 
output of the semantic labeling is the 
dependency representation of the text, where 
each sentence is a graph consisting of nodes 
(corresponding to words) and arcs. Each arc 
captures up to three relations between two 
words: (1) a SURFACE relation, the relation 
betwen a predicate and an argument in the 
parse tree of a sentence; (2) a LOGIC1 
(gramatical logical) relation which regularizes 
for lexical and syntactic phenomena like pasive, 
relative clauses, and deleted subjects; and (3) a 
LOGIC2 (predicate-argument) relation 
coresponding to relations in PropBank (Palmer 
et al. 205) and NomBank  
In constructing extraction paterns from this 
graph, we take each dependency link along with 
its predicate-argument role; if that role is nul, 
we use its logical gramatical role, and finaly, 
its surface role. For example, for the sentence: 
John is hit by Tom‚Äôs brother. 
we generate the patterns: 
<Arg1 hit John> 
<Arg0 hit brother> 
<T-pos brother Tom> 
where the first two represent LOGIC2 relations 
and the third a SURFACE relation.  To reduce 
data sparsenes, al inflected words are changed 
to their rot form (e.g. ‚Äúattackers‚Äù‚Üí‚Äúatacker‚Äù), 
and all names are replaced by their ACE type 
(person, organization, location, etc.), so the first 
patern would become 
<Arg1 hit PERSON> 
4.2 Document-based Ranking 
The document-centric method employs a 
682
re-implementation of the procedure described in 
(Yangarber et al. 200), using the disjunctive 
voting scheme for document relevance.  At 
each iteration i we compute a precison score 
Prec
i
(p) for each patern p and a relevance score 
Rel
i
(d) for each document d.  Initially the sed 
paterns have precision 1 and all other patterns 
precision 0.  These are updated by 
‚Ç¨ 
Rel
i
(d)=1‚àí(1‚àíPrec
i
(p))
p‚ààK(d)
‚àè
 
where K(d) is the set of acepted paterns  that 
match document d, and 
‚Ç¨ 
Prec
i+1
(p)=
1
|H(p)|
‚Ä¢Rel
i
(d)
d‚ààH(p)
‚àë
 
where H(p) is the set of documents matching 
patern p.  Patterns are then ranked by 
‚Ç¨ 
RankFun
Yangarber
(p)=
Sup(p)
H(p)
*logSup(p) 
where  
(a generalization of Yangarber‚Äôs metric), and the 
top-ranked candidates are aded to the set of 
accepted patterns. 
4.3 Patern
Similarity  
For two words, there are several ways to 
measure their similarity using WordNet, which 
can be roughly divided into two categories: 
distance-based, including Leacock and 
Chodorow (198), Wu and Palmer (194); and 
information content based, including Resnik 
(195), Lin (198), and Jiang and Conrath 
(197). We follow S&G (2005)‚Äôs method and 
use the semantic similarity of concepts based on 
Information Content (IC)  
Every patern consists of a predicate and a 
constraint (‚Äúargument‚Äù) on its local syntactic 
context, and so the similarity of two patterns 
depends on the similarity of the predicates and 
the similarity of the arguments.  We modified 
S&G‚Äôs structural similarity measure to reflect 
some diferences in patern structure: first, S&G 
only focus on paterns headed by verbs, while 
we include verbs, nouns and adjectives; second, 
they only record the subject and object to a verb, 
while we record al argument relations; third, 
our paterns only contain a predicate and a single 
constraint (argument), while their pattern might 
contain two arguments, subject and object. With 
two arguments, many more patterns are posible 
and the vector siilarity calculation over al 
paterns in a large corpus becomes very time 
consuming. 
We do not limit ourselves to verb paterns 
because nouns and (occasionaly) adjectives can 
also represent an event. For example, 
‚ÄúStevenson‚Äôs promotion is a signal ‚Ä¶‚Äù 
expresses a start-position event. Moreover, in 
our patern, we asume that the predicate is more 
important than constraint, because it is the rot 
(head) of the patern in the semantic graph 
structure, and place diferent weights on 
predicate and constraint. Finaly, the similarity 
of two paterns p
1
 and p
2
 is computed as follows: 
 
where Œ±+Œ≤=1, f reprsents a predicate, r 
represent a role, and a represent an argument. In 
our experiment, Œ± is set to 0.6 and Œ≤ is set to 0.4. 
The role similarity is 1 for identical roles and for 
roles which generaly correspond at the syntactic 
and predicate-argument level (arg0 ‚Üî subj; arg1 
‚Üî obj); selcted other role pairs are asigned a 
smal positive similarity (0.1 or 0.2), and others 
0. 
As with the document-centric method, 
bootstrapping begins by acepting a set of sed 
paterns. At each iteration, the procedure 
computes the similarity between all patterns in 
the training corpus and the curently acepted 
paterns and acepts the most similar patern(s). 
In S&G‚Äôs experiments the evaluation corpus 
also served as the training corpus. 
5 Experiments

There have ben two types of event extraction 
tasks. One involved several ‚Äòelmentary‚Äô event 
types, such as ‚Äúatack‚Äù, ‚Äúdie‚Äù, ‚Äúinjure‚Äù etc.; for 
example, the ACE 205 evaluation
3
 used a set 
of 33 event types and subtypes. The other type 
involved a scenario ‚Äì a set of related events, like 
‚Äúattacks and the damage, injury, and death they 
cause‚Äù, or ‚Äúarrest, trial, sentencing etc.‚Äù. The 
                                                             
3
See htp:/projects.ldc.upenn.edu/ace/docs/English-Events
Guidelines_v5.4.3.pdf for a description of this task. 
683
MUC evaluations include two scenarios that 
have been the subject of considerable research 
on learning methods: terorist incidents 
(MUC-3/4) and executive succession (MUC-6). 
We conducted experiments on the UC-6 
task to make a comparison to previous work. We 
also did experiments on ACE 205 data, because 
it provides many distinct event types; we 
conducted experiments on three disparate event 
types: attack, die, and start-position. Note that 
MUC-6 identifies a scenario while ACE 
identifies specific event types, and types which 
are in the same MUC scenario might represent 
diferent ACE events. For example, the 
executive succession scenario (MUC-6) includes 
the start-position ad end-position events in 
ACE.  
5.1 Data
Description 
There are four corpora used in the experiments: 
MUC-6 corpora 
‚Ä¢ Bootstraping: pre-selected data from the 
Reuters corpus (Rose et al. 202) from 196 
and 197, including 300 related documents 
and 300 randomly chosen unrelated 
documents 
‚Ä¢ Evaluation: MUC-6 annotated data, 
including 20 documents (oficial training 
and test). We were guided by the MUC-6 
key file in annotating every document and 
sentence as relevant or irelevant. 
ACE corpora 
‚Ä¢ Bootstraping: untaged dat from the 
Gigaword corpus from January 206, 
including 14,171 English newswire articles 
from Agence France-Prese (AFP). 
‚Ä¢ Evaluation: ACE 205 anotaed 
(training) data, including 589 documents 
5.2 Parameters
used in Experiments 
In our botstraping proces, we only extract 
paterns appearing more than 2 times in the 
corpus, and the similarity filter threshold is 
originaly set to 0.9. If no paterns are found, it is 
reduced by 0.1 until new paterns are found.  
In each iteration, the top 3 paterns in the 
ranking function wil be aded to the seds. 
For the similarity-centric method, only 
paterns appearing more than 2 times and in les 
than 30% of the documents will be extracted, 
which is the same as S&G‚Äôs aproach. 
5.3 MUC-6 Experiments 
Our overal goal was to demonstrate that filtered 
ranking was in al cases competitive with and in 
at least some cases clearly superior to the earlier 
methods, over a range of extraction tasks and 
bootstrapping corpora. We began with the 
MUC-6 task, where the eficacy of the earlier 
methods had already ben demonstrated. 
 
< Arg0 resign Person > 
< Arg1 appoint Person > 
< Arg0 appoint Org_commercial> 
<Arg1 succed Person > 
Table 1. Seds for MUC-6 evaluation 
 
For MUC-6 evaluation, we follow S&G‚Äôs 
approach and assess extraction patterns by their 
ability to identify event-relevant sentences.
4
 The 
system treats a sentence as relevant if it matches 
an extraction pattern. Botstrapping starts from 
four seds which yield 80% precision and 24% 
recal for sentence filtering.  
To compare with previous work, we tested the 
filtered ranking method on two corpora: the first 
is the Reuters corpus used in S&G‚Äôs recreation 
of Yangarber‚Äôs experiment (Filter1), to compare 
with their results for the document-centric 
method; the second uses the test corpus as S&G 
did (Filter2), to compare with their results for 
the similarity-centric method. We compare 
methods based on peak F score; in practice, this 
would mean controling the botstraping using 
a held-out test sample.  
 
 
Figure 1. F score for different ranking methods on 
MUC-6 evaluation 
 
Figure 1 showed that the filtered ranking 
                                                             
4
 We also tried the document filtering evaluation 
introduced by Yangarber but, as S&G observed, this metric 
is too insensitive because over 50% of the documents in the 
MUC-6 test set are relevant. 
684
methods edge out both document and 
similarity-centric methods.  Our scores are 
comparable to S&G‚Äôs, although they report 
somewhat beter performance for 
similarity-centric than for docuent-centric (5 
vs. 51) whereas document-centric did better for 
us. This diference may reflect diferences in 
patern generation (discussed above) and 
possibly diferences in the specific corpora used. 
However, document-centric botstrapping 
needs an extra corpus for bootstrapping; S&G 
used a pre-selected corpus that contains 
approximately same number of relevant and 
irrelevant documents
5
. We wanted to check if 
such a corpus is essential for the 
document-centric method, and if the need for 
pre-selection can be reduced through filtered 
ranking. Thus, we set up another experiment to 
see if the document-centric method is stable or 
sensitive to diferent corpora. We used two 
additional corpora for MUC-6 evaluation: one is 
a subset of the Wall Street Journal (WSJ) 191 
corpus, which contains 18,734 untagged 
documents; the other is the Gigaword AFP 
corpus described in section 5.1. Both corpora are 
much larger than the Reuters corpus, and while 
we do not have precise information about 
relevant document density, the WSJ contains 
quite a few start-position events because it is 
primarily busines news; the Gigaword corpus 
(AFP newswire) has fewer start-position events 
because it contains a wider variety of news.  
 
 
Figure 2. Document-centric and Filtered ranking 
results on diferent corpora for MUC-6  
 
Figure 2 showed that the document-centric 
method performs quite diferently on diferent 
corpora, which indicates that a pre-selected 
corpus plays an important role in 
                                                             
5
 The pre-selection of relevant and irelevant documents is 
based on document meta-data provided as part of the 
Reuters Corpus Volume I (Rose et al., 202). 
document-centric ranking. It sugests that the 
percentage of relevant documents may be more 
important than the overall corpus size. The 
figure also shows that filtered ranking is much 
more stable acros diferent corpora. Richer 
corpora stil have better peak performance, but 
the difference is not quite as great; also, peak 
performance on a given corpus is consistently 
beter than the document-centric method. 
From the above experiments, we conclude 
that our filtering method is better in two aspects: 
first, botstraping on the same corpus performs 
beter than either document or similarity-centric 
methods; second, if we can not get a corpus with 
an assured high density of relevant documents, it 
is safer to use filtered ranking because it is more 
stable across diferent corpora. 
5.4 ACE205
Experiments 
The ACE205 corpus includes anotations for 
33 diferent event types and subtypes, offering 
us an opportunity to ases the generality of our 
methods acros disparate event types. We 
selected 3 event types to report on here: 
‚Ä¢ Die: ‚Äúocurs whenever the life of a PERSON 
Entity ends. It can be acidental, intentional or 
self-inflicted.‚Äù This event apears 535 times in 
the corpus. 
‚Ä¢ Attack: ‚Äúis defined as a violent physical act 
causing harm or damage.‚Äù Atack events 
include a variety of sub-events like ‚Äúperson 
attack person‚Äù, ‚Äúcountry invade country‚Äù, and 
‚Äúweapons atack locations‚Äù. This event type 
appears 120 times. 
‚Ä¢ Start-Position: ‚Äúocurs whenever a PERSON 
Entity begins working for (or changes ofices 
within) an ORGANIZATION or GPE. This 
includes government oficials starting their 
terms, whether elected or appointed‚Äù. It 
appears 16 times in the corpus. 
We chose these thre event types because 
they reflect the diversity of events ACE 
annotated: die events apear frequently in the 
ACE corpus and its definition is very clear; 
attack events also apear frequently, but its 
definition is rather complicated and contains 
several diferent sub-events; start-position‚Äôs 
definition is clear, but it is relatively infrequent 
in the corpus. 
Based on the observations from the MUC-6 
corpus, we eschewed corpus pre-selection for 
685
two reasons: first, building a different corpus for 
training each event type is an extra burden in 
developing a system for handling multiple 
events; second, we want to demonstrate that 
filtered ranking would work without 
pre-selection, while the document-centric 
method does not. As a result, we used the 
Gigaword AFP corpus for al event types. 
In the ACE 205 corpus, for every event, the 
annotators recorded a triger, which is the main 
word that most clearly expreses an event 
occurrence. This added information alowed us 
to conduct dual evaluations: one based on 
sentence relevance folowing S&G presented 
in section 5.4.2, and one based on trigger 
identification, presented in section 5.4.3. 
5.4.1 ACE2005
Supervised Model 
To provide a benchmark for our semi-supervised 
learners, we built a very simple pattern-based 
supervised learning model. For training, for 
every pattern, we count how many times it 
contains an event triger and how many times it 
does not. If more than 50% of the time it 
contains an event trigger, we treat it as a positive 
patern.  
For sentence level evaluation, if there is a 
positive patern in a sentence, we tag this 
sentence as relevant; otherwise not. For word 
level evaluation, if the word is the predicator of 
a positive pattern, we tag it as a triger; 
otherwise not
6
.  
We did a 5-fold cros-validation on the ACE 
2005 data, report the average results and 
compare it to the semi-supervised learning 
method (se figure 3 & 4). 
5.4.2 Sentence
level ACE Event Evaluation
7
 
Diferent event types have quite diferent 
performance (se figure 3): for the die event, the 
peak performance of al methods is quite good, 
and quite close to the supervised result; for the 
attack event, filterd ranking performs much 
beter than both document and similarity-centric 
                                                             
6
For word-level evaluation, we only consider trigger words 
with at least one semantic argument such as subject, object 
or a preposition; for that reason the performance is quite 
diferent from sentence level evaluation. We did the same 
for the word-level evaluation of semi-supervised learning.  
7
 We do not list Attack seed patterns here as there are 34 
paterns used. 
methods, but stil worse than the supervised 
method; for start-position evnt, th
semi-supervised method beats the supervised 
method. The reason might be as folows: 
Die events apear frequently in ACE 205, 
and most instances corespond to a small 
number of forms, so it is easy to find the correct 
paterns both from WordNet or related 
documents. As a result, filtered ranking provides 
no apparent benefit.  
Attack is a more complicated event including 
several sub-events, which also have a lot of 
related events like die and injure. As a result, the 
document-centric method‚Äôs performance goes 
down much faster, because paterns for related 
event types get drawn in; while the 
similarity-centric method performs worse than 
filtered ranking because some ambiguous words 
are introduced. For example, ‚Äúhit‚Äù is an attack 
trigger, but words in the same Synset, such as 
‚Äúreach‚Äù, ‚Äúmake‚Äù, ‚Äúattain‚Äù, ‚Äúgain‚Äù are quite 
dangerous because most of the time, these words 
do not refer to an attack event. 
Start-position events do not apear frequently 
in ACE 205, and supervised learning canot 
achieve god performance because it can‚Äôt 
collect enough training samples. The 
similarity-centric and Filter2 methods, which 
also depend on the ACE 205 corpus, do not 
perform wel either. Filter1 performs quite wel 
because the Gigaword AFP corpus is quite large 
and contains more relevant documents, although 
the percentage is very small. This confirms our 
assumption that filtered ranking can achieve 
reasonable performance on a large unselected 
corpus, which is especially useful when the 
event is rare in the evaluation corpus. 
 
<Arg1 kil Person> 
<Arg1 slay Person> 
<Arg1 death Person> 
Table 2. Seds for Ace 205 Die evaluation 
 
<Arg0 hire ORG> 
<Arg1 hire Person> 
<Arg1 appoint Person> 
<Arg0 apoint ORG> 
Table 3. Seds for Ace 205 Start-Position 
evaluation 
 
686
 
Figure 3. Performance on diferent ranking methods on ACE205 sentence level evaluation 
 
 
Figure 4. Performance on diferent ranking methods on ACE205 word level evaluation 
 
5.4.3 Word-level ACE vent Evaluation 
Word-level evaluation is different from 
sentence-level evaluation because patterns 
which apear around an event but do not 
predicate an event are penalized in this 
evaluation. For example, the pattern <Sbj 
chairman PERSON>, which arises from a phrase 
like ‚ÄúPERSON was the chairman of
COMPANY‚Äù, appears much more in relevant 
start-position sentences than irelevant 
sentences, and ading this patern to the seeds 
wil improve performance using the 
relevant-sentence metric. We would prefer a 
metric which discounted such paterns. 
As noted above, ACE event anotations 
contain trigers, which are more specific event 
locators than a sentence, and we use this as the 
basis for a more specific evaluation. Extracted 
paterns are used to identify event triggers 
instead of identifying relevant sentences. For 
every word w in the ACE corpus, we extract all 
the patterns whose predicate is w. If the event 
extraction patterns include one of these patterns, 
we tag w as a trigger.  
In word level evaluation, document-centric 
performs worse than the other methods. The 
reason is that some paterns apear often in the 
context of an event and are positive patterns for 
sentence level evaluation, but they do not 
actually predicate an event and are negative 
paterns in word level evaluation. In this 
situation, the document-centric method performs 
worse than the similarity-centric method, 
because it extracts many such paterns. For 
example, of the sentences which contain die 
events, 29% also contain attack events.  
Thus in word level evaluation, filtered 
ranking continues to outperform either 
documentor similarity-centric methods, and its 
advantage over document-centric methods is 
accentuated. 
6 Conclusions

In this paper, we propose a new ranking method 
in botstraping for event extraction and 
investigate the performance on different 
bootstrapping corpora with diferent rankig 
methods. This new method can block some 
irrelevant patterns coming from relevant 
documents, and, by prefering paterns from 
relevant documents, can eliminate some lexical 
ambiguity. Experiments show that this new 
ranking method performs beter than previous 
ranking methods and is more stable acros 
diferent corpora.  
687
References  
D. Gildea and D. Jurafsky. 202. Automatic Labeling 
of Semantic Roles. Computainl Linguistcs, 
28:245‚Äì288. 
MA Grenwod, M. Stevenson. 206. Improving 
semi-supervised acquisition of relation extraction 
patterns. Proengs o the Wrkshop o
Information Extraction Beyond the Document, 
pages 29‚Äì35. 
Jay J. Jiang and David W. Conrath. 197. Semantic 
Similarity Based on Corpus Statistics and Lexical 
Taxonomy. In Procedings of International 
Conference Research on Computational 
Linguistics (ROCLING X), Taiwan 
C. Leacock and M. Chodorow. 198. Combining 
local context and WordNet similarity for word 
sense identification. In C. Febum, edit,
WordNet: An electronic lexical database, pages 
265‚Äì283. MIT Pres. 
D. Lin. 198. An information-theoretic definition of 
similarity. In Procedings of the International 
Conference on Machine Learning, Madison, 
August. 
A. Meyers, M. Kosaka, N. Xue, H. Ji, A. Sun, S. 
Liao and W. Xu. 209. Automatic Recognition of 
Logical Relations for English, Chinese and 
Japanese in the GLARF Framework. In SEW-2009 
(Semantic Evaluations Workshop) at NAACL 
HLT-2009 
MUC. 195. Procedings of the Sixth Mesage 
Understanding Conference (MUC-6), San ateo, 
CA. Morgan Kaufman. 
Martha Palmer, Dan Gildea, and Paul Kingsbury, The 
Proposition Bank: A Corpus Annotated with 
Semantic Roles, Computational Linguistics, 31:, 
2005. 
Sidharth Patwardhan, Satanjev Banerje, and Ted 
Pedersen. 203. Using measures of semantic 
relatedness for word sense disambiguation. In 
Procedings of the Fourth International 
Conference on Inteligent Text Procesing and 
Computational Linguistics, Mexico City, Mexico.  
Patwardhan, S. and Rilof, E. 207. Effective 
Information Extraction with Semantic Afinity 
Paterns and Relevant Regions. Procedings of the 
2007 Conference on Empirical Methods in Natural 
Language Procesing (EMNLP-07) 
T. Pedersen, S. Patwardhan, and J. Michelizi. 2004. 
WordNet:Similarity Measuring the Relatednes 
of Concepts. In Procedings of the Ninetenth 
National Conference on Artificial Inteligence 
(Inteligent Systems Demonstrations), pages 
1024-1025, San Jose, CA, July 2004. 
P. Resnik. 195. Using information content to 
evaluate semantic similarity in a taxonomy. In 
Procedings of the 14th International Joint 
Conference on Artificial Inteligence, pages 
448‚Äì453, Montreal, August. 
Ellen Rilof. 196. Automaticaly Generating 
Extraction Paterns from Untaged Text. In Proc. 
Thirtenth National Conference on Artificial 
Inteligence (AAI-96), 1996, pp. 1044-1049. 
T. Rose, M. Stevenson, and M. Whitehead. 202. The 
Reuters Corpus Volume 1 from Yesterday‚Äôs news 
to tomorow‚Äôs language resources. In LREC-02, 
pages 827‚Äì832, La Palmas, Spain. 
M. Stevenson and M. Grenwod. 205. A Semantic 
Approach to IE Patern Induction. Procedings of 
ACL 205. 
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 
2006. A Hybrid Approach for the Acquisition of 
Information Extraction Paterns. Procedings of 
the EACL 206 Workshop on Adaptive Text 
Extraction and Mining (ATEM 206) 
Z. Wu and M. Palmer. 194. Verb semantics and 
lexical selection. In 32nd Anual Meting of the 
Association for Computational Linguistics, pages 
133‚Äì138, Las Cruces, New Mexico. 
Roman Yangarber; Ralph Grishman; Pasi 
Tapanainen; Silja Hutunen. 200. Automatic 
Acquisition of Domain Knowledge for Information 
Extraction. Proc. COLING 200. 
Roman Yangarber. 203. Counter-Training in 
Discovery of Semantic Paterns. Procedings of 
ACL203  
688

