An Example-Based Chinese Word Segmentation System for CWSB-2 Chunyu Kit Xiaoyue Liu Department of Chinese, Translation and Linguistics City University of Hong Kong Tat Chee Ave., Kowloon, Hong Kong {ctckit, xyliu0}@cityu.edu.hk Abstract This paper reports the example-based segmentation system for our participation in the second Chinese Word Segmentation Bakeoff (CWSB-2), presenting its basic ideas, technical details and evaluation.
It is a preliminary implementation.
CWSB-2 valuation shows that it performs very well in identifying known words.
Its unknown word detection module also illustrates great potential.
However, proper facilities for identifying time expressions, numbers and other types of unknown words are needed for improvement.
1 Introduction
Word segmentation is to identify lexical items, especially individual word forms, in a text.
It involves two fundamental tasks, both aiming at minimizing segmentation errors: one is to infer out-of-vocabulary (OOV) words, also known as unknown (or unseen) word detection, and the other to identify in-vocabulary (IV) words, with an emphasis on disambiguation.
OOV words and ambiguities are the two major causes to segmentation errors.
Accordinglyword segmentation approaches can be divided into the categories summarized in Table 1 in terms of the resources in use to tackle these two causes.
The closed and open tracks in CWSB correspond, respectively, to the last two categories, both involving inferring OOV words Category Resource in use Major Task Lexicon Tr.
Corpus OOV Disamb.
WDa (-)b + WS/CLc + + WS/ILd + + + WS/TCe (+)f + + + WS/TC+Lg + + + + aWord discovery, or unsupervised lexical acquisition bInput data is used for unsupervised training cWord segmentation with a complete lexicon dWord segmentation with an incomplete lexicon eWord segmentation with a pre-segmented training corpus fIt can be extracted from the given training corpus.
gWord segmentation with a pre-segmented training corpus and an extra lexicon Table 1: Categories of segmentation approach beyond disambiguating IV words.
Word discovery and OOV word detection pursue a similar target, i.e., inferring new words.
The continuum connecting them is the size of the lexicon in use: the former assumes few words known and the latter an existing lexicon to some scale.
Inferring new words is an essential task in word segmentation, for a complete lexicon is rarely a realistic assumption in practice.
This paper presents our segmentation system for participation in CWSB-2.
It takes an examplebased approach to recognize IV words and follows description length gain (DLG) to infer OOV words in terms of their text compression effect.
Sections 2 and 3 below introduce the examplebased and DLG-based segmentation respectively.
Section 4 presents a strategy to combine their strength and Section 5 reports our systemâ€™s performance in CWSB-2.
Following error analysis in Section 6, Section 7 concludes the paper.
146 2 Example-based segmentation How to utilize as much information as possible from the training corpus to adapt a segmentation system towards a segmentation standard has been a critical issue.
Kit et al.(2002) and Kit et al.(2003) attempt to integrate case-based learning with statistical models (e.g., n-gram) by extracting transformation rules from the training corpus for disambiguation via error correction; Gao et al.(2004) adopt a similar strategy for adaptive segmentation, with transformation templates (instead of case-based rules) to modify word boundaries (instead of individual words).
The basic idea of example-based segmentation is very simple: existing pre-segmented strings in training corpus provide reliable examples for segmenting similar strings in input texts.
In contrast to dictionary checking for locating possible words in an input sentence to facilitate later segmentation operations, pre-segmented examples give exact segmentation to copy.
The example-based segmentation can be implemented in the following steps.
1. Find all exemplar pre-segmented fragments, with regards to a training corpus, and all possible words, with regards to a lexicon, from each character in an input sentence; 2.
Identify the optimal sequence, among all possibilities, of the above items over the sentence following some optimization criterion.
If adopting the minimal number of fragments or words in a sequence as optimization criterion, we have a maximal matching approach to word segmentation.
However, it differs remarkably from the previous maximal matching approaches: it matches pre-segmented fragments, instead of dictionary words, against an input sentence.
It can be carried out by a best-first strategy: repeatedly select the next longest example or word until the entire sentence is properly covered.
Unfortunately, the best-first approach does not guarantee to give the best answer.
For CWSB-2, we implemented a program following the Viterbi algorithm to perform a complete search in terms of the number of fragments, and then words, in a sequence.
However, a serious problem with this examplebased approach is the sparse data problem.
Long exemplar fragments are more reliable but small in number, whereas short ones are large in number but less reliable.
In the case of no exemplar fragment available for an input sentence, this approach draws back to the maximal match segmentation with a dictionary.
How to incorporate statistical inference into example-based segmentation to infer more reliable optimal segmentation beyond string matching remains a critical issue for us to tackle.
3 DLG-based segmentation DLG is formulated in Kit and Wilks (1999) and Kit (2000) as an empirical measure for the compression effect of extracting a substring from a given corpus as a lexical item.
DLG optimization is applied to detect OOV words for our participation in CWSB-2.
It works as follows in two steps.
1. Calculate the DLG for all known words and all new word candidate (i.e., substrings with frequency â‰¥ 2, preferably, in the test corpus), based on frequency information in the training and the test corpora; 2.
Find the optimal sequence of such items over an input sentence with the greatest sum of DLG.
Step 2 above in our system re-implements only the first round of DLG-based lexical learning in Kit (2000).
It is implemented by the same algorithm as the one for example-based segmentation, with DLG as optimization criterion.
Evaluation results show that this learning-via-compression approach discovers many OOV words successfully, in particular, person names.
4 Integration
The example-based segmentation is good at identifying IV words but incapable of recognizing any new words.
In contrast, the DLG-based segmentation performs slightly worse but has potential to detect new words.
It is expected that the strength of the two could be combined together for performance enhancement.
However, because of inadequate time we had to take a shortcut in order to catch the CWSB2 deadline: DLG segmentation is only applied to recognize new words among the sequences of mono-character items in the example-based segmentation output.
147 Track P R F OOV ROOV RIV ASc.944 .902 .923 .043 .234 .976 PKUc .929 .904 .916 .058 .252 .971 MSRc .965 .935 .950 .026 .189 .986 Table 2: System performance in CWSB-2 5 Performance Our group took part in three closed tracks in CWSB-2, namely, ASc, PKUc and MSRc, with a preliminary implementation of the example-based word segmentation presented above.
Our systemâ€™s performance in terms of CWSB-2â€™s official scores is presented in Table 2.
Its ROOV scores look undesirable, showing that applying the first round of DLG-based segmentation to sequences of mono-character items is inadequate for the OOV word discovery task.
Nevertheless, its RIV scores are, in general, quite close to the top systems in CWSB-2, although it does not have a disambiguation module to polish its maximal matching output.
However, this is not to say that the DLG-based segmentation deserves no credit in unknown word detection.
It does recognize many OOV words, as shown in Table 3.
The low ROOV rate has to do with our systemâ€™s incapability in handling time expressions, numbers, and foreign words.
6 Error
analysis Most errors made by our system are due to the following causes: (1) no knowledge, overt or implicit, in use for recognizing time expressions, numbers and foreign words, as restricted by CWSB-2 rules, (2) a premature module for OOV word detection, (3) no further disambiguation besides example application, and (4) significant inconsistency in the training and test data.
The inconsistency exists not only between the training and test corpora for each track but, more surprisingly, also within individual training corpora.
Some suspected cases are illustrated in Tables 4, 5 and 6.
They are observed to be in a large number in the CWSB-2 corpora.
Scoring with a golden standard involving so many of them appears to be problematic, for it penalizes the systems for handling such cases right and rewards the others for producing â€œcorrectâ€ answers.
What ASc:l ï¿½(106)P!,ï¿½(45) (31) ï¿½(29) jï¿½(21)ï¿½ ï¿½(20)"u V(18)ï¿½+ï¿½(17)ï¿½ ï¿½9(16) +!-M(15)"ï¿½ï¿½\(13)  ï¿½(12)Uï¿½ ï¿½ ï¿½(11) 5 }(11)ï¿½ï¿½  ï¿½(11)~ ï¿½}(10)Io(9)S!ï¿½r(8)R o(8) n ï¿½ ï¿½(7)!ï¿½_(7)ï¿½!ï¿½D(7)"uï¿½ ï¿½(6) ï¿½Bï¿½ï¿½(6)"ï¿½ "1(5)ï¿½ XM(5)ï¿½ï¿½\(5)!Sï¿½9(5)~ï¿½ *(5)ï¿½ ï¿½!~(5)\ï¿½(5)ï¿½ ï¿½ï¿½(5)Rï¿½ 1(4)ï¿½ï¿½(4)ï¿½ ï¿½(4)ï¿½ï¿½ B?(4) b  ï¿½(3)"u =(3) ï¿½ï¿½9Q0(3)~x %(2) b ï¿½(2)ï¿½ï¿½ ï¿½ï¿½ï¿½(2)ï¿½(ï¿½(2) Â· Â· Â· Â· Â· Â· PKUc:ï¿½ ï¿½(38) W b(23) ï¿½ ï¿½(21) ï¿½ ï¿½c:ï¿½(20)W ï¿½(19)Zï¿½ï¿½(17) +)(16)F,(15) ï¿½ï¿½h(12)ï¿½ï¿½(11)lï¿½ h(10) ï¿½ ï¿½(10)sï¿½~(9) #ï¿½(9) oï¿½:(8)ï¿½ ï¿½(8)ï¿½ï¿½ ï¿½(8)ï¿½ ï¿½(7) ï¿½ï¿½(7)ï¿½ï¿½(6)ï¿½ ï¿½ ï¿½(6)E }ï¿½(6)P Z(6)ï¿½ï¿½(6)ï¿½ b g(5) O: X+(5)ï¿½?(5) ï¿½  ï¿½(5) ï¿½v ï¿½(5) (5) ï¿½C(5)J ï¿½(5)W ï¿½r(4)ï¿½ #ï¿½(4)ï¿½E(4)ï¿½ï¿½(4) ï¿½ gï¿½(4) ï¿½C(4)ï¿½ ï¿½(4)>ï¿½(4) (4) "ï¿½E(4)ï¿½ï¿½ï¿½(4) ï¿½*(4)E ï¿½ï¿½(3) @ ï¿½ï¿½(3) Cï¿½ iE(3)ï¿½B(3)ï¿½ï¿½ (3)ï¿½ï¿½(3) +J(3)ï¿½ ï¿½(3)= ï¿½(3) ï¿½~h(3) Â· Â· Â· Â· Â· Â· MSRc:p(26) *ï¿½(19)ï¿½ï¿½ï¿½(19) ï¿½) ï¿½(17) U(15)ï¿½ _(14)ï¿½](14) > ï¿½(13)ï¿½(13)NSYJWSJY(12)ï¿½ ï¿½(12) ï¿½ (11)fy ï¿½(10)ï¿½ï¿½(10)2ï¿½ï¿½:(10)ï¿½== (10)Z (10)ï¿½ï¿½ ï¿½(9)% oE(9)v1h(8)!ï¿½(8)f *(8)ï¿½ Fï¿½(7) ï¿½ ï¿½ ï¿½ï¿½(7) ï¿½ï¿½(7)iï¿½(6) UE ï¿½(6)ï¿½ ï¿½(6)ï¿½ï¿½(6)ï¿½ (6)k (5) ï¿½ï¿½Y(5)49(5)v1(5)ï¿½d )(5) ï¿½ .(5)ï¿½aQ(4) ï¿½(4) ï¿½ï¿½(4) db(4)ï¿½ o ï¿½(4) Uï¿½I(4)1l0(3)=ï¿½ï¿½(3) ï¿½ ï¿½(3) ï¿½ï¿½ï¿½(3)~ lï¿½(3)ï¿½ï¿½ï¿½(3)ï¿½ ï¿½ï¿½(3)ï¿½ï¿½(3)ï¿½ï¿½ (3) d ï¿½ï¿½(3)ï¿½ï¿½E(3)ï¿½ï¿½ ï¿½(3)ï¿½ @(3)ï¿½ï¿½ C(3)ï¿½3 ^o(2) Â· Â· Â· Â· Â· Â· Table 3: Illustration of new words successfully detected, with frequency in parentheses is even more worth noting is that (1) an inconsistent case involves more than one word, and (2) the difference between a correct and an erroneous judgment of a word is 1, in a sense, but the difference between one system that loses it for doing right and another that earns it by doing wrong is surely greater.
7 Conclusions
In the above sections we have reported the example-based word segmentation system for our participation in CWSB-2, including its basic ideas, technical details and evaluation results.
It has illustrated an excellent performance in IV word identification and nice potential in OOV word discovery.
However, its weakness in handling time expressions, numbers and other types of unknown words has hindered it from performing better.
We are expecting to implement a fullfledged version of the system for improvement.
Acknowledgements The work described in this paper was supported by the RGC of HKSAR, China, through the CERG grant 9040861.
We wish to thank Alex Fang and Robert Neather for their help.
148 Training & Answer fT/fA Golden Standard fT/fA Nï¿½s4/8Nï¿½s0/0 ï¿½ï¿½28/7 ï¿½ï¿½0/0 "ï¿½> q5/7"ï¿½> q0/0 l{11/6l{0/0 'ï¿½186/5'ï¿½0/0 ï¿½ ï¿½ï¿½41/4ï¿½ ï¿½ï¿½0/0 ï¿½B}29/4ï¿½B}0/0 ï¿½ï¿½;*129/4ï¿½ï¿½;*0/0 -:ï¿½-:23/3-:ï¿½-:0/0 !-ï¿½47/3!-ï¿½0/0 ï¿½ï¿½hs33/2ï¿½ï¿½hs0/0 ï¿½ gu32/2 ï¿½ gu0/0 ï¿½i}V85/2 ï¿½i}V0/0 bï¿½>ï¿½10/2 bï¿½>ï¿½0/0 !|ï¿½ï¿½62/2!|ï¿½ï¿½0/0 ï¿½ 323/2ï¿½ 30/0 9ï¿½ ^Jï¿½192/19ï¿½ ^Jï¿½0/0 ï¿½ï¿½ ï¿½149/1ï¿½ï¿½ ï¿½0/0 ï¿½ï¿½ï¿½66/1ï¿½ï¿½ï¿½0/0 G"sï¿½31/1G"sï¿½0/0 Aï¿½ï¿½80/1 Aï¿½ï¿½0/0 ï¿½ï¿½ï¿½68/1ï¿½ï¿½ï¿½0/0 ï¿½!ï¿½V f13/1 ï¿½!ï¿½V f0/0 ;ï¿½ ï¿½ï¿½13/1;ï¿½ ï¿½ï¿½0/0 !ï¿½i!020/1!ï¿½i!00/0 !ï¿½Pï¿½!i6/1!ï¿½Pï¿½!i0/0 6B ï¿½29/1 6B ï¿½0/0 Hiï¿½F?4/1Hiï¿½F?0/0 ï¿½ï¿½!ï¿½!t24/7 ï¿½ï¿½!ï¿½!t25/0 oï¿½ï¿½"17/3oï¿½ï¿½"53/0 ^ï¿½ ^1201/2 ^ï¿½ ^2/0 Table 4: Some inconsistent cases in AS corpus Training & Answer fT/fA Golden Standard fT/fA Tï¿½ï¿½14/26Tï¿½ï¿½0/0 uï¿½ï¿½6/1 uï¿½ï¿½0/0 ï¿½ï¿½ï¿½5/21ï¿½ï¿½ï¿½0/0 W@ï¿½ï¿½24/19 W@ï¿½ï¿½0/0 ï¿½ï¿½23/18ï¿½ï¿½0/0 3ï¿½9ï¿½66/15 3ï¿½9ï¿½0/0 ï¿½ ï¿½ ï¿½ï¿½ ï¿½10/9ï¿½ ï¿½ ï¿½ï¿½ ï¿½0/0 ï¿½ï¿½ï¿½ ï¿½ V10/5ï¿½ï¿½ï¿½ ï¿½ V0/0 ï¿½MH S45/5ï¿½MH S0/0 Fï¿½ï¿½s42/5Fï¿½ï¿½s0/0 %ï¿½'ï¿½27/4%ï¿½'ï¿½0/0 Wï¿½ï¿½ï¿½21/4 Wï¿½ï¿½ï¿½0/0 Tï¿½ <126/4Tï¿½ <0/0 ï¿½1ï¿½120/4ï¿½1ï¿½10/0 8ï¿½S ï¿½15/48ï¿½S ï¿½0/0 9ï¿½ï¿½25/49ï¿½ï¿½0/0 ï¿½6 ï¿½25/3ï¿½6 ï¿½0/0 ï¿½ï¿½ 3ï¿½ q13/3 ï¿½ï¿½ 3ï¿½ q0/0 ï¿½Mï¿½32/3 ï¿½Mï¿½0/0 Wï¿½@ c30/3 Wï¿½@ c0/0 ï¿½ï¿½ï¿½11/3ï¿½ï¿½ï¿½0/0 p Y ï¿½ï¿½15/3p Y ï¿½ï¿½0/0 ï¿½l <22/3ï¿½l <0/0 ï¿½]ï¿½11/2ï¿½]ï¿½0/0 N Y25/2N Y0/0  ï¿½ï¿½ï¿½@?3/1 ï¿½ï¿½ï¿½@?0/0 ï¿½Cï¿½13/1ï¿½Cï¿½0/0 Eï¿½ï¿½24/5Eï¿½ï¿½1/0 iï¿½"49/4iï¿½"1/0 ï¿½112/3ï¿½14/0 ?rSE48/1?rSE1/0 Table 5: Some inconsistent cases in PKU corpus Training & Answer fT/fA Golden Standard fT/fA ï¿½<ï¿½12/7ï¿½<ï¿½0/0 ï¿½ï¿½?16/6ï¿½ï¿½?0/0  oï¿½ï¿½29/5 oï¿½ï¿½0/0 vï¿½ <"v6/3vï¿½ <"v0/0 ï¿½ @ 3/3ï¿½ @ 0/0 Zï¿½ï¿½ 1/2Zï¿½ï¿½ 0/0 g'ï¿½? 4/2g'ï¿½0/0 Bï¿½10/2Bï¿½0/0 ï¿½ï¿½3/2ï¿½ï¿½0/0  ï¿½7/1 ï¿½0/0 ï¿½`v2/1ï¿½`v0/0 ï¿½ï¿½SE1/1ï¿½ï¿½SE0/0 ï¿½B a= a ï¿½? 4/1ï¿½B a= a ï¿½0/0 ï¿½ï¿½ï¿½v~1/1ï¿½ï¿½ï¿½v~0/0 S=ï¿½ ï¿½ï¿½ ï¿½1/1S=ï¿½ ï¿½ï¿½ ï¿½0/0 9 ï¿½ï¿½ï¿½ ï¿½q1/19 ï¿½ï¿½ï¿½ ï¿½q0/0 E ï¿½ g? 10/1E ï¿½ g0/0 ï¿½? 16/1 ï¿½0/0 g ? 4/1 g 0/0 B g ? 16/1B g 0/0 ï¿½S ï¿½ ï¿½122/1ï¿½S ï¿½ ï¿½0/0 ï¿½ï¿½ï¿½'ï¿½ï¿½3/1ï¿½ï¿½ï¿½'ï¿½ï¿½0/0 Table 6: Some inconsistent cases in MSR corpus References E.
Brill. 1993.
A Corpus-Based Approach to Language Learning.
PhD thesis, University of Pennsylvania, Philadelphia.
J. Gao, A.
Wu, M.
Li, C.
Huang, H.
Li, X.
Xia and H.
Qin. 2004.
Adaptive Chinese word segmentation.
In ACL-04.
Barcelona, July 21-26.
C. Kit and Y.
Wilks. 1999.
Unsupervised learning of word boundary with description length gain.
In M.
Osborne and E.
T. K.
Sang (eds.), CoNLL-99, pp.16.
Bergen, Norway, June 12.
C. Kit 2000.
Unsupervised Lexical Learning as Inductive Inference.
PhD thesis, University of Sheffield.
C. Kit, H.
Pan and H.
Chen. 2002.
Learning casebased knowledge for disambiguating Chinese word segmentation: A preliminary study.
SIGHAN-1, pp.33&ï¿½39.
Taipei, Sept.
1, 2002.
C. Kit, Z.
Xu and J.
J. Webster.
2003. Integrating n-gram model and case-based learning for Chinese word segmentation.
In Q.
Ma and F.
Xia (eds.), SIGHAN-2, pp.160-163.
Sapporo, 11 July, 2003.
D. Palmer.
A trainable rule-based algorithm for word segmentation.
In ACL-97, pp.321-328.
Madrid. 149

