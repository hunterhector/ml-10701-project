Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 73â€“83,
Edinburgh, Scotland, UK, July 27â€“31, 2011. c 2011 Association for Computational Linguistics
SMT Helps Bitext Dependency Parsing
Wenliang Chenâ€ â€¡, Junâ€™ichi Kazamaâ€¡, Min Zhangâ€ , Yoshimasa Tsuruokaâˆ—â€¡,
Yujie Zhangstarâ€¡, Yiou Wangâ€¡, Kentaro Torisawaâ€¡ and Haizhou Liâ€ 
â€ Human Language Technology, Institute for Infocomm Research, Singapore
â€¡National Institute of Information and Communications Technology (NICT), Japan
âˆ—School of Information Science, JAIST, Japan
starBeijing Jiaotong University, China
{wechen, mzhang, hli}@i2r.a-star.edu.sg
{kazama, torisawa, yujie, wangyiou}@nict.go.jp
tsuruoka@jaist.ac.jp
Abstract
We propose a method to improve the accuracy
of parsing bilingual texts (bitexts) with the
help of statistical machine translation (SMT)
systems. Previous bitext parsing methods use
human-annotated bilingual treebanks that are
hard to obtain. Instead, our approach uses an
auto-generated bilingual treebank to produce
bilingual constraints. However, because the
auto-generated bilingual treebank contains er-
rors, the bilingual constraints are noisy. To
overcome this problem, we use large-scale
unannotated data to verify the constraints and
design a set of effective bilingual features for
parsing models based on the verified results.
The experimental results show that our new
parsers significantly outperform state-of-the-
art baselines. Moreover, our approach is still
able to provide improvement when we use a
larger monolingual treebank that results in a
much stronger baseline. Especially notable
is that our approach can be used in a purely
monolingual setting with the help of SMT.
1 Introduction
Recently there have been several studies aiming to
improve the performance of parsing bilingual texts
(bitexts) (Smith and Smith, 2004; Burkett and Klein,
2008; Huang et al., 2009; Zhao et al., 2009; Chen
et al., 2010). In bitext parsing, we can use the in-
formation based on â€œbilingual constraintsâ€ (Burkett
and Klein, 2008), which do not exist in monolingual
sentences. More accurate bitext parsing results can
be effectively used in the training of syntax-based
machine translation systems (Liu and Huang, 2010).
Most previous studies rely on bilingual treebanks
to provide bilingual constraints for bitext parsing.
Burkett and Klein (2008) proposed joint models on
bitexts to improve the performance on either or both
sides. Their method uses bilingual treebanks that
have human-annotated tree structures on both sides.
Huang et al. (2009) presented a method to train a
source-language parser by using the reordering in-
formation on words between the sentences on two
sides. It uses another type of bilingual treebanks
that have tree structures on the source sentences and
their human-translated sentences. Chen et al. (2010)
also used bilingual treebanks and made use of tree
structures on the target side. However, the bilingual
treebanks are hard to obtain, partly because of the
high cost of human translation. Thus, in their experi-
ments, they applied their methods to a small data set,
the manually translated portion of the Chinese Tree-
bank (CTB) which contains only about 3,000 sen-
tences. On the other hand, many large-scale mono-
lingual treebanks exist, such as the Penn English
Treebank (PTB) (Marcus et al., 1993) (about 40,000
sentences in Version 3) and the latest version of CTB
(over 50,000 sentences in Version 7).
In this paper, we propose a bitext parsing ap-
proach in which we produce the bilingual constraints
on existing monolingual treebanks with the help of
SMT systems. In other words, we aim to improve
source-language parsing with the help of automatic
translations.
In our approach, we first use an SMT system
to translate the sentences of a source monolingual
treebank into the target language. Then, the target
sentences are parsed by a parser trained on a tar-
get monolingual treebank. We then obtain a bilin-
gual treebank that has human annotated trees on the
source side and auto-generated trees on the target
side. Although the sentences and parse trees on the
73
target side are not perfect, we expect that we can
improve bitext parsing performance by using this
newly auto-generated bilingual treebank. We build
word alignment links automatically using a word
alignment tool. Then we can produce a set of bilin-
gual constraints between the two sides.
Because the translation, parsing, and word align-
ment are done automatically, the constraints are not
reliable. To overcome this problem, we verify the
constraints by using large-scale unannotated mono-
lingual sentences and bilingual sentence pairs. Fi-
nally, we design a set of bilingual features based on
the verified results for parsing models.
Our approach uses existing resources including
monolingual treebanks to train monolingual parsers
on both sides, bilingual unannotated data to train
SMT systems and to extract bilingual subtrees,
and target monolingual unannotated data to extract
monolingual subtrees. In summary, we make the fol-
lowing contributions:
â€¢ We propose an approach that uses an auto-
generated bilingual treebank rather than
human-annotated bilingual treebanks used in
previous studies (Burkett and Klein, 2008;
Huang et al., 2009; Chen et al., 2010). The
auto-generated bilingual treebank is built with
the help of SMT systems.
â€¢ We verify the unreliable constraints by using
the existing large-scale unannotated data and
design a set of effective bilingual features over
the verified results. Compared to Chen et al.(2010) that also used tree structures on the tar-
get side, our approach defines the features on
the auto-translated sentences and auto-parsed
trees, while theirs generates the features by
some rules on the human-translated sentences.
â€¢ Our parser significantly outperforms state-of-
the-art baseline systems on the standard test
data of CTB containing about 3,000 sentences.
Moreover, our approach continues to achieve
improvement when we build our system us-
ing the latest version of CTB (over 50,000 sen-
tences) that results in a much stronger baseline.
â€¢ We show the possibility that we can improve
the performance even if the test set has no hu-
man translation. This means that our proposed
approach can be used in a purely monolingual
setting with the help of SMT. To our knowl-
edge, this paper is the first one that demon-
strates this widened applicability, unlike the
previous studies that assumed that the parser is
applied only on the bitexts made by humans.
Throughout this paper, we use Chinese as the
source language and English as the target language.
The rest of this paper is organized as follows. Sec-
tion 2 introduces the motivation of this work. Sec-
tion 3 briefly introduces the parsing model used in
the experiments. Section 4 describes a set of bilin-
gual features based on the bilingual constraints and
Section 5 describes how to use large-scale unanno-
tated data to verify the bilingual constraints and de-
fine another set of bilingual features based on the
verified results. Section 6 explains the experimental
results. Finally, in Section 7 we draw conclusions.
2 Motivation
Here, bitext parsing is the task of parsing source sen-
tences with the help of their corresponding transla-
tions. Figure 1-(a) shows an example of the input
of bitext parsing, where ROOT is an artificial root
token inserted at the beginning and does not depend
on any other token in the sentence, the dashed undi-
rected links are word alignment links, and the di-
rected links between words indicate that they have
a dependency relation. Given such inputs, we build
dependency trees for the source sentences. Figure
1-(b) shows the output of bitext parsing for the ex-
ample in 1-(a).
R O O T g3Qï¿½Aï¿½'ï¿½>~Tk*6,ï¿½JB85ï¿½t a g a o d u p i n g j i a l e y u l i p e n g z o n g l i d e h u i t a n j i e g u ot a g a o d u p i n g j i a l e g3 g3 y u l i p e n g z o n g l i d e g3 g3 g3 h u i t a n j i e g u o
R O O T H h i h l d d h l f h f i h P L iR O O T g3 g3 H e g3 h i g h l y g3 c o m m e n d e d g3 t h e g3 r e s u l t s g3 g3 o f g3 g3 g3 t h e g3 c o n f e r e n c e g3 g3 g3 g3 w i t h g3 g3 P e n g L i
( a )( a )
R O O T g3Qï¿½Aï¿½'ï¿½>~Tk*6,ï¿½JB85ï¿½t a g a o d u p i n g j i a l e y u l i p e n g z o n g l i d e h u i t a n j i e g u ot a g a o d u p i n g j i a l e g3 g3 y u l i p e n g z o n g l i d e g3 g3 h u i t a n j i e g u o
( b )
Figure 1: Input and output of our approach
In bitext parsing, some ambiguities exist on the
source side, but they may be unambiguous on the
74
target side. These differences are expected to help
improve source-side parsing.
Suppose we have a Chinese sentence shown in
Figure 2-(a). In this sentence, there is a nomi-
nalization case (Li and Thompson, 1997) in which
the particle â€œï¿½(de)/nominalizerâ€ is placed after the
verb compound â€œï¿½ï¿½(peiyu)	ï¿½(qilai)/cultivateâ€
to modify â€œ/F(jiqiao)/skillâ€. This nominaliza-
tion is a relative clause, but does not have a clue
about its boundary. That is, it is very hard to deter-
mine which word is the head of â€œ/F(jiqiao)/skillâ€.
The head may be â€œ?ï¿½(fahui)/demonstrateâ€ or â€œï¿½
ï¿½(peiyu)/cultivateâ€, as shown in Figure 2-(b) and
-(c), where (b) is correct.
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a oP N g3 g3 g3 g3 g3 g3 g3 V V g3 g3 g3 g3 g3 g3 g3 g3 g3 D T g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 N N g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 A D g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 g3 V V g3 g3 g3 g3 g3 g3 A D g3 g3 g3 g3 g3 g3 g3 V V g3 g3 g3 g3 V V D E C N N g3 g3 g3 g3 C C g3 g3 g3 N N
( a )
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
( b )
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
( c )
Figure 2: Example of an ambiguity on the Chinese side
In its English translation (Figure 3), word â€œthatâ€ is
a clue indicating the relative clause which shows the
relation between â€œskillâ€ and â€œcultivateâ€, as shown in
Figure 3. The figure shows that the translation can
provide useful bilingual constraints. From the de-
pendency tree on the target side, we find that the
word â€œskillâ€ corresponding to â€œ/F(jiqiao)/skillâ€
depends on the word â€œdemonstrateâ€ corresponding
to â€œ?ï¿½(fahui)/demonstrateâ€, while the word â€œcul-
tivateâ€ corresponding to â€œï¿½ï¿½(peiyu)/cultivateâ€ is a
grandchild of â€œskillâ€. This is a positive evidence for
supporting â€œ?ï¿½(fahui)/demonstrateâ€ as being the
head of â€œ/F(jiqiao)/skillâ€.
The above case uses the human translation on
the target side. However, there are few human-
annotated bilingual treebanks and the existing bilin-
gual treebanks are usually small. In contrast, there
are large-scale monolingual treebanks, e.g., the PTB
and the latest version of CTB. So we want to use
existing resources to generate a bilingual treebank
with the help of SMT systems. We hope to improve
source side parsing by using this newly built bilin-
gual treebank.
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
H e g3 h o p e d g3 t h a t g3 a l l g3 t h e g3 a t h l e t e s g3 w o u l d g3 g3 f u l l y g3 d e m o n s t r a t e g3 t h e g3 s t r e n g t h g3 a n d g3 s k i l l g3 t h a t g3 t h e y g3 c u l t i v a t e g3 d a i l y
Figure 3: Example of human translation
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
h e g3 e x p r e s s e d g3 t h e g3 h o p e g3 t h a t g3 a l l g3 a t h l e t e s g3 u s e d g3 t o g3 g i v e g3 f u l l g3 p l a y g3 t o g3 t h e g3 c o u n t r y g3 ' s g3 s t r e n g t h g3 a n d g3 s k i l l s g3
Figure 4: Example of Moses translation
Figure 4 shows an example of a translation us-
ing a Moses-based system, where the target sen-
tence is parsed by a monolingual target parser. The
translation contains some errors, but it does contain
some correct parts that can be used for disambigua-
tion. In the figure, the word â€œskillsâ€ corresponding
to â€œ/F(jiqiao)/skillâ€ is a grandchild of the word
â€œplayâ€ corresponding to â€œ?ï¿½(fahui)/demonstrateâ€.
This is a positive evidence for supporting â€œ?
ï¿½(fahui)/demonstrateâ€ as being the head of â€œ/
F(jiqiao)/skillâ€.
From this example, although the sentences and
parse trees on the target side are not perfect, we
still can explore useful information to improve bitext
parsing. In this paper, we focus on how to design
a method to verify such unreliable bilingual con-
straints.
3 Parsing
model
In this paper, we implement our approach based
on graph-based parsing models (McDonald and
Pereira, 2006; Carreras, 2007). Note that our ap-
proach can also be applied to transition-based pars-
ing models (Nivre, 2003; Yamada and Matsumoto,
2003).
The graph-based parsing model is to search for
the maximum spanning tree (MST) in a graph (Mc-
Donald and Pereira, 2006). The formulation defines
the score of a dependency tree to be the sum of edge
scores,
75
s(x,y) =
summationdisplay
gâˆˆy
score(w,x,g) =
summationdisplay
gâˆˆy
wÂ·f(x,g) (1)
where x is an input sentence, y is a dependency
tree for x, and g is a spanning subgraph of y. f(x,g)
can be based on arbitrary features of the subgraph
and the input sequence x and the feature weight
vector w are the parameters to be learned by using
MIRA (Crammer and Singer, 2003) during training.
In our approach, we use two types of features
for the parsing model. One is monolingual fea-
tures based on the source sentences. The mono-
lingual features include the firstand secondorder
features presented in McDonald and Pereira (2006)
and the parent-child-grandchild features used in Car-
reras (2007). The other one is bilingual features (de-
scribed in Sections 4 and 5) that consider the bilin-
gual constraints.
We call the parser with the monolingual features
on the source side Parsers, and the parser with the
monolingual features on the target side Parsert.
4 Original
bilingual features
In this paper, we generate two types of bilingual fea-
tures, original and verified bilingual features. The
original bilingual features (described in this section)
are based on the bilingual constraints without being
verified by large-scale unannotated data. And the
verified bilingual features (described in Section 5)
are based on the bilingual constraints verified by us-
ing large-scale unannotated data.
4.1 Auto-generated bilingual treebank
Assuming that we have monolingual treebanks on
the source side, an SMT system that can translate
the source sentences into the target language, and a
Parsert trained on the target monolingual treebank.
We first translate the sentences of the source
monolingual treebank into the target language using
the SMT system. Usually, SMT systems can output
the word alignment links directly. If they can not, we
perform word alignment using some publicly avail-
able tools, such as Giza++ (Och and Ney, 2003) or
Berkeley Aligner (Liang et al., 2006; DeNero and
Klein, 2007). The translated sentences are parsed by
the Parsert. Then, we have a newly auto-generated
bilingual treebank. `
4.2 Bilingual
constraint functions
In this paper, we focus on the firstand second-
order graph models (McDonald and Pereira, 2006;
Carreras, 2007). Thus we produce the constraints
for bigram (a single edge) and trigram (adjacent
edges) dependencies in the graph model. For the tri-
gram dependencies, we consider the parent-sibling
and parent-child-grandchild structures described in
McDonald and Pereira (2006) and Carreras (2007).
We leave the third-order models (Koo and Collins,
2010) for a future study.
Suppose that we have a (candidate) dependency
relation rs that can be a bigram or trigram de-
pendency. We examine whether the corresponding
words of the source words of rs have a dependency
relation rt in the target trees. We also consider the
direction of the dependency relation. The corre-
sponding word of the head should also be the head
in rt. We define a binary function for this bilingual
constraint: Fbn(rsn : rtk), where n and k refers to
the types of the dependencies (2 for bigram and 3 for
trigram). For example, in rs2 : rt3, rs2 is a bigram
dependency on the source side and rt3 is a trigram
dependency on the target side.
4.2.1 Bigram
constraint function: Fb2
For rs2, we consider two types of bilingual con-
straints. The first constraint function, denoted as
Fb2(rs2 : rt2), checks if the corresponding words
also have a direct dependency relation rt2. Figure
5 shows an example, where the source word â€œï¿½
8(quanti)â€ depends on â€œï¿½ï¿½(yundongyuan)â€
and word â€œallâ€ corresponding to â€œï¿½8(quanti)â€ de-
pends on word â€œathletesâ€ corresponding to â€œï¿½
ï¿½(yundongyuan)â€. In this case, Fb2(rs2 : rt2) =
+. However, when the source words are â€œï¿½(ta)â€
and â€œï¿½(xiwang)â€, this time their corresponding
words â€œHeâ€ and â€œhopeâ€ do not have a direct depen-
dency relation. In this case, Fb2(rs2: rt2)=âˆ’.
The second constraint function, denoted as
Fb2(rs2 : rt3), checks if the corresponding words
form a parent-child-grandchild relation that often
occurs in translation (Koehn et al., 2003). Figure 6
shows an example. The source word â€œ/F(jiqiao)â€
depends on â€œ?ï¿½(fahui)â€ while its corresponding
word â€œskillsâ€ indirectly depends on â€œplayâ€ which
corresponds to â€œ?ï¿½(fahui)â€ via â€œtoâ€. In this case,
Fb2(rs2 : rt3)=+.
76
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
h e g3 e x p r e s s e d g3 t h e g3 h o p e g3 t h a t g3 a l l g3 a t h l e t e s g3 u s e d g3 t o g3 g i v e g3 f u l l g3 p l a y g3 t o g3 t h e g3 c o u n t r y g3 ' s g3 s t r e n g t h g3 a n d g3 s k i l l s g3
Figure 5: Example of bilingual constraints (2to2)
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
h e g3 e x p r e s s e d g3 t h e g3 h o p e g3 t h a t g3 a l l g3 a t h l e t e s g3 u s e d g3 t o g3 g i v e g3 f u l l g3 p l a y g3 t o g3 t h e g3 c o u n t r y g3 ' s g3 s t r e n g t h g3 a n d g3 s k i l l s g3
Figure 6: Example of bilingual constraints (2to3)
4.2.2 Trigram
constraint function: Fb3
For a second-order relation on the source side,
we consider one type of constraint. We have three
source words that form a second-order relation and
all of them have the corresponding words. We
define function Fb3(rs3 : rt3) for this constraint.
The function checks if the corresponding words
form a trigram dependencies structure. An exam-
ple is shown in Figure 7. The source words â€œ	ï¿½

(liliang)â€, â€œï¿½(he)â€, and â€œ/F(jiqiao)â€ form a
parent-sibling structure, while their corresponding
words â€œstrengthâ€, â€œandâ€, and â€œskillsâ€ also form a
parent-sibling structure on the target side. In this
case, function Fb3(rs3: rt3)=+.
<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½<Kï¿½ï¿½F ï¿½
ï¿½u6
Uï¿½&)6ï¿½Cï¿½ï¿½,ï¿½ï¿½Gï¿½
ï¿½ï¿½t a x i w a n g q u a n t i y u n d o n g y u a n c h o n g f e n g f a h u i p i n g s h i p e i y u q i l a i d e g3 g3 l i l i a n g h e g3 j i q i a o
h e g3 e x p r e s s e d g3 t h e g3 h o p e g3 t h a t g3 a l l g3 a t h l e t e s g3 u s e d g3 t o g3 g i v e g3 f u l l g3 p l a y g3 t o g3 t h e g3 c o u n t r y g3 ' s g3 s t r e n g t h g3 a n d g3 s k i l l s g3
Figure 7: Example of bilingual constraints (3to3)
4.3 Bilingual
reordering function: Fro
Huang et al. (2009) proposed features based on
reordering between languages for a shift-reduce
parser. They define the features based on word-
alignment information to verify whether the corre-
sponding words form a contiguous span to resolve
shift-reduce conflicts. We also implement similar
features in our system. For example, in Figure 1-
(a) the source span is [ï¿½(huitan),ï¿½T(jieguo)],
which maps onto [results, conference]. Because no
word within this target span is aligned to a source
word outside of the source span, this span is a con-
tiguous span. In this case, function Fro =+, other-
wise Fro=âˆ’.
4.4 Original
bilingual features
We define original bilingual features based on the
bilingual constraint functions and the bilingual re-
ordering function.
Table 1 lists the original features, where Dir
refers to the directions1 of the source-side dependen-
cies, Fb2 can be Fb2(rs2 : rt2) and Fb2(rs2 : rt3),
and Fb3 is Fb3(rs3 : rt3). Each line of the table
defines a feature template that is a combination of
functions.
First-order features Second-order features
ã€ˆFroã€‰
ã€ˆFb2,Dirã€‰ ã€ˆFb3,Dirã€‰
ã€ˆFb2,Dir,Froã€‰ ã€ˆFb3,Dir,Froã€‰
Table 1: Original bilingual features
We use an example to show how to generate the
original bilingual features in practice. In Figure 4,
we want to define the bilingual features for the bi-
gram dependency (rs2) between â€œ?ï¿½(fahui)â€ and
â€œ/F(jiqiao)â€. The corresponding words form a tri-
gram relation rt3 in the target dependency tree. The
direction of the bigram dependency is right. Then
we have feature â€œã€ˆFb2(rs2: rt3)=+,RIGHTã€‰â€ for
the second first-order feature template in Table 1.
5 Verified
bilingual features
However, because the bilingual treebank is gener-
ated automatically, using the bilingual constraints
alone is not reliable. Therefore, in this section we
verify the constraints by using large-scale unanno-
tated data to overcome this problem. More specifi-
cally, rtk of the constraint is verified by checking a
list of target monolingual subtrees and rsn : rtk is
verified by checking a list of bilingual subtrees. The
subtrees are extracted from the large-scale unanno-
tated data. The basic idea is as follows: if the de-
pendency structures of a bilingual constraint can be
found in the list of the target monolingual subtrees
1For the second order features, Dir is the combination of
the directions of two dependencies.
77
or bilingual subtrees, this constraint will probably be
reliable.
We first parse the large-scale unannotated mono-
lingual and bilingual data. Subsequently, we ex-
tract the monolingual and bilingual subtrees from
the parsed data. We then verify the bilingual con-
straints using the extracted subtrees. Finally, we
generate the bilingual features based on the verified
results for the parsing models.
5.1 Verified
constraint functions
5.1.1 Monolingual
target subtrees
Chen et al. (2009) proposed a simple method to
extract subtrees from large-scale monolingual data
and used them as features to improve monolingual
parsing. Following their method, we parse large
unannotated data with the Parsert and obtain the sub-
tree list (STt) on the target side. We extract two
types of subtrees: bigram (two words) subtree and
trigram (three words) subtree.
H b h t b h b k
R O O T g3 g3 H e g3 g3 g3 g3 g3 b o u g h t g3 g3 g3 g3 g3 a g3 g3 g3 g3 b o o k
H e g3 g3 g3 g3 g3 b o u g h t b o u g h t g3 g3 b o o k g3
a b o o k b h t b ka g3 g3 g3 g3 g3 b o o k
( a ) ( b )b o u g h t g3 g3 g3 a g3 g3 g3 g3 g3 b o o k g3
Figure 8: Example of monolingual subtree extraction
From the dependency tree in Figure 8-(a), we ob-
tain the subtrees shown in Figure 8-(b) where the
first three are bigram subtrees and the last one is
a trigram subtree. After extraction, we obtain the
subtree list STt that includes two sets, one for bi-
gram subtrees, and the other one for trigram sub-
trees. We remove the subtrees occurring only once
in the data. For each set, we assign labels to the
extracted subtrees according to their frequencies by
using the same method as that of Chen et al. (2009).
If the frequency of a subtree is in the top 10% in the
corresponding set, it is labeled HF. If the frequency
is between the top 20% and 30%, it is labeled MF.
We assign the label LF to the remaining subtrees.
We use Type(stt) to refer to the label of a subtree,
stt.
5.1.2 Verified
target constraint function:
Fvt(rtk)
We use the extracted target subtrees to verify the
rtk of the bilingual constraints. In fact, rtk is a can-
didate subtree. If the rtk is included in STt, func-
tion Fvt(rtk) = Type(rtk), otherwise Fvt(rtk) =
ZERO. For example, in Figure 5 the bigram struc-
ture of â€œallâ€ and â€œathletesâ€ can form a bigram sub-
tree that is included STt and its label is HF. In this
case, Fvt(rt2)= HF.
5.1.3 Bilingual
subtrees
We extract bilingual subtrees from a bilingual
corpus, which is parsed by the Parsers and Parsert
on both sides. We extract three types of bilingual
subtrees: bigram-bigram (stbi22), bigram-trigram
(stbi23), and trigram-trigram (stbi33) subtrees. For
example, stbi22 consists of a bigram subtree on the
source side and a bigram subtree on the target side.
_
=ï¿½+O__ï¿½+OR O O T g3_0
=ï¿½+Ot a s h i y i m i n g x u e s h e n g__ï¿½+O
R O O T g3 g3 H e g3 g3 g3 g3 g3 i s g3 g3 g3 g3 g3 a g3 g3 g3 g3 g3 s t u d e n t H e g3 g3 g3 g3 g3 i s i s g3 g3 g3 g3 g3 s t u d e n t
( a ) ( b )
Figure 9: Example of bilingual subtree extraction
From the dependency tree in Figure 9-(a), we
obtain the bilingual subtrees shown in Figure 9-
(b). Figure 9-(b) shows the extracted bigram-bigram
bilingual subtrees. After extraction, we obtain the
bilingual subtrees STbi. We remove the subtrees oc-
curring only once in the data.
5.1.4 Verified
bilingual constraint function:
Fvb(rbink)
We use the extracted bilingual subtrees to verify
the rsn : rtk (rbink in short) of the bilingual con-
straints. rsn and rtk form a candidate bilingual sub-
tree stbink. If the stbink is included in STbi, function
Fvb(rbink)=+, otherwise Fvb(rbink)=âˆ’.
5.2 Verified
bilingual features
Then, we define another set of bilingual features by
combining the verified constraint functions. We call
these bilingual features â€˜verified bilingual featuresâ€™.
78
Table 2 lists the verified bilingual features used in
our experiments, where each line defines a feature
template that is a combination of functions.
We use an example to show how to generate the
verified bilingual features in practice. In Figure 4,
we want to define the verified features for the bi-
gram dependency (rs2) between â€œ?ï¿½(fahui)â€ and
â€œ/F(jiqiao)â€. The corresponding words form a
trigram relation rt3. The direction of the bigram
dependency is right. Suppose we can find rt3 in
STt with label MF and can not find the candidate
bilingual subtree in STbi. Then we have feature
â€œã€ˆFb2(rs2 : rt3) = +,Fvt(rt3) = MF,RIGHTã€‰â€
for the third first-order feature template and feature
â€œã€ˆFb2(rs2: rt3)=+,Fvb(rbi23)=âˆ’,RIGHTã€‰â€ for
the fifth in Table 2.
First-order features Second-order features
ã€ˆFroã€‰
ã€ˆFb2,Fvt(rtk)ã€‰ ã€ˆFb3,Fvt(rtk)ã€‰
ã€ˆFb2,Fvt(rtk),Dirã€‰ ã€ˆFb3,Fvt(rtk),Dirã€‰
ã€ˆFb2,Fvb(rbink)ã€‰ ã€ˆFb3,Fvb(rbink)ã€‰
ã€ˆFb2,Fvb(rbink),Dirã€‰ ã€ˆFb3,Fvb(rbink),Dirã€‰
ã€ˆFb2,Fro,Fvb(rbink)ã€‰
Table 2: Verified bilingual features
6 Experiments
We evaluated the proposed method on the translated
portion of the Chinese Treebank V2 (referred to as
CTB2tp) (Bies et al., 2007), articles 1-325 of CTB,
which have English translations with gold-standard
parse trees. The tool â€œPenn2Maltâ€2 was used to con-
vert the data into dependency structures. Following
the studies of Burkett and Klein (2008), Huang et
al. (2009) and Chen et al. (2010), we used the ex-
act same data split: 1-270 for training, 301-325 for
development, and 271-300 for testing. Note that we
did not use human translation on the English side
of this bilingual treebank to train our new parsers.
For testing, we used two settings: a test with hu-
man translation and another with auto-translation.
To process unannotated data, we trained a first-order
Parsers on the training data.
To prove that the proposed method can work on
larger monolingual treebanks, we also tested our
2http://w3.msi.vxu.se/Ëœnivre/research/Penn2Malt.html
methods on the CTB7 (LDC2010T07) that includes
much more sentences than CTB2tp. We used arti-
cles 301-325 for development, 271-300 for testing,
and the other articles for training. That is, we eval-
uated the systems on the same test data as CTB2tp.
Table 3 shows the statistical information on the data
sets.
Train Dev Test
CTB2tp 2,745 273 290
CTB7 50,747 273 290
Table 3: Number of sentences of data sets used
We built Chinese-to-English SMT systems based
on Moses3. Minimum error rate training (MERT)
with respect to BLEU score was used to tune the de-
coderâ€™s parameters. The translation model was cre-
ated from the FBIS corpus (LDC2003E14). We used
SRILM4 to train a 5-gram language model. The lan-
guage model was trained on the target side of the
FBIS corpus and the Xinhua news in English Gi-
gaword corpus (LDC2009T13). The development
and test sets were from NIST MT08 evaluation cam-
paign5. We then used the SMT systems to translate
the training data of CTB2tp and CTB7.
To directly compare with the results of Huang
et al. (2009) and Chen et al. (2010), we also used
the same word alignment tool, Berkeley Aligner
(Liang et al., 2006; DeNero and Klein, 2007), to
perform word alignment for CTB2tp and CTB7.
We trained a Berkeley Aligner on the FBIS corpus
(LDC2003E14). We removed notoriously bad links
in {a, an, the}Ã—{ï¿½(de),
(le)} following the work
of Huang et al. (2009).
To train an English parser, we used the PTB
(Marcus et al., 1993) in our experiments and the
tool â€œPenn2Maltâ€ to convert the data. We split the
data into a training set (sections 2-21), a develop-
ment set (section 22), and a test set (section 23).
We trained first-order and second-order Parsert on
the training data. The unlabeled attachment score
(UAS) of the second-order Parsert was 91.92, in-
dicating state-of-the-art accuracy on the test data.
We used the second-order Parsert to parse the auto-
translated/human-made target sentences in the CTB
3http://www.statmt.org/moses/
4http://www.speech.sri.com/projects/srilm/download.html
5http://www.itl.nist.gov/iad/mig//tests/mt/2008/
79
data.
To extract English subtrees, we used the BLLIP
corpus (Charniak et al., 2000) that contains about
43 million words of WSJ texts. We used the MX-
POST tagger (Ratnaparkhi, 1996) trained on train-
ing data to assign POS tags and used the first-order
Parsert to process the sentences of the BLLIP cor-
pus. To extract bilingual subtrees, we used the FBIS
corpus and an additional bilingual corpus contain-
ing 800,000 sentence pairs from the training data of
NIST MT08 evaluation campaign. On the Chinese
side, we used the morphological analyzer described
in (Kruengkrai et al., 2009) trained on the training
data of CTBtp to perform word segmentation and
POS tagging and used the first-order Parsers to parse
all the sentences in the data. On the English side, we
used the same procedure as we did for the BLLIP
corpus. Word alignment was performed using the
Berkeley Aligner.
We reported the parser quality by the UAS, i.e.,
the percentage of tokens (excluding all punctuation
tokens) with correct HEADs.
6.1 Experimental
settings
For baseline systems, we used the monolingual fea-
tures mentioned in Section 3. We called these fea-
tures basic features. To compare the results of (Bur-
kett and Klein, 2008; Huang et al., 2009; Chen et
al., 2010), we used the test data with human trans-
lation in the following three experiments. The tar-
get sentences were parsed by using the second-order
Parsert. We used PAG to refer to our parsers trained
on the auto-generated bilingual treebank.
6.2 Training
with CTB2tp
Order-1 Order-2
Baseline 84.35 87.20
PAGo 84.71(+0.36) 87.85(+0.65)
PAG 85.37(+1.02) 88.49(+1.29)
ORACLE 85.79(+1.44) 88.87(+1.67)
Table 4: Results of training with CTB2tp
First, we conducted the experiments on the stan-
dard data set of CTB2tp, which was also used in
other studies (Burkett and Klein, 2008; Huang et al.,
2009; Chen et al., 2010). The results are given in
Table 4, where Baseline refers to the system with
the basic features, PAGo refers to that after adding
the original bilingual features of Table 1 to Baseline,
PAG refers to that after adding the verified bilingual
features of Table 2 to Baseline, and ORACLE6 refers
to using human-translation for training data with
adding the features of Table 1. We obtained an ab-
solute improvement of 1.02 points for the first-order
model and 1.29 points for the second-order model by
adding the verified bilingual features. The improve-
ments of the final systems (PAG) over the Baselines
were significant in McNemarâ€™s Test (p < 0.001 for
the first-order model and p < 0.0001 for the second-
order model). If we used the original bilingual fea-
tures (PAGo), the system dropped 0.66 points for the
first-order and 0.64 points for the second-order com-
pared with system PAG. This indicated that the ver-
ified bilingual constraints did provide useful infor-
mation for the parsing models.
We also found that PAG was about 0.3 points
lower than ORACLE. The reason is mainly due
to the imperfect translations, although we used
the large-scale subtree lists to help verify the con-
straints. We tried adding the features of Table 2 to
the ORACLE system, but the results were worse.
These facts indicated that our approach obtained the
benefits from the verified constraints, while using
the bilingual constraints alone was enough for OR-
ACLE.
6.3 Training
with CTB7
 0.83
 0.84
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 5  10  20  30  40  50
UAS
Amount of training data (K)
Baseline1 PAG1
Baseline2 PAG2
Figure 10: Results of using different sizes of training data
Here, we demonstrate that our approach is still
able to provide improvement, even if we use larger
6Note that we also used the tool to perform the word align-
ment automatically.
80
Baseline D10 D20 D50 D100 GTran
BLEU n/a 14.71 15.84 16.92 17.95 n/a
UAS 87.20 87.63 87.67 88.20 88.49 88.58
Table 5: Results of using different translations
training data that result in strong baseline systems.
We incrementally increased the training sentences
from the CTB7. Figure 10 shows the results of us-
ing different sizes of CTB7 training data, where the
numbers of the x-axis refer to the sentence numbers
of training data used, Baseline1 and Baseline2 re-
fer to the firstand second-order baseline systems,
and PAG1 and PAG2 refer to our firstand second-
order systems. The figure indicated that our sys-
tem always outperformed the baseline systems. For
small data sizes, our system performed much better
than the baselines. For example, when using 5,000
sentences, our second-order system provided a 1.26
points improvement over the second-order baseline.
Finally, when we used all of the CTB7 training
data, our system achieved 91.66 for the second-order
model, while the baseline achieved 91.10.
6.4 With
different settings of SMT systems
We investigated the effects of different settings of
SMT systems. We randomly selected 10%, 20%,
and 50% of FBIS to train the Moses systems and
used them to translate CTB2tp. The results are in
Table 5, where D10, D20, D50, and D100 refer to
the system with 10%, 20%, 50%, and 100% data re-
spectively. For reference, we also used the Google-
translate online system7, indicated as GTran in the
table, to translate the CTB2tp.
From the table, we found that our system outper-
formed the Baseline even if we used only 10% of the
FBIS corpus. The BLEU and UAS scores became
higher, when we used more data of the FBIS corpus.
And the gaps among the results of D50, D100, and
GTran were small. This indicated that our approach
was very robust to the noise produced by the SMT
systems.
6.5 Testing
with auto-translation
We also translated the test data into English using
the Moses system and tested the parsers on the new
7http://translate.google.com/
test data. Table 6 shows the results. The results
showed that PAG outperformed the baseline systems
for both the firstand second-order models. This
indicated that our approach can provide improve-
ment in a purely monolingual setting with the help
of SMT.
Order-1 Order-2
Baseline 84.35 87.20
PAG 84.88(+0.53) 87.89(+0.69)
Table 6: Results of testing with auto-translation (training
with CTB2tp)
6.6 Comparison
results
With CTB2tp With CTB7
Type System UAS System UAS
M Baseline 87.20 Baseline 91.10
HA
Huang2009 86.3 n/a
Chen2010BI 88.56
Chen2010ALL 90.13
AG PAG 88.49 PAG 91.66PAG+ST
s 89.75
Table 7: Comparison of our results with other pre-
vious reported systems. Type M denotes training on
monolingual treebank. Types HA and AG denote training
on human-annotated and auto-generated bilingual tree-
banks respectively.
We compared our results with the results reported
previously for the same data. Table 7 lists the re-
sults, where Huang2009 refers to the result of Huang
et al. (2009), Chen2010BI refers to the result of
using bilingual features in Chen et al. (2010), and
Chen2010ALL refers to the result of using all of
the features in Chen et al. (2010). The results
showed that our new parser achieved better accuracy
than Huang2009 and comparable to Chen2010BI.
To achieve higher performance, we also added the
source subtree features (Chen et al., 2009) to our
system: PAG+STs. The new result is close to
Chen2010ALL. Compared with the approaches of
81
Huang et al. (2009) and Chen et al. (2010), our
approach used an auto-generated bilingual treebank
while theirs used a human-annotated bilingual tree-
bank. By using all of the training data of CTB7, we
obtained a more powerful baseline that performed
much better than the previous reported results. Our
parser achieved 91.66, much higher accuracy than
the others.
7 Conclusion
We have presented a simple yet effective approach
to improve bitext parsing with the help of SMT sys-
tems. Although we trained our parser on an auto-
generated bilingual treebank, we achieved an accu-
racy comparable to the systems trained on human-
annotated bilingual treebanks on the standard test
data. Moreover, our approach continued to pro-
vide improvement over the baseline systems when
we used a much larger monolingual treebank (over
50,000 sentences) where target human translations
are not available and very hard to construct. We also
demonstrated that the proposed approach can be ef-
fective in a purely monolingual setting with the help
of SMT.
Acknowledgments
This study was started when Wenliang Chen, Yu-
jie Zhang, and Yoshimasa Tsuruoka were members
of Language Infrastructure Group, National Insti-
tute of Information and Communications Technol-
ogy (NICT), Japan. We would also thank the anony-
mous reviewers for their detailed comments, which
have helped us to improve the quality of this work.
References
Ann Bies, Martha Palmer, Justin Mott, and Colin Warner.
2007. English Chinese Translation Treebank V 1.0,
LDC2007T02. Linguistic Data Consortium.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP 2008, pages 877â€“886, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957â€“961, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 2000. BLLIP 1987-
89 WSJ Corpus Release 1, LDC2000T43. Linguistic
Data Consortium.
Wenliang Chen, Junâ€™ichi Kazama, Kiyotaka Uchimoto,
and Kentaro Torisawa. 2009. Improving dependency
parsing with subtrees from auto-parsed data. In Pro-
ceedings of EMNLP 2009, pages 570â€“579, Singapore,
August.
Wenliang Chen, Junâ€™ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of ACL 2010, pages
21â€“29, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951â€“991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of ACL 2007, pages 17â€“24, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP 2009, pages 1222â€“
1231, Singapore, August. Association for Computa-
tional Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
NAACL 2003, pages 48â€“54. Association for Computa-
tional Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of ACL
2010, pages 1â€“11, Uppsala, Sweden, July. Association
for Computational Linguistics.
Canasai Kruengkrai, Kiyotaka Uchimoto, Junâ€™ichi
Kazama, Yiou Wang, Kentaro Torisawa, and Hitoshi
Isahara. 2009. An error-driven word-character hybrid
model for joint Chinese word segmentation and POS
tagging. In Proceedings of ACL-IJCNLP2009, pages
513â€“521, Suntec, Singapore, August. Association for
Computational Linguistics.
Charles N. Li and Sandra A. Thompson. 1997. Man-
darin Chinese A Functional Reference Grammar.
University of California Press.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of NAACL 2006,
pages 104â€“111, New York City, USA, June. Associa-
tion for Computational Linguistics.
Yang Liu and Liang Huang. 2010. Tree-based and forest-
based translation. In Tutorial Abstracts of ACL 2010,
page 2, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
82
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguisticss, 19(2):313â€“330.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL 2006, pages 81â€“88.
Joakim Nivre. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
IWPT2003, pages 149â€“160.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19â€“51.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP
1996, pages 133â€“142.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceedings of EMNLP 2004, pages
49â€“56.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
Proceedings of IWPT 2003, pages 195â€“206.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing us-
ing a bilingual lexicon. In Proceedings of ACL-
IJCNLP2009, pages 55â€“63, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83

