Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 21‚Äì30,
Beijing, August 2010
Automatic ClassiÔ¨Åcation of Semantic Relations
between Facts and Opinions
Koji Murakami‚Ä† Eric Nichols‚Ä° Junta Mizuno‚Ä†‚Ä° Yotaro Watanabe‚Ä°
Hayato Goto‚Ä† Megumi Ohki‚Ä† Suguru Matsuyoshi‚Ä† Kentaro Inui‚Ä° Yuji Matsumoto‚Ä†
‚Ä†Nara Institute of Science and Technology
‚Ä°Tohoku University
{kmurakami,matuyosi,hayato-g,megumi-o,matsu}@is.naist.jp
{eric-n,junta-m,inui}@ecei.tohoku.ac.jp
Abstract
Classifying and identifying semantic re-
lations between facts and opinions on
the Web is of utmost importance for or-
ganizing information on the Web, how-
ever, this requires consideration of a
broader set of semantic relations than are
typically handled in Recognizing Tex-
tual Entailment (RTE), Cross-document
Structure Theory (CST), and similar
tasks. In this paper, we describe the con-
struction and evaluation of a system that
identiÔ¨Åes and classiÔ¨Åes semantic rela-
tions in Internet data. Our system targets
a set of semantic relations that have been
inspired by CST but that have been gen-
eralized and broadened to facilitate ap-
plication to mixed fact and opinion data
from the Internet. Our system identi-
Ô¨Åes these semantic relations in Japanese
Webtextsusingacombinationoflexical,
syntactic, and semantic information and
evaluate our system against gold stan-
dard data that was manually constructed
for this task. We will release all gold
standard data used in training and eval-
uation of our system this summer.
1 Introduction
The task of organizing the information on the In-
ternet to help users Ô¨Ånd facts and opinions on
their topics of interest is increasingly important
as more people turn to the Web as a source of
important information. The vast amounts of re-
search conducted in NLP on automatic summa-
rization, opinion mining, and question answer-
ing are illustrative of the great interest in mak-
ing relevant information easier to Ô¨Ånd. Provid-
ing Internet users with thorough information re-
quires recognizing semantic relations between
both facts and opinions, however the assump-
tions made by current approaches are often in-
compatible with this goal. For example, the
existing semantic relations considered in Rec-
ognizing Textual Entailment (RTE) (Dagan et
al., 2005) are often too narrow in scope to be
directly applicable to text on the Internet, and
theories like Cross-document Structure Theory
(CST) (Radev, 2000) are only applicable to facts
or second-hand reporting of opinions rather than
relations between both.
As part of the STATEMENT MAP project we
proposed the development of a system to sup-
port information credibility analysis on the Web
(Murakami et al., 2009b) by automatically sum-
marizing facts and opinions on topics of inter-
est to users and showing them the evidence and
conÔ¨Çicts for each viewpoint. To facilitate the de-
tection of semantic relations in Internet data, we
deÔ¨Ånedasentence-likeunitofinformationcalled
the statement that encompasses both facts and
opinions, started compiling a corpus of state-
ments annotated with semantic relations (Mu-
rakami et al., 2009a), and begin constructing a
system to automatically identify semantic rela-
tions between statements.
In this paper, we describe the construction and
evaluation of a prototype semantic relation iden-
tiÔ¨Åcation system. We build on the semantic rela-
tions proposed in RTE and CST and in our pre-
vious work, reÔ¨Åning them into a streamlined set
of semantic relations that apply across facts and
opinions, but that are simple enough to make
automatic recognition of semantic relations be-
tween statements in Internet text possible.Our
semantic relations are [AGREEMENT], [CON-
FLICT], [CONFINEMENT], and [EVIDENCE].
[AGREEMENT] and [CONFLICT] are expansions
ofthe [EQUIVALENCE] and [CONTRADICTION]
21
relations used in RTE. [CONFINEMENT] and
[EVIDENCE] are new relations between facts
and opinions that are essential for understanding
how statements on a topic are inter-related.
Our task differs from opinion mining and sen-
timent analysis which largely focus on identify-
ing the polarity of an opinion for deÔ¨Åned param-
eters rather than identify how facts and opinions
relate to each other, and it differs from web doc-
ument summarization tasks which focus on ex-
tractinginformationfromwebpagestructureand
contextual information from hyperlinks rather
than analyzing the semantics of the language on
the webpage itself.
We present a system that automatically iden-
tiÔ¨Åes semantic relations between statements in
Japanese Internet texts. Our system uses struc-
tural alignment to identify statement pairs that
are likely to be related, then classiÔ¨Åes seman-
tic relations using a combination of lexical, syn-
tactic, and semantic information. We evaluate
cross-statement semantic relation classiÔ¨Åcation
on sentence pairs that were taken from Japanese
Internet texts on several topics and manually an-
notated with a semantic relation where one is
present. In our evaluation, we look closely at the
impactthateachoftheresourceshasonsemantic
relation classiÔ¨Åcation quality.
The rest of this paper is organized as follows.
In Section 2, we discuss related work in summa-
rization, semantic relation classiÔ¨Åcation, opinion
mining, and sentiment analysis, showing how
existing classiÔ¨Åcation schemes are insufÔ¨Åcient
for our task. In Section 3, we introduce a set of
cross-sentential semantic relations for use in the
opinionclassiÔ¨Åcationneededtosupportinforma-
tion credibility analysis on the Web. In Section
4, we present our cross-sentential semantic re-
lation recognition system, and discuss the algo-
rithms and resources that are employed. In Sec-
tion 5, we evaluate our system in a semantic rela-
tion classiÔ¨Åcation task. In Section 6, we discuss
our Ô¨Åndings and conduct error analysis. Finally,
we conclude the paper in Section 7.
2 Related
Work
2.1 Recognizing
Textual Entailment
Identifying logical relations between texts is the
focus of Recognizing Textual Entailment, the
task of deciding whether the meaning of one
text is entailed from another text. A major
task in the RTE Challenge (Recognizing Tex-
tual Entailment Challenge) is classifying the se-
mantic relation between a Text (T) and a Hy-
pothesis (H) into [ENTAILMENT], [CONTRA-
DICTION], or [UNKNOWN]. Over the last sev-
eral years, several corpora annotated with thou-
sands of (T,H) pairs have been constructed for
this task. In these corpora, each pair was tagged
indicating its related task (e.g. Information Ex-
traction, Question Answering, Information Re-
trieval or Summarization).
The RTE Challenge has successfully em-
ployed a variety of techniques in order to rec-
ognize instances of textual entailment, including
methods based on: measuring the degree of lex-
ical overlap between bag of words (Glickman
et al., 2005; Jijkoun and de Rijke, 2005), the
alignment of graphs created from syntactic or se-
mantic dependencies (Marsi and Krahmer, 2005;
MacCartney et al., 2006), statistical classiÔ¨Åers
which leverage a wide range of features (Hickl
etal.,2005),orreferencerulegeneration(Szpek-
tor et al., 2007). These approaches have shown
great promise in RTE for entailment pairs in the
corpus, but more robust models of recognizing
logical relations are still desirable.
The deÔ¨Ånition of contradiction in RTE is that
T contradicts H if it is very unlikely that both T
and H can be true at the same time. However, in
real documents on the Web, there are many pairs
of examples which are contradictory in part, or
where one statement conÔ¨Ånes the applicability of
another, as shown in the examples in Table 1.
2.2 Cross-document Structure Theory
Cross-document Structure Theory (CST), devel-
oped by Radev (2000), is another task of rec-
ognizing semantic relations between sentences.
CST is an expanded rhetorical structure analy-
sis based on Rhetorical Structure Theory (RST:
(William and Thompson, 1988)), and attempts
to describe the semantic relations that exist
between two or more sentences from differ-
ent source documents that are related to the
same topic, as well as those that come from
a single source document. A corpus of cross-
document sentences annotated with CST rela-
tions has also been constructed (The CSTBank
Corpus: (Radev et al., 2003)). CSTBank is
organized into clusters of topically-related ar-
ticles. There are 18 kinds of semantic rela-
tions in this corpus, not limited to [EQUIVA-
LENCE] or [CONTRADICTION], but also includ-
ing [JUDGEMENT], [ELABORATION], and [RE-
22
Query Matching sentences Output
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩ'wtÔøΩL
UKÔøΩ
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩoMÔøΩÔøΩUMÔøΩrÔøΩ`ÔøΩ'wwÔøΩLxÔøΩMÔøΩOpbÔøΩÔøΩ
The cavity-prevention effects are greater the more Xylitol is included. [AGREEMENT].
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩUSÔøΩwHÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ'wtÔøΩÔøΩLÔøΩC4`ÔøΩbÔøΩÔøΩ
Xylitol is effective at preventing
cavities.
Xylitolshowseffectivenessatmaintaininggoodoralhygieneandpreventingcavities. [AGREEMENT]
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩwÔøΩÔøΩH
MÔøΩLtmMoxMÔøΩMÔøΩsÔøΩ_UKÔøΩÔøΩbUÔøΩMxÔøΩ
LUKÔøΩÔøΩZpxKÔøΩÔøΩdÔøΩ
0q
There are many opinions about the cavity-prevention effectiveness of Xylitol, but it
is not really effective.
[CONFLICT]
i
+xHÔøΩtÔøΩM
	ÔøΩÔøΩÔøΩÔøΩ
QwÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩi
+UKshqHwHÔøΩÔøΩÔøΩQÔøΩbÔøΩÔøΩ
Reduced water, which has weak alkaline ions, supports the health of you and your
family.
[AGREEMENT]
i
+xÔøΩ
QÔøΩ
ÔøΩÔøΩ	ÔøΩÔøΩbÔøΩqtÔøΩÔøΩHÔøΩÔøΩÔøΩÔøΩ`oXÔøΩÔøΩÔøΩVÔøΩÔøΩhÔøΩbÔøΩÔøΩ
Reduced water is good for the
health.
Reduced water is said to remove active oxygen from the body, making it effective at
promoting good health.
[AGREEMENT]
ÔøΩÔøΩ`XoÔøΩÔøΩ=^dÔøΩ
+xHÔøΩtxÔøΩqjÔøΩdÔøΩ0q
Even if oxidized water tastes good, it does not help one‚Äôs health. [CONFLICT]
ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩxHÔøΩÔøΩÔøΩtÔøΩL
UKÔøΩ
G~ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩpa	ÔøΩ
ÔøΩ	bÔøΩqHÔøΩÔøΩÔøΩtxÔøΩwÔøΩÔøΩÔøΩ)
QÔøΩALqsÔøΩÔøΩb
v
IsoÔ¨Çavone is effective at
maintaining good health.
Taking too much soy isoÔ¨Çavone as a supplement will have a negative effect on one‚Äôs
health
[CONFINEMENT]
Table 1: Example semantic relation classiÔ¨Åcation.
FINEMENT]. Etoh et al. (Etoh and Okumura,
2005) constructed a Japanese Cross-document
Relation Corpus, and they redeÔ¨Åned 14 kinds of
semantic relations in their corpus.
CST was designed for objective expressions
because its target data is newspaper articles re-
lated to the same topic. Facts, which can be ex-
tracted from newspaper articles, have been used
in conventional NLP research, such as Informa-
tion Extraction or Factoid Question Answering.
However, there are a lot of opinions on the Web,
and it is important to survey opinions in addition
to facts to give Internet users a comprehensive
view of the discussions on topics of interest.
2.3 Cross-document Summarization Based
on CST Relations between Sentences
Zhang and Radev (2004) attempted to classify
CST relations between sentence pairs extracted
fromtopicallyrelateddocuments. However, they
used a vector space model and tried multi-class
classiÔ¨Åcation. The results were not satisfactory.
This observation may indicate that the recog-
nition methods for each relation should be de-
veloped separately. Miyabe et al. (2008) at-
tempted to recognize relations that were deÔ¨Åned
in a Japanese cross-document relation corpus
(Etoh and Okumura, 2005). However, their tar-
get relations were limited to [EQUIVALENCE]
and [TRANSITION]; other relations were not tar-
geted. Recognizing [EVIDENCE] is indispens-
able for organizing information on the Internet.
We need to develop satisfactory methods of [EV-
IDENCE] recognition.
2.4 Opinion
Mining and Sentiment Analysis
Subjective statements, such as opinions, have
recently been the focus of much NLP re-
search including review analysis, opinion ex-
traction, opinion question answering, and senti-
ment analysis. In the corpus constructed in the
Multi-Perspective Question Answering (MPQA)
Project (Wiebe et al., 2005), individual expres-
sions are tagged that correspond to explicit men-
tions of private states, speech event, and expres-
sive subjective elements.
The goal of opinion mining to extract expres-
sions with polarity from texts, not to recognize
semanticrelationsbetweensentences. Sentiment
analysis also focus classifying subjective expres-
sions in texts into positive/negative classes. In
comparison, although we deal with sentiment in-
formation in text, our objective is to recognize
semantic relations between sentences. If a user‚Äôs
query requires positive/negative information, we
will also need to extract sentences including sen-
timent expression like in opinion mining, how-
ever, our semantic relation, [CONFINEMENT], is
more precise because it identiÔ¨Åes the condition
or scope of the polarity. Queries do not neces-
sarily include sentiment information; we also ac-
cept queries that are intended to be a statement
of fact. For example, for the query ‚ÄúXylitol is
effective at preventing cavities.‚Äù in Table 1, we
extract a variety of sentences from the Web and
recognize semantic relations between the query
and many kinds of sentences.
23
3 Semantic
Relations between
Statements
In this section, we deÔ¨Åne the semantic relations
that we will classify in Japanese Internet texts as
well as their corresponding relations in RTE and
CST. Our goal is to deÔ¨Åne semantic relations that
are applicable over both fact and opinions, mak-
ing them more appropriate for handling Internet
texts. See Table 1 for real examples.
3.1 [AGREEMENT]
A bi-directional relation where statements have
equivalent semantic content on a shared topic.
Here we use topic in a narrow sense to mean that
the semantic contents of both statements are rel-
evant to each other.
The following is an example of [AGREE-
MENT] onthetopicofbio-ethanolenvironmental
impact.
(1) a. Bio-ethanol is good for the environment.
b. Bio-ethanol is a high-quality fuel, and it
has the power to deal with the environ-
ment problems that we are facing.
Once relevance has been established,
[AGREEMENT] can range from strict logi-
cal entailment or identical polarity of opinions.
Here is an example of two statements that
share a broad topic, but that are not classiÔ¨Åed as
[AGREEMENT] because preventing cavities and
tooth calciÔ¨Åcation are not intuitively relevant.
(2) a. Xylitol is effective at preventing cavities.
b. Xylitol advances tooth calciÔ¨Åcation.
3.2 [CONFLICT]
A bi-directional relation where statements have
negative or contradicting semantic content on a
shared topic. This can range from strict logical
contradiction to opposite polarity of opinions.
The next pair is a [CONFLICT] example.
(3) a. Bio-ethanol is good for our earth.
b. There is a fact that bio-ethanol further the
destruction of the environment.
3.3 [EVIDENCE]
A uni-directional relation where one statement
provides justiÔ¨Åcation or supporting evidence for
the other. Both statements can be either facts or
opinions. The following is a typical example:
(4) a. I believe that applying the technology of
cloning must be controlled by law.
b. There is a need to regulate cloning, be-
cause it can be open to abuse.
The statement containing the evidence con-
sists of two parts: one part has a [AGREEMENT]
or [CONFLICT] with the other statement, the
other part provides support or justiÔ¨Åcation for it.
3.4 [CONFINEMENT]
A uni-directional relation where one statement
provides more speciÔ¨Åc information about the
other or quantiÔ¨Åes the situations in which it ap-
plies. The pair below is an example, in which
one statement gives a condition under which the
other can be true.
(5) a. Steroids have side-effects.
b. There is almost no need to worry about
side-effects when steroids are used for lo-
cal treatment.
4 Recognizing
Semantic Relations
In order to organize the information on the
Internet, we need to identify the [AGREE-
MENT], [CONFLICT], [CONFINEMENT], and
[EVIDENCE] semantic relations. Because iden-
tiÔ¨Åcation of [AGREEMENT] and [CONFLICT] is
a problem of measuring semantic similarity be-
tween two statements, it can be cast as a sen-
tence alignment problem and solved using an
RTE framework. The two sentences do not need
to be from the same source.
However, the identiÔ¨Åcation of [CONFINE-
MENT] and [EVIDENCE] relations depend on
contextual information in the sentence. For ex-
ample, conditional statements or speciÔ¨Åc dis-
course markers like ‚Äúbecause‚Äù act as important
cues for their identiÔ¨Åcation. Thus, to identify
these two relations across documents, we must
Ô¨Årst identify [AGREEMENT] or [CONFLICT] be-
tween sentences in different documents and then
determine if there is a [CONFINEMENT] or [EV-
IDENCE] relation in one of the documents.
Furthermore, the surrounding text often con-
tains contextual information that is important for
identifying these two relations. Proper handling
of surrounding context requires discourse analy-
sis and is an area of future work, but our basic
detection strategy is as follows:
1. Identifya [AGREEMENT] or [CONFLICT] re-
lation between the Query and Text
2. Search the Text sentence for cues that iden-
tify [CONFINEMENT] or [EVIDENCE]
24
3. Infer the applicability of the [CONFINE-
MENT] or [EVIDENCE] relations in the Text
to the Query
4.1 System
Overview
We have Ô¨Ånished constructing a prototype sys-
tem that detects semantic relation between state-
ments. It has a three-stage architecture similar to
the RTE system of MacCartney et al. (2006):
1. Linguistic analysis
2. Structural alignment
3. Feature extraction for detecting [EVIDENCE]
and [CONFINEMENT]
4. Semantic relation classiÔ¨Åcation
However, we differ in the following respects.
First, our relation classiÔ¨Åcation is broader than
RTE‚Äôs simple distinction between [ENTAIL-
MENT], [CONTRADICTION], and [UNKNOWN];
in place of [ENTAILMENT] and [CONTRA-
DICTION, we use broader [AGREEMENT] and
[CONFLICT] relations. We also consider cover
gradations of applicability of statements with the
[CONFINEMENT] relation.
Second, we conduct structural alignment with
the goal of aligning semantic structures. We do
this by directly incorporating dependency align-
mentsandpredicate-argumentstructureinforma-
tion for both the user query and the Web text
into the alignment scoring process. This allows
us to effectively capture many long-distance
alignments that cannot be represented as lexical
alignments. This contrasts with MacCartney et
al. (2006), who uses dependency structures for
the Hypothesis to reduce the lexical alignment
search space but do not produce structural align-
ments and do not use the dependencies in detect-
ing entailment.
Finally, we apply several rich semantic re-
sourcesinalignmentandclassiÔ¨Åcation: extended
modality information that helps align and clas-
sify structures that are semantically similar but
divergent in tense or polarity; and lexical simi-
larity through ontologies like WordNet.
4.2 Linguistic
Analysis
In order to identify semantic relations between
the user query (Q) and the sentence extracted
from Web text (T), we Ô¨Årst conduct syntactic and
semanticlinguisticanalysistoprovideabasisfor
alignment and relation classiÔ¨Åcation.
For syntactic analysis, we use the Japanese
dependency parser CaboCha (Kudo and Mat-
g32g33g34g36g35g37g29g1g7g18g13g1g19g27g24g16g9g32g24g23g13g1 g2g5g27g1g14g24g26g1g25g26g24g22g24g29g23g17g1g18g13g9g21g28g18g1g3g7g22g1g13g15g13g11g29g32g13g1g20g30g1g19g27g1g34g36g35g37g29g2 g3g6g1 g5g1g32g33g34g36g35g37g29g1
g7g18g13g1g19g27g24g16g9g32g24g23g13g1g16g13g17g11g28g1g24g27g28g13g24g25g24g26g24g27g19g27g1
g9g19g26g1g32g9g26g19g24g31g27g1g6g4g26g25g38g1g27g31g11g18g1g9g27g1g39g1g28g26g13g9g28g22g13g23g28g1
g12g14g23g31g24g21g30g1g18g9g27g1g10g13g13g23g1g27g18g24g33g23g1g2g5g1g10g22g1 g42g1 g43g1
g32g7g18
g31g11g18g1g9g27
g12g14g18g9g27
g9g9g2g5g1g27g31
g16
g39g31g11g18g9g27
g12g1
g9g1
g11g1g10g1g41g1g28g24g1g18g9g32g13g1g1g18g13g9g21g28g18g1g9g25g25g21g19g11g9g29g24g23g27g1
g43g1g41g1
g41g1
g42g1
g42g1g43g1
Figure 1: An example of structural alignment
sumoto, 2002) and the predicate-argument struc-
ture analyzer ChaPAS (Watanabe et al., 2010).
CaboCha splits the Japanese text into phrase-like
chunks and represents syntactic dependencies
between the chunks as edges in a graph. Cha-
PAS identiÔ¨Åes predicate-argument structures in
the dependency graph produced by CaboCha.
We also conduct extended modality analysis
using the resources provided by Matsuyoshi et
al. (2010), focusing on tense, modality, and po-
larity, because such information provides impor-
tant clues for the recognition of semantic rela-
tions between statements.
4.3 Structural
Alignment
In this section, we describe our approach to
structural alignment. The structural alignment
process is shown in Figure 1. It consists of the
following two phases:
1. lexical alignment
2. structural alignment
We developed a heuristic-based algorithm to
align chunk based on lexical similarity infor-
mation. We incorporate the following informa-
tion into an alignment conÔ¨Ådence score that has
a range of 0.0-1.0 and align chunk whose scores
cross an empirically-determined threshold.
‚Ä¢ surface level similarity: identical content
words or cosine similarity of chunk contents
‚Ä¢ semantic similarity of predicate-argument
structures
predicates we check for matches in predi-
cate entailment databases (Hashimoto et
al., 2009; Matsuyoshi et al., 2008) consid-
ering the default case frames reported by
ChaPAS
arguments we check for synonym or hy-
pernym matches in the Japanese WordNet
(2008) or the Japanese hypernym collec-
tion of Sumida et al. (2008)
25
g15g24g28g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g21g19g24g34g24g24g24g24g2g14g26g33g1
g15g24g30g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g24g21g19g24g34g24g24g24g24g11g7g26g33g1g41g1
T :g1
H :g1(field) (in)g1(agricultural chemicals) (ACC)g1(use)g1(field) (on)g1(agricultural chemicals) (ACC)g1(spray)g1
Figure 2: Determining the compatibility of se-
mantic structures
We compare the predicate-argument structure
of the query to that of the text and determine
if the argument structures are compatible. This
processisillustratedinFigure2wheretheT(ext)
‚ÄúAgricultural chemicals are used in the Ô¨Åeld.‚Äù is
aligned with the H(ypothesis) ‚ÄúOver the Ô¨Åeld,
agricultural chemicals are sprayed.‚Äù Although
the verbs used and sprayed are not directly se-
mantically related, they are aligned because they
share the same argument structures. This lets up
align predicates for which we lack semantic re-
sources. We use the following information to de-
termine predicate-argument alignment:
‚Ä¢ the number of aligned children
‚Ä¢ the number of aligned case frame arguments
‚Ä¢ the number of possible alignments in a win-
dow of n chunk
‚Ä¢ predicates indicating existence or quantity.
E.g. many, few, to exist, etc.
‚Ä¢ polarity of both parent and child chunks us-
ing the resources in (Higashiyama et al.,
2008; Kobayashi et al., 2005)
We treat structural alignment as a machine
learning problem and train a Support Vector Ma-
chine (SVM) model to decide if lexically aligned
chunks are semantically aligned.
We train on gold-standard labeled alignment
of 370 sentence pairs. This data set is described
in more detail in Section 5.1. As features for our
SVM model, we use the following information:
‚Ä¢ thedistanceinedgesinthedependencygraph
between parent and child for both sentences
‚Ä¢ the distance in chunks between parent and
child in both sentences
‚Ä¢ binary features indicating whether each
chunk is a predicate or argument according
to ChaPAS
‚Ä¢ the parts-of-speech of Ô¨Årst and last word in
each chunk
‚Ä¢ when the chunk ends with a case marker, the
case of the chunk , otherwise none
‚Ä¢ the lexical alignment score of each chunk
pair
4.4 Feature
Extraction for Detecting
Evidence and ConÔ¨Ånement
Once the structural alignment system has iden-
tiÔ¨Åed potential [AGREEMENT] or [CONFLICT]
relations, we need to extract contextual cues in
the Text as features for detecting [CONFINE-
MENT] and [EVIDENCE] relations. Conditional
statements, degree adverbs, and partial negation,
which play a role in limiting the scope or degree
of a query‚Äôs contents in the statement, can be im-
portant cues for detecting the these two semantic
relations. We currently use a set of heuristics to
extract a set of expressions to use as features for
classifying these relations using SVM models.
4.5 Relation
ClassiÔ¨Åcation
Once the structural alignment is successfully
identiÔ¨Åed, the task of semantic relation classi-
Ô¨Åcation is straightforward. We also solve this
problem with machine learning by training an
SVM classiÔ¨Åer. As features, we draw on a com-
bination of lexical, syntactic, and semantic infor-
mation including the syntactic alignments from
the previous section. The feature set is:
alignments We deÔ¨Åne two binary function,
ALIGNword(qi,tm) for the lexical align-
ment and ALIGNstruct((qi,qj),(tm,tk))
for the structural alignment to be true if and
only if the node qi,qj ‚àà Q has been seman-
tically and structurally aligned to the node
tm,tk ‚àà T. Q and T are the (Q)uery and the
(T)ext, respectively. We also use a separate
feature for a score representing the likelihood
of the alignment.
modality We have a feature that encodes all of
the possible polarities of a predicate node
from modality analysis, which indicates the
utterance type, and can be assertive, voli-
tional, wish, imperative, permissive, or in-
terrogative. Modalities that do not repre-
sentopinions(i.e. imperative, permissive and
interrogative) often indicate [OTHER] rela-
tions.
antonym We deÔ¨Åne a binary function
ANTONYM(qi,tm) that indicates if
the pair is identiÔ¨Åed as an antonym. This
information helps identify [CONFLICT].
26
Relation Measure 3-class Cascaded 3-class ‚àÜ
[AGREEMENT] precision 0.79 (128 / 162) 0.80 (126 / 157) +0.01
[AGREEMENT] recall 0.86 (128 / 149) 0.85 (126 / 149) -0.01
[AGREEMENT] f-score 0.82 0.82 -
[CONFLICT] precision 0 (0 / 5) 0.36 (5 / 14) +0.36
[CONFLICT] recall 0 (0 / 12) 0.42 (5 / 12) +0.42
[CONFLICT] f-score 0 0.38 +0.38
[CONFINEMENT] precision 0.4 (4 / 10) 0.8 (4 / 5) +0.4
[CONFINEMENT] recall 0.17 (4 / 23) 0.17 (4 / 23) -
[CONFINEMENT] f-score 0.24 0.29 +0.05
Table 2: Semantic relation classiÔ¨Åcation results comparing 3-class and cascaded 3-class approaches
negation To identify negations, we primar-
ily rely on a predicate‚Äôs Actuality value,
which represents epistemic modality and
existential negation. If a predicate pair
ALIGNword(qi,tm) has mismatching actu-
ality labels, the pair is likely a [OTHER].
contextual cues This set of features is used to
mark the presence of any contextual cues
that identify of [CONFINEMENT] or [EVI-
DENCE] relations in a chunk . For example,
‚Äúwp(because)‚Äù or ‚ÄúhÔøΩ(due to)‚Äù are typ-
ical contextual cues for [EVIDENCE], and ‚Äú
qV(when)‚Äù or ‚ÄúsÔøΩy(if)‚Äù are typical for
[CONFINEMENT].
5 Evaluation
5.1 Data
Preparation
In order to evaluate our semantic relation clas-
siÔ¨Åcation system on realistic Web data, we con-
structed a corpus of sentence pairs gathered from
a vast collection of webpages (2009a). Our basic
approach is as follows:
1. Retrieve documents related to a set number
of topics using the Tsubaki1 search engine
2. Extract real sentences that include major sub-
topic words which are detected based on
TF/IDF in the document set
3. Reduce noise in data by using heuristics to
eliminate advertisements and comment spam
4. Reduce the search space for identifying sen-
tence pairs and prepare pairs, which look fea-
sible to annotate
5. Annotate corresponding sentences with
[AGREEMENT], [CONFLICT], [CONFINE-
MENT], or [OTHER]
1http://tsubaki.ixnlp.nii.ac.jp/
Although our target semantic relations in-
clude [EVIDENCE], they difÔ¨Åcult annotate con-
sistently, so we do not annotate them at this
time. Expanding our corpus and semantic re-
lation classiÔ¨Åer to handle [EVIDENCE] remains
and area of future work.
The data that composes our corpus comes
from a diverse number of sources. A hand sur-
vey of a random sample of the types of domains
of 100 document URLs is given below. Half of
the URL domains were not readily identiÔ¨Åable,
but the known URL domains included govern-
mental, corporate, and personal webpages. We
believe this distribution is representative of in-
formation sources on the Internet.
type count
academic 2
blogs 23
corporate 10
governmental 4
news 5
press releases 4
q&a site 1
reference 1
other 50
We have made a partial release of our corpus
of sentence pairs manually annotated with the
correct semantic relations2. We will fully release
allthedataannotatedsemanticrelationsandwith
gold standard alignments at a future date.
5.2 Experiment
Settings
In this section, we present results of empiri-
cal evaluation of our proposed semantic rela-
tion classiÔ¨Åcation system on the dataset we con-
structed in the previous section. For this experi-
ment, we use SVMs as described in Section 4.5
2http://stmap.naist.jp/corpus/ja/
index.html (in Japanese)
27
to classify semantic relations into one of the four
classes: [AGREEMENT], [CONFLICT], [CON-
FINEMENT], or [OTHER] in the case of no re-
lation. As data we use 370 sentence pairs that
have been manually annotated both with the cor-
rect semantic relation and with gold standard
alignments. Annotations are checked by two na-
tive speakers of Japanese, and any sentence pair
where annotation agreement is not reached is
discarded. Because we have limited data that is
annotated with correct alignments and semantic
relations, we perform Ô¨Åve-fold cross validation,
training both the structural aligner and semantic
relation classiÔ¨Åer on 296 sentence pairs and eval-
uating on the held out 74 sentence pairs. The
Ô¨Ågures presented in the next section are for the
combined results on all 370 sentence pairs.
5.3 Results
We compare two different approaches to classi-
Ô¨Åcation using SVMs:
3-class semantic relations are directly classiÔ¨Åed
into one of [AGREEMENT], [CONFLICT],
and [CONFINEMENT] with all features de-
scribed in 4.5
cascaded 3-class semantic relations are Ô¨Årst
classiÔ¨Åed into one of [AGREEMENT], [CON-
FLICT] withoutcontextualcuefeatures. Then
an additional judgement with all features de-
termines if [AGREEMENT] and [CONFLICT]
should be reclassiÔ¨Åed as [CONFINEMENT]
Initial results using the 3-class classiÔ¨Åca-
tion model produced high f-scores for [AGREE-
MENT] but unfavorable results for [CONFLICT]
and [CONFINEMENT]. We signiÔ¨Åcantly im-
proved classiÔ¨Åcation of [CONFLICT] and [CON-
FINEMENT] by adopting the cascaded 3-class
model. We present these results in Table 2 and
successfully recognized examples in Table 1.
6 Discussion
and Error Analysis
We constructed a prototype semantic relation
classiÔ¨Åcation system by combining the compo-
nents described in the previous section. While
the system developed is not domain-speciÔ¨Åc and
capable of accepting queries on any topic, we
evaluate its semantic relation classiÔ¨Åcation on
several queries that are representative of our
training data.
Figure 3 shows a snapshot of the semantic re-
lation classiÔ¨Åcation system and the various se-
mantic relations it recognized for the query.
Baseline Structural Upper-boundAlignment
Precision 0.44 0.52 0.74(56/126) (96/186) (135/183)
Recall 0.30 0.52 0.73(56/184) (96/184) (135/184)
F1-score 0.36 0.52 0.74
Table 3: Comparison of lexical, structural, and
upper-bound alignments on semantic relation
classiÔ¨Åcation
In the example (6), recognized as [CONFINE-
MENT] in Figure 3, our system correctly identi-
Ô¨Åed negation and analyzed the description ‚ÄúXyl-
itol alone can not completely‚Äù as playing a role
of requirement.
(6) a.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩxÔøΩÔøΩ'wtÔøΩLUKÔøΩ
(Xylitol is effective at preventing cavi-
ties.)
b.ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩiZpxÔøΩ
ÔøΩs'wx	Z
RÔøΩdÔøΩ
(Xylitol alone can not completely prevent
cavities.)
Our system correctly identiÔ¨Åes [AGREE-
MENT] relations in other examples about re-
duced water from Table 1 by structurally align-
ing phrases like ‚Äúpromoting good health‚Äù and
‚Äúsupports the health‚Äù to ‚Äúgood for the health.‚Äù
These examples show how resources like
(Matsuyoshi et al., 2010) and WordNet (Bond et
al., 2008) have contributed to the relation clas-
siÔ¨Åcation improvement of structural alignment
over them baseline in Table 3. Focusing on sim-
ilarity of syntactic and semantic structures gives
our alignment method greater Ô¨Çexibility.
However, there are still various examples
which the system cannot recognized correctly.
In examples on cavity prevention, the phrase
‚Äúeffective at preventing cavities‚Äù could not be
aligned with ‚Äúcan prevent cavities‚Äù or ‚Äúgood for
cavity prevention,‚Äù nor can ‚Äúcavity prevention‚Äù
and ‚Äúcavity-causing bacteria control.‚Äù
The above examples illustrate the importance
of the role played by the alignment phase in the
whole system‚Äôs performance.
Table 3 compares the semantic relation classi-
Ô¨Åcation performance of using lexical alignment
only (as the baseline), lexical alignment and
structural alignment, and, to calculate the maxi-
mum possible precision, classiÔ¨Åcation using cor-
rect alignment data (the upper-bound). We can
28
Figure 3: Alignment and classiÔ¨Åcation example for the query ‚ÄúXylitol is effective at preventing
cavities.‚Äù
see that structural alignment makes it possible to
align more words than lexical alignment alone,
leading to an improvement in semantic relation
classiÔ¨Åcation. However, there is still a large gap
between the performance of structural alignment
andthemaximumpossibleprecision. Erroranal-
ysis shows that a big cause of incorrect classiÔ¨Å-
cation is incorrect lexical alignment. Improving
lexical alignment is a serious problem that must
be addressed. This entails expanding our cur-
rent lexical resources and Ô¨Ånding more effective
methods of apply them in alignment.
The most serious problem we currently face is
the feature engineering necessary to Ô¨Ånd the op-
timal way of applying structural alignments or
other semantic information to semantic relation
classiÔ¨Åcation. We need to conduct a quantita-
tive evaluation of our current classiÔ¨Åcation mod-
els and Ô¨Ånd ways to improve them.
7 Conclusion
Classifying and identifying semantic relations
between facts and opinions on the Web is of ut-
most importance to organizing information on
the Web, however, this requires consideration of
a broader set of semantic relations than are typi-
cally handled in RTE, CST, and similar tasks. In
thispaper, weintroducedasetofcross-sentential
semantic relations speciÔ¨Åcally designed for this
task that apply over both facts and opinions. We
presented a system that identiÔ¨Åes these semantic
relations in Japanese Web texts using a combina-
tion of lexical, syntactic, and semantic informa-
tion and evaluated our system against data that
was manually constructed for this task. Prelimi-
nary evaluation showed that we are able to detect
[AGREEMENT] with high levels of conÔ¨Ådence.
Our method also shows promise in [CONFLICT]
and [CONFINEMENT] detection. We also dis-
cussed some of the technical issues that need to
be solved in order to identify [CONFLICT] and
[CONFINEMENT].
Acknowledgments
This work is supported by the National Institute
ofInformationandCommunicationsTechnology
Japan.
References
Bond, Francis, Hitoshi Isahara, Kyoko Kanzaki, and
Kiyotaka Uchimoto. 2008. Boot-strapping a
wordnetusingmultipleexistingwordnets. InProc.
of the 6th International Language Resources and
Evaluation (LREC‚Äô08).
Dagan, Ido, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Proc. of the PASCAL Challenges
Workshop on Recognising Textual Entailment.
Etoh, Junji and Manabu Okumura. 2005. Cross-
document relationship between sentences corpus.
29
InProc.ofthe14thAnnualMeetingoftheAssocia-
tion for Natural Language Processing, pages 482‚Äì
485. (in Japanese).
Glickman, Oren, Ido Dagan, and Moshe Koppel.
2005. Web based textual entailment. In Proc. of
the First PASCAL Recognizing Textual Entailment
Workshop.
Hashimoto, Chikara, Kentaro Torisawa, Kow
Kuroda, Masaki Murata, and Jun‚Äôichi Kazama.
2009. Large-scale verb entailment acquisition
from the web. In Conference on Empiri-
cal Methods in Natural Language Processing
(EMNLP2009), pages 1172‚Äì1181.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2005. Recog-
nizingtextualentailmentwithlcc‚Äôsgroundhogsys-
tem. In Proc. of the Second PASCAL Challenges
Workshop.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Acquiring noun polarity knowl-
edge using selectional preferences. In Proc. of the
14th Annual Meeting of the Association for Natu-
ral Language Processing.
Jijkoun, Valentin and Maarten de Rijke. 2005. Rec-
ognizing textual entailment using lexical similar-
ity. InProc.oftheFirstPASCALChallengesWork-
shop.
Kobayashi, Nozomi, Kentaro Inui, Yuji Matsumoto,
Kenji Tateishi, and Toshikazu Fukushima. 2005.
Collecting evaluative expressions for opinion ex-
traction. Journal of natural language processing,
12(3):203‚Äì222.
Kudo, Taku and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc of CoNLL 2002, pages 63‚Äì69.
MacCartney, Bill, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D.
Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proc. of
HLT/NAACL 2006.
Marsi, Erwin and Emiel Krahmer. 2005. ClassiÔ¨Å-
cation of semantic relations by humans and ma-
chines. In Proc. of ACL-05 Workshop on Empiri-
cal Modeling of Semantic Equivalence and Entail-
ment, pages 1‚Äì6.
Matsuyoshi, Suguru, Koji Murakami, Yuji Mat-
sumoto, and Kentaro Inui. 2008. A database of re-
lations between predicate argument structures for
recognizing textual entailment and contradiction.
In Proc. of the Second International Symposium
on Universal Communication, pages 366‚Äì373, De-
cember.
Matsuyoshi, Suguru, Megumi Eguchi, Chitose Sao,
Koji Murakami, Kentaro Inui, and Yuji Mat-
sumoto. 2010. Annotating event mentions in text
with modality, focus, and source information. In
Proc. of the 7th International Language Resources
and Evaluation (LREC‚Äô10), pages 1456‚Äì1463.
Miyabe, Yasunari, Hiroya Takamura, and Manabu
Okumura. 2008. Identifying cross-document re-
lations between sentences. In Proc. of the 3rd In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-08), pages 141‚Äì148.
Murakami, Koji, Shouko Masuda, Suguru Mat-
suyoshi, Eric Nichols, Kentaro Inui, and Yuji Mat-
sumoto. 2009a. Annotating semantic relations
combining facts and opinions. In Proceedings of
the Third Linguistic Annotation Workshop, pages
150‚Äì153, Suntec, Singapore, August. Association
for Computational Linguistics.
Murakami, Koji, Eric Nichols, Suguru Matsuyoshi,
Asuka Sumida, Shouko Masuda, Kentaro Inui, and
Yuji Matsumoto. 2009b. Statement map: Assist-
ing information credibility analysis by visualizing
arguments. In Proc. of the 3rd ACM Workshop
on Information Credibility on the Web (WICOW
2009), pages 43‚Äì50.
Radev, Dragomir, Jahna Otterbacher,
and Zhu Zhang. 2003. CSTBank:
Cross-document Structure Theory Bank.
http://tangra.si.umich.edu/clair/CSTBank.
Radev, Dragomir R. 2000. Common theory of infor-
mation fusion from multiple text sources step one:
Cross-document structure. In Proc. of the 1st SIG-
dial workshop on Discourse and dialogue, pages
74‚Äì83.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in wikipedia. In Proc. of the 6th International
Language Resources and Evaluation (LREC‚Äô08).
Szpektor, Idan, Eyal Shnarch, and Ido Dagan. 2007.
Instance-basedevaluationofentailmentruleacqui-
sition. In Proc. of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
456‚Äì463.
Watanabe, Yotaro, Masayuki Asahara, and Yuji Mat-
sumoto. 2010. A structured model for joint learn-
ing of argument roles and predicate senses. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (to appear).
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and
emotions in language. Language Resources and
Evaluation, 39(2-3):165‚Äì210.
William, Mann and Sandra Thompson. 1988.
Rhetorical structure theory: towards a functional
theory of text organization. Text, 8(3):243‚Äì281.
Zhang, Zhu and Dragomir Radev. 2004. Combin-
ing labeled and unlabeled data for learning cross-
document structural relationships. In Proc. of the
Proceedings of IJC-NLP.
30

