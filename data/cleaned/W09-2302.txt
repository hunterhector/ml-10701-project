Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 10â€“18,
Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics
Statistical Phrase Alignment Model
Using Dependency Relation Probability
Toshiaki Nakazawa Sadao Kurohashi
Graduate School of Informatics, Kyoto University
Yoshida-honmachi, Sakyo-ku
Kyoto, 606-8501, Japan
nakazawa@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp
Abstract
When aligning very different language pairs,
the most important needs are the use of struc-
tural information and the capability of gen-
erating one-to-many or many-to-many corre-
spondences. In this paper, we propose a
novel phrase alignment method which models
wordorphrasedependencyrelationsindepen-
dency tree structures of source and target lan-
guages. The dependency relation model is a
kind of tree-based reordering model, and can
handle non-local reorderings which sequen-
tial word-based models often cannot handle
properly. The model is also capable of esti-
mating phrase correspondences automatically
without any heuristic rules. Experimental re-
sults of alignment show that our model could
achieve F-measure 1.7 points higher than the
conventionalwordalignmentmodelwithsym-
metrization algorithms.
1 Introduction
We consider that there are two important needs in
aligning parallel sentences written in very differ-
ent languages such as Japanese and English. One
is to adopt structural or dependency analysis into
the alignment process to overcome the difference in
word order. The other is that the method needs to
have the capability of generating phrase correspon-
dences, that is, one-to-many or many-to-many word
correspondences. Most existing alignment methods
simply consider a sentence as a sequence of words
(Brown et al., 1993), and generate phrase correspon-
dences using heuristic rules (Koehn et al., 2003).
Some studies incorporate structural information into
the alignment process after this simple word align-
ment (Quirk et al., 2005; Cowan et al., 2006). How-
ever, this is not sufficient because the basic word
alignment itself is not good.
On the other hand, a few models have been pro-
posed which use structural information from the be-
ginning of the alignment process. Watanabe et al.(2000) and Menezes and Richardson (2001) pro-
posed a structural alignment methods. These meth-
ods use heuristic rules when resolving correspon-
dence ambiguities. Yamada and Knight (2001) and
Gildea (2003) proposed a tree-based probabilistic
alignment methods. These methods reorder, insert
ordeletesub-treesononesidetoreproducetheother
side, but the constraints of using syntactic informa-
tion is often too rigid. Yamada and Knight flat-
tened the trees by collapsing nodes. Gildea cloned
sub-trees to deal with the problem. Cherry and Lin
(2003) proposed a model which uses a source side
dependency tree structure and constructs a discrim-
inative model. However, there is the defect that its
alignment unit is a word, so it can only find one-
to-one alignments. Nakazawa and Kurohashi (2008)
also proposed a model focusing on the dependency
relations. Theirmodelhastheconstraintthatcontent
words can only correspond to content words on the
other side, and the same applies for function words.
This sometimes leads to an incorrect alignment. We
have removed this constraint to make more flexi-
ble alignments possible. Moreover, in their model,
some function words are brought together, and thus
they cannot handle the situation where each func-
tion word corresponds to a different part. The small-
est unit of our model is a single word, which should
solve this problem.
10
In this paper, we propose a novel phrase align-
ment method which models word or phrase de-
pendency relations in dependency tree structures of
source and targetlanguages. Fora pair of correspon-
dences which has a parent-child relation on one side,
the dependency relation on the other side is defined
as the relation between the two correspondences.
It is a kind of tree-based reordering model, and
can capture non-local reorderings which sequential
word-based models often cannot handle properly.
The model is also capable of estimating phrase cor-
respondences automatically without heuristic rules.
The model is trained in two steps: Step 1 estimates
word translation probabilities, and Step 2 estimates
phrase translation probabilities and dependency re-
lation probabilities. Both Step 1 and Step 2 are per-
formed iteratively by the EM algorithm. During the
Step 2 iterations, word correspondences are grown
into phrase correspondences.
2 Proposed
Model
Wesuppose thatJapanese is thesource language and
English is the target language in the description of
our model. Note that the model is not specialized
for this language pair, and it can be applied to any
language pair.
Because our model uses dependency tree struc-
tures, both source and target sentences are parsed
beforehand. Japanese sentences are converted into
dependency structures using the morphological ana-
lyzer JUMAN (Kurohashi et al., 1994), and the de-
pendency analyzer KNP (Kawahara and Kurohashi,
2006). MSTparser (McDonald et al., 2005) is used
to convert English sentences. Figure 1 shows an ex-
ample of dependency structures. The root of a tree is
placed at the extreme left and words are placed from
top to bottom.
2.1 Overview
Thissectionoutlinesourproposedmodelincompar-
ison to the IBM models, which are the conventional
statistical alignment models.
In the IBM models (Brown et al., 1993), the best
alignment Ë†a between a given source sentence f and
its target sentence e is acquired by the following
equation:
Ë†a = argmax
a
p(f,a|e)
= argmax
a
p(f|e,a)Â·p(a|e) (1)
w
ï¿½
(ï¿½ ï¿½
Fï¿½
Fï¿½
Gc G7 GV
G@ Gï¿½ GV
G"
#ï¿½ Fï¿½ Fï¿½
Fï¿½
A
photogate
is
used
for
the
photodetector
.
(accept)
(light)
(device)
(photo)
(gate)
(used)
(ni)
(ha)
(wo)
Figure 1: An example of a dependency tree and its align-
ment.
where p(f|e,a) is called lexicon probability and
p(a|e) is called alignment probability.
Suppose f consists ofnwordsf1,f2,...,fn, and e
consists ofmwordse1,e2,...,em and a NULL word
(e0). The alignment mapping a consists of associa-
tions j â†’ i = aj from source position j to target
position i = aj. The two probabilities above are
broken down as:
p(f|e,a) =
Jâˆ
j=1
p(fj|eaj) (2)
p(a|e) =
Iâˆ
i=1
p(âˆ†j|ei) (3)
where âˆ†j is a relative position of words in the
source side which corresponds to ei. Equation 2 is
the product of the word translation probabilities, and
Equation 3 is the product of relative position proba-
bilities.
In the proposed model, we refine the IBM models
in three ways. First, as for Equation 2, we consider
phrases instead of words. Second, as for Equation 3,
we consider dependencies of words instead of their
positions in a sentence.
Finally, the proposed model can find the best
alignment Ë†a by not using f-to-e alone, but simulta-
neously with e-to-f. That is, Equation 1 is modified
as follows:
Ë†a = argmax
a
p(f|e,a)Â·p(a|e)Â·
p(e|f,a)Â·p(a|f) (4)
Since our model regards a phrase as a basic unit,
the above formula is calculated in a straightforward
way. In contrast, the IBM models can consider
a many-to-one alignment by combining one-to-one
11
alignments, but they cannot consider a one-to-many
or many-to-many alignment.
The models are estimated by EM-like algorithm
which is very similar to (Liang et al., 2006). The
important difference is that we are using tree struc-
tures.
We maximize the data likelihood:
max
Î¸ef,Î¸fe
âˆ‘
f,e
(logpef(f,e;Î¸ef) + logpfe(f,e;Î¸fe))
(5)
In the E-step, we compute the posterior distribution
of the alignments with the current parameter Î¸:
q(a;f,e) := pef(a|f,e;Î¸ef)Â·pfe(a|f,e;Î¸fe) (6)
In the M-step, we update the parameter Î¸:
Î¸prime := argmax
Î¸
âˆ‘
a,f,e
q(a;f,e)logpef(a,f,e;Î¸ef)
+
âˆ‘
a,f,e
q(a;f,e)logpfe(a,f,e;Î¸fe)
= argmax
Î¸
âˆ‘
a,f,e
q(a;f,e)logp(e)Â·pef(a,f|e;Î¸ef)
+
âˆ‘
a,f,e
q(a;f,e)logp(f)Â·pfe(a,e|f;Î¸fe)
(7)
Note that p(e) and p(f) have no effect on maxi-
mization, and pef(a,f|e;Î¸ef) and pfe(a,e|f;Î¸fe)
appeared in Equation 1 or Equation 4.
In the following sections, we decompose the lexi-
con probability and alignment probability.
2.2 Phrase
Translation Probability
Suppose f consists of N phrases F1,F2,...,FN, and
e consists of M phrases E1,E2,...,EM. The align-
ment mapping a consists of associations j â†’ i =
Aj from source phrase j to target phrase i = Aj.
We consider phrase translation probability
p(Fj|Ei) instead of word translation probability.
There is one restriction: that phrases composed of
more than one word cannot be aligned to NULL.
Only a single word can be aligned to NULL.
We denote a phrase which the word fj belongs to
as Fs(j), and a phrase which the word ei belongs to
as Et(i). With these notations, we refine Equation 2
as follows:
p(f|e,a) =
Jâˆ
j=1
p(Fs(j)|EAs(j)) (8)
Suppose phrase Fj and Ei are aligned where the
number of words in Fj is denoted by |Fj| and that
number in Ei is |Ei|, the probability mass related to
this alignment in Equation 8 is as follows:
p(Fj|Ei)|Fj| Â·p(Ei|Fj)|Ei| (9)
We call this probability for the link between Fj and
Ei phrase alignment probability. The upper part of
Table 1 shows phrase alignment probabilities for the
alignment in Figure 1.
2.3 Dependency
Relation Probability
The reordering model in the IBM Models is defined
on the relative position between an alignment and
its previous alignment, as shown in Equation 3. Our
model, on the other hand, considers dependencies of
words instead of positional relations.
We start with a dependency relation where fc de-
pends on fp in the source sentence. In a possible
alignment, fc belongs to Fs(c), fp belongs to Fs(p),
andFs(c) dependsonFs(p). Inthissituation, wecon-
sider the relation between EAs(p) and EAs(c). Even
if two languages have different word order, their de-
pendency structures are similar in many cases, and
EAs(c) tends to depend on EAs(p). Our model takes
this tendency into consideration. In order to de-
note the relationship between phrases, we introduce
rel(EAs(p),EAs(c)). This is defined as the path from
EAs(p) to EAs(c). It is represented by applying the
notations below:
â€¢ â€™câ€™ if going down to the child node
â€¢ â€™pâ€™ if going down to the parent node
For example, in Figure 1, the path from â€œforâ€ to
â€œphotodetectorâ€ is â€™câ€™, from â€œtheâ€ to â€œforâ€ is â€™p;pâ€™
because it travels across two nodes. All the phrases
are considered as a single node, so the path from
â€œphotogateâ€ to â€œtheâ€ is â€™p;c;c;câ€™ with the alignment
in Figure 1.
We refine Equation 3 using rel as follows:
p(a|e) =
âˆ
(p,c)âˆˆDs-pc
pt(rel(EAs(p),EAs(c))|pc)
(10)
where Ds-pc denotes a set of parent-child
word pairs in the source sentence. We call
pt(rel(EAs(p),EAs(c))|pc) target side dependency
relation probability. pt is a kind of tree-based
reordering model.
12
Table 1: A probability calculation example.
Source Target Phrase alignment probability
	! ï¿½ 
ï¿½ï¿½ photodetector p(	! ï¿½ 
ï¿½ï¿½ |photodetector)3 Â·p(photodetector| 	! ï¿½ 
ï¿½ï¿½ )
t x for p(t x |for)2 Â·p(for|t x )
ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ photogate p(ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ |a photogate)2 Â·p(a photogate|ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ )2
ï¿½ ;Mh is used p(ï¿½ ;Mh |is used)2 Â·p(is used|ï¿½ ;Mh )2
NULL the p(the|NULL)
Source Target dependency Target Source dependency
c p relation probability c p relation probability
	! ï¿½ pt(SAME|pc) A photogate ps(SAME|pc)
ï¿½ 
ï¿½ï¿½ pt(SAME|pc) photogate is ps(c|pc)

ï¿½ï¿½ t pt(c|pc) used is ps(SAME|pc)
t x pt(SAME|pc) for used ps(c|pc)
x ;Mh pt(c|pc) the photodetector ps(NULL c|pc)
ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ pt(SAME|pc) photodetector for ps(c|pc)
ï¿½ï¿½ï¿½ ï¿½ pt(c|pc)
ï¿½ ;Mh pt(SAME|pc)
There are some special cases for rel. When Fs(c)
and Fs(p) are the same, that is, fc and fp belong
to the same phrase, rel is represented as â€™SAMEâ€™.
Whenfp isalignedtoNULL,fc isalignedtoNULL,
and both of them are aligned to NULL, rel is repre-
sented as â€™NULL pâ€™, â€™NULL câ€™, and â€™NULL bâ€™, re-
spectively. The lower part of Table 1 shows depen-
dency relation probabilities corresponding to Figure
1.
Actually, we extend the dependency relation
probability to consider a wider relation, i.e, the
grandparent-child relation, as follows:
p(a|e) =
âˆ
(p,c)âˆˆDs-pc
pt(rel(EAs(p),EAs(c))|pc)Â·
âˆ
(g,c)âˆˆDs-gc
pt(rel(EAs(g),EAs(c))|gc)
(11)
where Ds-gc denotes a set of grandparent-child word
pairs in the source sentence.
3 Model
Training
Our model is trained in two steps. In Step 1, word
translation probability is estimated. Then, in Step 2,
possible phrases are acquired, and both phrase trans-
lation probability and dependency relation probabil-
ity are estimated. In both steps, parameter estima-
tion is done with the EM algorithm.
3.1 Step
1
In Step 1, word translation probability in each di-
rection is estimated independently. This is done in
exactly the same way as in IBM Model 1.
In this process, the alignment unit is a word.
When we consider f-to-e alignment, each word on
the source side fj can correspond to a word on the
target side ei or a NULL word, independently of
other source words. The probability of one possible
alignment a is calculated as follows:
p(a,f|e) =
Jâˆ
j=1
p(fj|eaj) (12)
By considering all possible alignments, p(f|e) is
calculated as:
p(f|e) =
âˆ‘
a
p(a,f|e) (13)
As initial parameters of p(f|e), we use uniform
probabilities. Then, after calculating Equation 12
and 13, we give the fractional count p(a,f|e)p(f|e) to all
word alignments in a, and we estimate p(f|e) by
MLE. We perform this estimation iteratively.
The inverse model e-to-f can be calculated in the
same manner.
3.2 Step
2
Both phrase translation probability and dependency
relation probability are estimated, and one undi-
rectedalignmentis foundusing the e-to-f and f-to-e
probabilities simultaneously in this step. In contrast
to Step 1, it is impossible to enumerate all the possi-
ble alignments. To find the best alignment, we first
create an initial alignment based on phrase trans-
lation probability only, and then gradually revise it
13
by considering the dependency relation probability
with a hill-climbing algorithm.
The initial parameters of Step 2 are calculated
as follows. The dependency relation probability is
calculated using the final alignment result of Step
1, and we use the word translation probability esti-
mated in Step 1 as the initial phrase translation prob-
ability.
3.2.1 Initial
Alignment
We first create an initial alignment based on the
phrase translation probability without considering
the dependency relation probabilities.
For all the combinations of possible phrases
(including NULL), phrase alignment probabilities
are calculated (equation 9). Correspondences are
adopted one by one in descending order of geomet-
ric mean of the phrase alignment probabilities. All
the words should be aligned only once, that is, the
correspondences are adopted exclusively. Genera-
tionofpossiblephrasesisexplainedinSection3.2.3.
3.2.2 Hill-climbing
To find better alignments, the initial alignment is
graduallyrevisedwithahill-climbingalgorithm. We
use four kinds of revising operations:
Swap: Focusing on any two correspondences, the
partners are swapped. In the first step in
Figure2, the correspondences â€œï¿½ â†” photo-
gateâ€andâ€œï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ â†”photodetectorâ€are
swapped to â€œï¿½ â†” photodetectorâ€ and â€œï¿½ï¿½ï¿½
ï¿½ï¿½ï¿½ â†” photogateâ€.
Extend: Focusing on one correspondence, the
source or target phrase is extended to include
its neighboring (parent or child) NULL-aligned
word.
Add: A new correspondence is added between a
source word and a target word both of which
are aligned to NULL.
Reject: Acorrespondenceisrejectedandthesource
and target phrase are aligned to NULL.
Figure 2 shows an illustrative example of hill
climbing. The alignment is revised only if the align-
ment probability gets increased. It is repeated un-
til no operation can improve the alignment probabil-
ity, and the final state is the best approximate align-
ment. As a by-product of hill-climbing, pseudo n-
best alignment can be acquired. It is used in collect-
ing fractional counts.
3.2.3 Phrase
Generation
If there is a word which is aligned to NULL in the
best approximate alignment, a new possible phrase
is generated by merging the word into a neighbor-
ing phrase which is not aligned to NULL. In the last
alignment result in Figure 2, for example, â€œ
ï¿½ï¿½ â€
is treated as being included in the correspondence
between â€œ	! ï¿½ â€ and â€œphotodetectorâ€ and the cor-
respondence between â€œt â€ and â€œforâ€. As a result,
we consider the correspondence between â€œ	! ï¿½ 
ï¿½
ï¿½ â€ and â€œphotodetectorâ€ and the correspondence be-
tween â€œ
ï¿½ï¿½ t â€ and â€œforâ€ existing in parallel sen-
tences. The new possible phrase is taken into con-
sideration from the next iteration.
3.2.4 Model
Estimation
Collecting all the alignment results, we estimate
phrase alignment probabilities and dependency rela-
tion probabilities.
One way of estimating parameters of phrase
alignment probabilities is using the following equa-
tions:
p(Fj|Ei) = C(Fj,Ei)âˆ‘
k C(Fk,Ei)
p(Ei|Fj) = C(Fj,Ei)âˆ‘
k C(Ek,Fj)
(14)
where C(Fj,Ei) is a frequency of Fj and Ei is
aligned.
However, if we use this in our model, the phrase
translation probability of the new possible phrase
can become extremely high (often it becomes 1).
To avoid this problem, we use the equations below
for the estimation of phrase translation probability
in place of Equation 14:
p(Fj|Ei) = C(Fj,Ei)C(E
i),p(Ei|Fj) = C(Fj,Ei)C(F
j)
(15)
C(Ei) is the frequency of the phrase Ei in the train-
ingcorpuswhichcanbepre-counted. Thisdefinition
can resolve the problem where the phrase translation
probability of the new possible phrase becomes too
high.
As for the NULL, we use Equation 14 because we
cannot pre-count the frequency of NULL.
Using the estimated phrase alignment probabil-
ities and dependency relation probabilities, we go
back to the initial alignment described in Section
3.2.1 iteratively.
14
w
ï¿½
(ï¿½ ï¿½
Fï¿½
Fï¿½
Gc G7 GV
G@ Gï¿½ GV
G"
#ï¿½ Fï¿½ Fï¿½
Fï¿½
A
photogate
is
used
for
the
photodetector
.
w
ï¿½
(ï¿½ ï¿½
Fï¿½
Fï¿½
Gc G7 GV
G@ Gï¿½ GV
G"
#ï¿½ Fï¿½ Fï¿½
Fï¿½
A
photogate
is
used
for
the
photodetector
.
w
ï¿½
(ï¿½ ï¿½
Fï¿½
Fï¿½
Gc G7 GV
G@ Gï¿½ GV
G"
#ï¿½ Fï¿½ Fï¿½
Fï¿½
A
photogate
is
used
for
the
photodetector
.
w
ï¿½
(ï¿½ ï¿½
Fï¿½
Fï¿½
Gc G7 GV
G@ Gï¿½ GV
G"
#ï¿½ Fï¿½ Fï¿½
Fï¿½
A
photogate
is
used
for
the
photodetector
.
w
ï¿½
(ï¿½ ï¿½
Fï¿½
Fï¿½
Gc G7 GV
G@ Gï¿½ GV
G"
#ï¿½ Fï¿½ Fï¿½
Fï¿½
A
photogate
is
used
for
the
photodetector
.
Ini al!alignment
Swap Reject
Add Extend
(accept)
(light)
(device)
(ni)
(ha)
(photo)
(gate)
(used)
(wo)
Figure 2: An example of hill-climbing.
4 Experimental
Results
We conducted alignment experiments. A JST1
Japanese-English paper abstract corpus consisting
of 1M parallel sentences was used for the model
training. This corpus was constructed from a 2M
Japanese-English paper abstract corpus by NICT2
using the method of Uchiyama and Isahara (2007).
As gold-standard data, we used 475 sentence pairs
which were annotated by hand. The annotations
were only sure (S) alignments (there were no possi-
ble (P) alignments) (Och and Ney, 2003). The unit
of evaluation was word-base for both Japanese and
English. We used precision, recall, and F-measure
as evaluation criteria.
We conducted two experiments to reveal 1) the
contribution of our proposed model compared to the
existing models, and 2) the effectiveness of using
dependency tree structure and phrases, which are
larger alignment units than words. Trainings were
run on the original forms of words for both the pro-
posed model and the models used for comparison.
4.1 Comparison
with Word Sequential Model
For comparison, we used GIZA++ (Och and Ney,
2003) which implements the prominent sequential
word-base statistical alignment model of IBM Mod-
els. We conducted word alignment bidirectionally
with its default parameters and merged them using
three types of symmetrization heuristics (Koehn et
al., 2003). The results are shown in Table 2.
1http://www.jst.go.jp/
2http://www.nict.go.jp/
The result of â€™Step 1â€™ uses parameters estimated
after 5 iterations of Step 1. The alignment is ob-
tained by the method of initial alignment shown in
Section 3.2.1. In â€™Step 2-1â€™, the phrase translation
probabilities are the same as those in â€™Step 1â€™. In ad-
dition, dependency relation probabilities estimated
fromtheâ€™Step1â€™alignmentresultareused. Bycom-
paring â€™Step 1â€™ and â€™Step 2-1â€™, we can see the ef-
fectiveness of dependency relation probability. We
performed 5 iterations for Step 2 and calculated the
alignment accuracy each time. As a result, the pro-
posed model could achieve a higher F-measure by
1.7 points compared to the sequential model. â€™In-
tersectionâ€™ achieved best Precision, but its Recall is
quite low. â€™grow-diag-final-andâ€™ achieved best Re-
call, but its Precision is lower than our best result
where the Recall is almost same. Thus, we can say
our result is better than sequential word alignment
models.
4.2 Effectiveness
of Dependency Trees and
Phrases
Toconfirmtheeffectivenessofdependencytreesand
phrases,weconductedalignmentexperimentsonthe
following four conditions:
â€¢ Using both dependency trees and phrases (re-
ferred to as â€™proposedâ€™).
â€¢ Using dependency trees only.
â€¢ Using phrases only.
â€¢ Notusingdependencytreesorphrases(referred
to as â€™noneâ€™)
For the conditions which do not use dependency
trees, we used positional relations of a sentence as
15

