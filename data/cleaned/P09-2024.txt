Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 93–96,
Suntec, Singapore, 4 August 2009. c 2009 ACL and AFNLP
Automatic Generation of Information-seeking Questions Using Concept
Clusters
Shuguang Li
Department of Computer Science
University of York, YO10 5DD, UK
sgli@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
University of York, YO10 5DD, UK
suresh@cs.york.ac.uk
Abstract
One of the basic problems of efficiently
generating information-seeking dialogue
in interactive question answering is to find
the topic of an information-seeking ques-
tion with respect to the answer documents.
In this paper we propose an approach to
solving this problem using concept clus-
ters. Our empirical results on TREC col-
lections and our ambiguous question col-
lection shows that this approach can be
successfully employed to handle ambigu-
ous and list questions.
1 Introduction
Question Answering systems have received a lot
of interest from NLP researchers during the past
years. But it is often the case that traditional QA
systems cannot satisfy the information needs of
the users as the question processing part may fail
to properly classify the question or the informa-
tion needed for extracting and generating the an-
swer is either implicit or not present in the ques-
tion. In such cases, interactive dialogue is needed
to clarify the information needs and reformulate
the question in a way that will help the system to
find the correct answer.
Due to the fact that casual users often ask ques-
tions with ambiguity and vagueness, and most of
the questions have multiple answers, current QA
systems return a list of answers for most questions.
The answers for one question usually belong to
different topics. In order to satisfy the information
needs of the user, information-seeking dialogue
should take advantage of the inherent grouping of
the answers.
Several methods have been investigated for gen-
erating topics for questions in information-seeking
dialogue. Hori et al. (2003) proposed a method
for generating the topics for disambiguation ques-
tions. The scores are computed purely based on
the syntactic ambiguity present in the question.
Phrases that are not modified by other phrases are
considered to be highly ambiguous while phrases
that are modified are considered less ambiguous.
Small et al. (2004) utilizes clarification dialogue
to reduce the misunderstanding of the questions
between the HITIQA system and the user. The
topics for such clarification questions are based
on manually constructed topic frames. Similarly
in (Hickl et al., 2006), suggestions are made to
users in the form of predictive question and answer
pairs (known as QUABs) which are either gener-
ated automatically from the set of documents re-
turned for a query (using techniques first described
in (Harabagiu et al., 2005), or are selected from a
large database of questions-answer pairs created
offline (prior to a dialogue) by human annotators.
In Curtis et al. (2005), query expansion of the
question based on Cyc Knowledge is used to gen-
erate topics for clarification questions. In Duan et
al. (2008), the tree-cutting model is used to select
topics from a set of relevant questions from Yahoo
Answers.
None of the above methods consider the con-
texts of the list of answers in the documents re-
turned by QA systems. The topic of a good
information-seeking question should not only be
relevant to the original question but also should be
able to distinguish each answer from the others so
that the new information can reduce the ambiguity
and vagueness in the original question. Instead of
using traditional clustering methods on categoriza-
tion of web results, we present a new topic gener-
ation approach using concept clusters and a sepa-
rability scoring mechanism for ranking the topics.
2 Topic
Generation Based on Concept
Clustering
Text categorization and clustering especially hier-
archical clustering are predominant approaches to
organizing large amounts of information into top-
93
ics or categories. But the main issue of catego-
rization is that it is still difficult to automatically
construct a good category structure, and manu-
ally formed hierarchies are usually small. And the
main challenge of clustering algorithms is that the
automatically formed cluster hierarchy may be un-
readable or meaningless for human users. In order
to overcome the limits of the above methods, we
propose a concept clusters method and choose the
labels of the clusters as topics.
Recent research on automatically extracting
concepts and clusters of words from large database
makes it feasible to grow a big set of concept clus-
ters. Clustering by Committee (CBC) in Pantel
et al. (2002) made use of the fact that words in
the same cluster tend to appear in similar con-
texts. Pasca et al. (2008) utilized Google logs and
lexico-syntactic patterns to get clusters with labels
simultaneously. Google also released Google Sets
which can be used to grow concept clusters with
different sizes.
Currently our clusters are the union of the sets
generated by the above three approaches, and
we label them using the method described in
Pasca et al. (2008). We define the concept
clusters in our collection as {C1,C2,...,Cn}.
Ci={ei1,ei2,...,eim}, eij is jth subtopic of clus-
ter Ci and m is the size of Ci.
We designed our system to take a question
and its corresponding list of answers as input
and then retrieve Google snippet documents for
each of the answers with respect to the ques-
tion. In a vectorspace model, a document is
represented by a vector of keywords extracted
from the document, with associated weights rep-
resenting the importance of the keywords in the
document and within the whole document col-
lection. A document Dj in the collection is
represented as {W0j,W1j,...,Wnj}, and Wij is
the weight of word i in document j. Here we
use our concept clusters to create concept clus-
ter vectors. A document Dj now is represented
as <WC1j,WC2j,...,WCnj>, and WCij is the
score vector of document Dj for concept cluster
Ci:
WCij = <Scorej(ei1),Scorej(ei2),...Scorej(eim)>
Scorej(eip) is the weight of subtopic eip of cluster Ci in
document Dj.
Currently we use tf-idf scheme (Yang et al., 1999)
to calculate the weight of subtopics.
3 Concept
Cluster Separability Measure
We view different concept clusters from the con-
texts of the answers as different groups of fea-
tures that can be used to classify the answers docu-
ments. We rank different context features by their
separability on the answers. Currently our system
retrieves the answers from Google search snippets,
and each snippet is quite short. So we combine the
top 50 snippets for one answer into one document.
One answer is associated with one such big doc-
ument. We propose the following interclass mea-
sure to compare the separability of different clus-
ters:
Score(Ci) = DN
Nsummationdisplay
p<q
Dis(Dp,Dq),
D is the Dimension Penalty score, D = 1M ,
M is the size of cluster Ci,
N is the combined total number of classes from all the answers
Dis(Dp,Dq) =
radicalBigg nsummationdisplay
m=0
(Scorep(eim)−Scoreq(eim))2
We introduce D, the ”Dimension Penalty” score
which gives higher penalty to bigger clusters. Cur-
rently we use the reciprocal of the size of the clus-
ter. The second part is the average pairwise dis-
tance between answers. N is the total number of
classes of the answers. Next we describe in detail
how to use the concept cluster vectors and separa-
bility measure to rank clusters.
4 Cluster
Ranking Algorithm
Input:
Answer set A ={A1,A2,...,Ap};
Documents set D ={D1,D2,...,Dp}associated with answer set A;
Concept cluster set CS ={Ci|some of the subtopics from Ci occurs in D};
Threshold Θ1, Θ2; The question Q;
Concept cluster set QS ={Ci|some of the subtopics from Ci occurs in Q}
Output:
T ={< Ci,Score >}, a set of pairs of a concept cluster and its ranking
score;
QS;
Variables: X, Y ;
Steps:
1. CS = CS−QS
2. For each cluster Ci in CS
3. X = No. of answers in which context subtopics from Ci are present;
4. Y = No. of subtopics from Ci that occurs in the answers’ contexts;
5. If X < Θ1 or Y < Θ2
6. delete Ci from CS
7. continue
8. Represent every document as a concept cluster vector on Ci (see
section 2)
9. Calculate the Score(Ci) using our separability measure
10. Store < Ci,Score > in T
11. return T the medoid.
Figure 1: Concept Cluster Ranking Algorithm
Figure 1 describes the algorithm for rank-
ing concept clusters based on their separabil-
ity score. This algorithm starts by deleting all
94
the clusters which are in QS from CS so that
we only focus on the context clusters whose
subtopics are present in the answers. However
in some cases this assumption is incorrect1. Tak-
ing the question shown in Table 2 for example,
there are 6 answers for question LQ1, and in
Step 1 CS = {C41American State, C1522Times,
C414Tournament, C10004Year, ...} and QS =
{C4545Event}. Using cluster C414 (see Table 2),
D = {D1{Daytona 500, 24 Hours of Daytona,
24 Hours of Le Mans, ...}, D2{3M Performance
400, Cummins 200, ...}, D3{Indy 500, Truck se-
ries, ...}, ...}, and hence the vector representa-
tion for a given document Dj using C414 will
be <Scorej(indy 500), Scorej(Cummins 200),
Scorej(daytona 500), ...>.
In Step 2 through 11 from Figure 1, for each
context cluster Ci in CS we calculate X (the num-
ber of answers in which context subtopics from Ci
are present), and Y (the number of subtopics from
Ci that occurs in the answers’ contexts). We would
like the clusters to hold two characteristics: (a) at
least occur in Θ1 answers as we want to have a
cluster whose subtopics are widely distributed in
the answers. Currently we set Θ1 as half the num-
ber of the answers; (b) at least have Θ2 subtopics
occurring in the answers’ documents. We set Θ2
as the number of the answers. For example, for
cluster C414, X = 6, Y = 10, Θ1 = 3 and Θ2 =
6, so this cluster has the above two characteris-
tics. If a cluster has the above two characteris-
tics, we use our separability measure described in
section 3 to calculate a score for this cluster. The
size of C414 is 11, so Score(C414) = 111×6 summationtextNp<q
Dis(Dp,Dq). Ranking the clusters based on this
separability score means we will select a clus-
ter which has several subtopics occurring in the
answers and the answers are distinguished from
each other because they belong to these different
subtopics. The top three clusters for question LQ1
is shown in Table 2.
5 Experiment
5.1 Data
Set and Baseline Method
To the best of our knowledge, the only available
test data of multiple answer questions are list ques-
tions from TREC 2004-2007 Data. For our first
1For the question ”In which movies did Christopher
Reeve acted?”, cluster Actor{Christopher Reeve, michael
caine, anthony hopkins, ...} is quite useful. While for ”Which
country won the football world cup?” cluster Sports{football,
hockey, ...} is useless.
list question collection we randomly selected 200
questions which have at least 3 answers. We
changed the list questions to factoid ones with
additional words from their context questions to
eliminate ellipsis and reference. For the ambigu-
ous questions, we manually choose 200 questions
from TREC 1999-2007 data and some questions
discussed as examples in Hori et al. (2003) and
Burger et al. (2001).
We compare our approach with a baseline
method. Our baseline system does not rank the
clusters by the above separability score instead it
prefers the cluster which occurs in more answers
and have more subtopics distributed in the answer
documents. If we still use X to represent the num-
ber of answers in which context subtopics from
one cluster are present and Y to represent the num-
ber of subtopics from this cluster that occurs in the
answers’ contexts, for the baseline system, we will
use X×Y to rank all the concept clusters found
in the contexts.
5.2 Results
and Error Analysis
We applied our algorithm on the two collections
of questions. Two assessors were involved in the
manual judgments with an inter-rater agreement
of 97%. For each approach, we obtained the top
20 clusters based on their scores. Given a clus-
ter with its subtopics in the contexts of the an-
swers, an assessor manually labeled each cluster
’good’ or ’bad’. If it is labeled ’good’, the cluster
is deemed relevant to the question and the clus-
ter’s label could be used as dialogue seeking ques-
tion’s topic to distinguish one answer from the oth-
ers. Otherwise, the assessor will label a cluster as
’bad’. We use the above two ranking approaches
to rank the clusters for each question. Table 1 pro-
vides the statistics of the performance on the the
two question collection. List B means the base-
line method on the list question set while Am-
biguous S means our separability method on the
ambiguous questions. The ’MAP’ column is the
mean of average precisions over the set of clusters.
The ’P@1’ column is the precision of the top one
cluster while the ’P@3’ column is the precision
of the top three clusters2. The ’Err@3’ column is
the percentage of questions whose top three clus-
ters are all labeled ’bad’. One example associated
with the manually constructed desirable questions
2’P@3’ is the number of ’good’ clusters out of the top
three clusters
95
Table 1: Experiment results
Methods MAP P@1 P@3 Err@3
List B 41.3% 42.1% 27.7% 33.0%
List S 60.3% 90.0% 81.3% 11.0%
Ambiguous B 31.1% 33.2% 21.8% 47.1%
Ambiguous S 53.6% 71.1% 64.2% 29.7%
Table 2: TREC Question Examples
LQ1: Who is the winners of the NASCAR races?
1st C414(Tournament):{indy 500, Cummins 200, day-
tona 500, ...}
Q1 Which Tournament are you interested in?
2nd C41(American State):{houston, baltimore, los an-
geles, ...}
Q2 Which American State were the races held?
3rd C1522(Times):{once, twice, three times, ...}
Q3 How many times did the winner win?
is shown in Table 2.
From Table 1, we can see that our approach
outperforms the baseline approach in terms of all
the measures. We can see that 11% of the ques-
tions have no ‘good’ clusters. Further analysis
of the answer documents shows that the ‘bad’
clusters fall into four categories. First, there are
noisy subtopics in some clusters. Second, some
questions’ clusters are all labeled ‘bad’ because
the contexts for different answers are too simi-
lar. Third, unstructured web document soften con-
tain multiple subtopics. This means that different
subtopics are in the context of the same answer.
Currently we only look for context words while
not using any scheme to specify whether there is a
relationship between the answer and the subtopics.
Finally, for other ‘bad’ cases and the questions
with no good clusters all of the separability scores
are quite low. This is because the answers fall
into different topics which do not share a common
topic in our cluster collection.
6 Conclusion
and Discussion
This paper proposes a new approach to solve
the problem of generating an information-seeking
question’s topic using concept clusters that can be
used in a clarification dialogue to handle ambigu-
ous questions. Our empirical results show that this
approach leads to good performance on TREC col-
lections and our ambiguous question collections.
The contribution of this paper are: (1) a new con-
cept cluster method that maps a document into a
vector of subtopics; (2) a new ranking scheme to
rank the context clusters according to their sepa-
rability. The labels of the chosen clusters can be
used as topics in an information-seeking question.
Finally our approach shows significant improve-
ment (nearly 48% points) over comparable base-
line system.
But currently we only consider the context clus-
ters while ignoring the clusters associated with the
questions. In the future, we will further investigate
the relationships between the concept clusters in
the question and the answers.
References
Tiphaine Dalmas, Bonnie L. Webber: Answer com-
parison in automated question answering. J. Applied
Logic (JAPLL) 5(1):104-120, (2007).
Chiori Hori, Sadaoki Furui: A new approach to auto-
matic speech summarization. IEEE Transactions on
Multimedia (TMM) 5(3):368-378, (2003).
Sharon Small and Tomek Strzalkowski, HITIQA:
A Data Driven Approach to Interactive Analyti-
cal Question Answering, in Proceedings of HLT-
NAACL 2004: Short Papers, (2004).
Andrew Hickl, Patrick Wang, John Lehmann, Sanda
M. Harabagiu: FERRET: Interactive Question-
Answering for Real-World Environments. ACL,
(2006).
Sanda M. Harabagiu, Andrew Hickl, John Lehmann,
Dan I. Moldovan: Experiments with Interactive
Question-Answering. ACL, (2005).
John Burger et al.: Issues, Tasks and Program Struc-
tures to Roadmap Research in Question and An-
swering (Q&A),DARPA/NSF committee publica-
tion, (2001).
Patrick Pantel, Dekang Lin: Document clustering with
committees. SIGIR 2002:199-206, (2002).
Marius Pasca and Benjamin Van Durme: Weakly-
Supervised Acquisition of Open-Domain Classes
and Class Attributes from Web Documents and
Query Logs. ACL, (2008).
Sanda M. Harabagiu, Andrew Hickl, V. Finley La-
catusu: Satisfying information needs with multi-
document summaries. Inf. Process. Manage. (IPM)
43(6):1619-1642, (2007).
Huizhong Duan, Yunbo Cao, Chin-Yew Lin and Yong
Yu: Searching Questions by Identifying Question
Topic and Question Focus. ACL, (2008).
Jon Curtis, G. Matthews and D. Baxter: On the Effec-
tive Use of Cyc in a Question Answering System.
IJCAI Workshop on Knowledge and Reasoning for
Answering Questions, Edinburgh, (2005).
96

