Proceedings of NAACL HLT 2007, Companion Volume, pages 165–168, Rochester, NY, April 2007.
c©2007 Association for Computational Linguistics Virtual Evidence for Training Speech Recognizers using Partially Labeled data Amarnag Subramanya Department of Electrical Engineering University of Washington Seattle, WA 98195-2500 asubram@u.washington.edu Jeff Bilmes Department of Electrical Engineering University of Washington Seattle, WA 98195-2500 bilmes@ee.washington.edu Abstract Collecting supervised training data for automatic speech recognition (ASR) systems is both time consuming and expensive.
In this paper we use the notion of virtual evidence in a graphical-model based system to reduce the amount of supervisory training data required for sequence learning tasks.
We apply this approach to a TIMIT phone recognition system, and show that our VE-based training scheme can, relative to a baseline trained with the full segmentation, yield similar results with only 15.3% of the frames labeled (keeping the number of utterances xed).
1 Introduction
Current state-of-the-art speech recognizers use thousands of hours of training data, collected from a large number of speakers with various backgrounds in order to make the models more robust.
It is well known that one of the simplest ways of improving the accuracy of a recognizer is to increase the amount of training data.
Moreover, speech recognition systems can bene t from being trained on hand-transcribed data where all the appropriate word level segmentations (i.e., the exact time of the word boundaries) are known.
However, with increasing amounts of raw speech data being made available, it is both time consuming and expensive to accurately segment every word for every given sentence.
Moreover, for languages for which only a small amount of training data is available, it can be expensive and challenging to annotate with precise word transcriptions the researcher may have no choice but to use partially erroneous training data.
There are a number of different ways to label data used to train a speech recognizer.
First, the most expensive case (from an annotation perspective) is fully supervised training, where both word sequences and time segmentations are completely speci ed1.
A second case is most commonly used in speech recognition systems, where only the word sequences of utterances are given, but their precise segmentations are unknown.
A third case falls under the realm of semi-supervised approaches.
As one possible example, a previously trained recognizer is used to generate transcripts for unlabeled data, which are then used to re-train the recognizer based on some measure of recognizer con dence (Lamel et al., 2002).
The above cases do not exhaust the set of possible training scenarios.
In this paper, we show how the notion of virtual evidence (VE) (Pearl, 1988) may be used to obtain the bene ts of data with time segmentations but using only partially labeled data.
Our method lies somewhere between the rst and second cases above.
This general framework has been successfully applied in the past to the activity recognition domain (Subramanya et al., 2006).
Here we make use of the TIMIT phone recognition task as an example to show how VE may be used to deal with partially labeled speech training data.
To the best of our knowledge, this paper presents the rst system to express training uncertainty using VE in the speech domain.
2 Baseline
System Figure 1 shows two consecutive time slices of a dynamic Bayesian network (DBN) designed for con1This does not imply that all variables are observed during training.
While the inter-word segmentations are known, the model is not given information about intra-word segmentations.
165 Transition Transition Phone Phone Position State Phone State C =1 C =1 S H A R P OObservation t 1 t 1 P H St 1 Rt 1 At 1 Ot 1 t t t t t t tt 1VE observed child VE applied via this CPT VE applied via this CPT Figure 1: Training Graph.
text independent (CI) phone recognition.
All observed variables are shaded, deterministic dependences are depicted using solid black lines, value speci c dependences are shown using a dot-dash lines, and random dependencies are represented using dashed lines.
In this paper, given any random variable (rv) X, x denotes a particular value of that rv, DX is the domain of X (x ∈ DX), and |DX| represents its cardinality.
In the above model, Pt is the rv representing the phone variable, Ht models the current position within a phone, St is the state, Ot the acoustic observations, At and Rt indicate state and phone transitions respectively.
Here, DXt = DXt−1, ∀t,∀X.
In our implementation here, DHt,DAt ∈ {0,1,2}, DRt ∈ {0,1}.
Also δ{c1,,...,cn} is an indicator function that turns on when all the conditions {c1,,...,cn} are true (i.e.
a conjunction over all the conditions).
The distribution for Ht is given by p(ht|ht−1,rt−1,at−1) = δ{ht=0,rt−1=1} + δ{ht=at−1+ht−1,rt−1=0}, which implies that we always start a phone with Ht = 0.
We allow skips in each phone model, and At=0, indicates no transition, At=1 implies you transition to the next state, At=2 causes a state to skip (Ht+1 = Ht + 2).
As the TIMIT corpus provides phone level segmentations, Pt is observed during training.
However, for reasons that will become clear in the next section, we treat Pt as hidden but make it the parent of a rv Ct, with, p(ct = 1|pt) = δlt=pt where lt is obtained from the transcriptions (lt ∈ DPt).
The above formulation has exactly the same effect as making Pt observed and setting it equal to lt (Bilmes, 2004).
Additional details on other CPTs in this model may be found in (Bilmes and Bartels, 2005).
We provide more details on the baseline system in section 4.1.
Our main reason for choosing the TIMIT phone recognition task is that TIMIT includes both sequence and segment transcriptions (something rare t t t1 4 7 p2p1 Labeled Unlabeled t5t2 t6tt 8t30 Figure 2: Illustration showing our rendition of Virtual Evidence.
for LVCSR corpora such as Switchboard and Fisher).
This means that we can compare against a model that has been trained fully supervised.
It is also well known that context-dependent (CD) models outperform CI models for the TIMIT phone recognition task (Glass et al., 1996).
We used CI models primarily for the rapid experimental turnaround time and since it still provides a reasonable test-bed for evaluating new ideas.
We do note, however, that our baseline CI system is competitive with recently published CD systems (Wang and Fosler-Lussier, 2006), albeit which uses many fewer components per mixture (see Section 4.1). 3 Soft-supervised Learning With VE Given a joint distribution over n variables p(x1,,...,xn), evidence simply means that one of the variables (w.l.o.g.
x1) is known.
We denote this by ¯x1, so the probability distribution becomes p(¯x1,,...,xn) (no longer a function of x1).
Any con guration of the variables where x1 negationslash= ¯x1 is never considered.
We can mimic this behavior by introducing a new virtual child variable c into the joint distribution that is always observed to be one (so c = 1), and have c interact only with x1 via the CPT p(c = 1|x1) = δx1=¯x1.
Therefore,summationtext x1 p(c = 1,x1,,...,xn) = p(¯x1,,...,xn).
Now consider setting p(c = 1|x1) = f(x1), where f() is an arbitrary non-negative function.
With this, different treatment can be given to different assignments to x1, but unlike hard evidence, we are not insisting on only one particular value.
This represents the general notion of VE.
In a certain sense, the notion of VE is similar to the prior distribution in Bayesian inference, but it is different in that VE expresses preferences over combinations of values of random variables whereas a Bayesian prior expresses preferences over combinations of model parameter values.
For a more information on VE, see (Bilmes, 2004; Pearl, 1988).
VE can in fact be used when accurate phone level segmentations are not available.
Consider the illustration in Figure 2.
As shown, t1 and t4 are the 166 start and end times respectively for phone p1, while t4 and t7 are the start and end times for phone p2.
When the start and end times for each phone are given, we have information about the identity of the phone that produced each and every observation.
The general training scenario in most large vocabulary speech recognition systems, however, does not have access to these starting/ending times, and they are trained knowing only the sequence of phone labels (e.g., that p2 follows p1).
Consider a new transcription based on Figure 2, where we know that p1 ended at some time t3 ≤ t4 and that p2 started at sometime t5 > t4.
In the region between t3 and t5 we have no information on the identity of the phone variable for each acoustic frame, except that it is either p1 or p2.
A similar case occurs at the start of phone p1 and the end of phone p2.
The above information can be used in our model (Figure 1) in the following way (here given only for t2 ≤ t ≤ t6): p(Ct = 1|pt) = δ{pt=p1,t2≤t≤t3} + δ{pt=p2,t5≤t≤t6} + ft(p1)δ{pt=p1,t3≤t≤t5} + gt(p2)δ{pt=p2,t3≤t≤t5}.
Here ft(p1) and gt(p2) represent our relative beliefs at time t in whether the value of Pt is either p1 or p2.
It is important to highlight that rather than the absolute values of these functions, it is their relative values that have an effect on inference (Bilmes, 2004).
There are number of different ways of choosing these functions.
First, we can set ft(p1) = gt(p2) = α,α > 0.
This encodes our uncertainty regarding the identity of the phone in this region while still forcing it to be either p1 or p2, and equal preference is given for both (referred to as uniform over two phones ).
Alternatively, other functions could take into account the fact that, in the frames ‘close’ to t3, it is more likely to be p1, whereas in the frames ‘close’ to t5, it is more likely to be p2.
This can be represented by using a decreasing function for ft(p1) and an increasing function for gt(p2) (for example linearly increasing or decreasing with time).
As more frames are dropped around transitions (e.g., as t3 − t2 decreases), we use lesser amounts of labeled data.
In an extreme situation, we can drop all the labels (t3 < t2) to recover the case where only sequence and not segment information is available.
Alternatively, we can have t3 = t2 +1, which means that only one frame is labeled for every phone in an utterance all other frames of a phone are left untranscribed.
From the perspective of a transcriber, this simulates the task of going through an utterance and identifying only one frame that belongs to 0 10 20 30 40 50 60 70 80 90 10052 54 56 58 60 62 64 % of Unused Segmentation Data Phone Accur a cy Baseline Uniform over 2 phones Linear Interpolation Figure 3: Virtual Evidence Results each particular phone without having to identify the phone boundary.
In contrast to the task of determining the phone boundary, identifying one frame per word unit is much simpler, less prone to error or disagreement, and less costly (Greenberg, 1995).
4 Experimental
Results 4.1 Baseline System We trained a baseline TIMIT phone recognition system that made full use of all phone level segmentations (the fully supervised case).
To obtain the acoustic observations, the signal was rst preemphasized (α = 0.97) and then windowed using a Hamming window of size 25ms at 100Hz.
We then extracted MFCC’s from these windowed features.
Deltas and double deltas were appended to the above observation vector.
Each phone is modeled using 3 states, and 64 Gaussians per state.
We follow the standard practice of building models for 48 different phones and then mapping them down to 39 phones for scoring purposes (Halberstadt and Glass, 1997).
The decoding DBN graph is similar to the training graph (Figure 1) except that the variable Ct is removed when decoding.
We test on the NIST Core test set (Glass et al., 1996).
All results reported in this paper were obtained by computing the string edit (Levenshtein) distance between the hypothesis and the reference.
All models in this paper were implemented using the Graphical Models Toolkit (GMTK) (Bilmes and Bartels, 2005).
4.2 VE
Based Training and Results We tested various cases of VE-based training by varying the amount of dropped frame labels on either side of the transition (the dropped labels became the unlabeled frames of Figure 2).
We did this until there was only one frame left labeled for every phone.
Moreover, in each of the above cases, we tested a number of different functions to gener167 ate the VE scores (see section 3).
The results of our VE experiments are shown in Figure 3.
The curves were obtained by tting a cubic spline to the points shown in the gure.
The phone accuracy (PA) of our baseline system (trained in a fully supervised manner) is 61.4%.
If the total number of frames in the training set is NT, and we drop labels on N frames, the amount of unused data is given by U = NNT ∗100 (the x-axis in the gure).
Thus U = 0% is the fully supervised case, whereas U = 100% corresponds to using only the sequence information.
Dropping the label for one frame on either side of every phone transition yielded U = 24.5%.
It can be seen that in the case of both uniform over 2 phones and linear interpolation, the PA actually improves when we drop a small number (≤ 5 frames) of frames on either side of the transition.
This seems to suggest that there might be some inherent errors in the frame level labels near the phone transitions.
The points on the plot at U=84.7% correspond to using a single labeled frame per phone in every utterance in the training set (average phone length in TIMIT is about 7 frames).
The PA of the system using a single label per phone is 60.52%.
In this case, we also used a trapezoidal function de ned as follows: if t = ti were the labeled frames for phone p1, then ft(p1) = 1,ti − 1 ≤ t ≤ ti + 1, and a linear interpolation function for the other values t during the transition to generate the VE weights.
This system yielded a PA of 61.29% (baseline accuracy 61.4%).
We should highlight that even though this system used only 15.3% of the labels used by the baseline, the results were similar!
The gure also shows the PA of the system that used only the sequence information was about 53% (compare against baseline accuracy of 61.4%).
This lends evidence to the claim that training recognizers using data with time segmentation information can lead to improved performance.
Given the procedure we used to drop the frames around transitions, the single labeled frame for every phone is usually located on or around the midpoint of the phone.
This however cannot be guaranteed if a transcriber is asked to randomly label one frame per phone.
To simulate such a situation, we randomly choose one frame to be labeled for every phone in the utterance.
We then trained this system using the uniform over 2 phones technique and tested it on the NIST core test set.
This experiment was repeated 10 times, and the PA averaged over the 10 trails was found to be 60.5% (standard deviation 0.402), thus showing the robustness of our technique even for less carefully labeled data.
5 Discussion
In this paper we have shown how VE can be used to train a TIMIT phone recognition system using partially labeled data.
The performance of this system is not signi cantly worse than the baseline that makes use of all the labels.
Further, though this method of data transcription is only slightly more time consuming that sequence labeling, it yeilds signi cant gains in performance (53% v/s 60.5%).
The results also show that even in the presence of fully labaled data, allowing for uncertainity at the transitions during training can be bene cial for ASR performance.
It should however be pointed out that while phone recognition accuracy is not always a good predictor of word accuracy, we still expect that our method will ultimately generalize to word accuracy as well, assuming we have access to a corpus where at least one frame of each word has been labeled with the word identity.
This work was supported by an ONR MURI grant, No.
N000140510388. References [Bilmes and Bartels2005] J.
Bilmes and C.
Bartels. 2005.
Graphical model architectures for speech recognition.
IEEE Signal Processing Magazine, 22(5):89 100, September.
[Bilmes2004] J.
Bilmes. 2004.
On soft evidence in Bayesian networks.
Technical Report UWEETR-2004-0016, University of Washington, Dept.
of EE.
[Glass et al.1996] J.
Glass, J.
Chang, and M.
McCandless. 1996.
A probabilistic framework for feature-based speech recognition.
In Proc.
ICSLP ’96, volume 4, Philadelphia, PA.
[Greenberg1995] S Greenberg.
1995. The Switchboard transcription project.
Technical report, The Johns Hopkins University (CLSP) Summer Research Workshop.
[Halberstadt and Glass1997] A.
K. Halberstadt and J.
R. Glass.
1997. Heterogeneous acoustic measurements for phonetic classi cation.
In Proc.
Eurospeech ’97, pages 401 404, Rhodes, Greece.
[Lamel et al.2002] L.
Lamel, J.
Gauvian, and G.
Adda. 2002.
Lightly supervised and unsupervised acoustic model training.
Computer Speech and Language.
[Pearl1988] J.
Pearl. 1988.
Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann Publishers, Inc.
[Subramanya et al.2006] A.
Subramanya, A.
Raj, J.
Bilmes, and D.
Fox. 2006.
Recognizing activities and spatial context using wearable sensors.
In Proc.
of the Conference on Uncertainty in Arti cial Intelligence (UAI).
[Wang and Fosler-Lussier2006] Y.
Wang and E.
Fosler-Lussier. 2006.
Integrating phonetic boundary discrimination explicity into HMM systems.
In Proc.
of the Interspeech.

