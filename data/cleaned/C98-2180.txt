An Interactive Domain Independent Approach to Robust 
Dialogue Interpretation 
Carolyn Penstein Ros6 
LRDC 520, University of Pittsburgh 
3939 Oh~tra St., 
Pittsburgh PA, 15260 
rosecp©pitt, edu 
Lori S. Levin 
Ca, rne.gie Mellon University 
Center for M~chine Translation 
I)ittsburgh, PA 15213 
lsl~cs, cmu. edu 
Abstract 
We discuss an interactive approach to robust, inter
pretation in a large scale speech-to-speech transla
tion system. Where other interactive approaches to 
robust interpretation ha,re depeuded upon domain 
dependent repair rules, the approach described here 
operates etIiciently without any such hand-coded re
pair knowledge and yields a 37% reduction in error 
rate over a corpus of noisy sentences. 
1 Introduction

Iu this paper we discuss lt.OSE, all interactive ap
1)roach to robust interpretation developed in the 
context of the JANUS speech-to-speech translation 
system (Lavie et al., 1996). Previous interactive 
approaches to robust interl)retation have either re
quired excessive amounts of interaction (II.os~ and 
Wail)el, 1994), depended upon domain dependent 
repair rules (Van Noord, 1996; l)anieli and Gerbino, 
1995), or relied on the rninimum distance parsing 
approach (Hipp, 1992; Smith, 1992; Lehman, 11989) 
which has been shown to be intractable in a large
scale system (F/.os~ and Lavie, 1997). in contrast, 
the ROSE approach operates efficiently without any 
hand-coded repair knowledge. All empirical evalu
ation demonstrates the efficacy of t.his domain in
dependent approach. A further evaluation demon
strates that the ROSE approach combines easily 
with available domain knowledge in order to improve 
the quMity of the interaction. 
The ROSE approach is based on a model of hu
man comnmnication between speakers of different 
languages with a small shared language base. Hu
mans who share a very small language base are 
able to communicate when the need arises by sim
plifying their speech patterns and negotiating un
til they manage to transmit their ideas to one an
other (Hatch, 1983). As the speaker is speaking, 
the listener "casts his net" in order to catch those 
fragments of speech that are comprehensible to him, 
which he then attempts to fit together semantically. 
His subsequent negotiation with the speaker builds 
upon this partial understanding. Similarly, ROSE 
repairs ext.ragranmlatical input in two phases. The 
first phase, Repair ltypothesis Formation, is respon
sible for assembling a set of hypotheses about the 
meaning of the ungrammatical utterance. In the 
second phase, hlteraetion with the User, the sys
tem generates a set of queries, negotiating with the 
speaker in order to narrow down to a single best 
meaning representation hypothesis. 
This approach was evaluated in the context of 
the JANUS multi-lingual machine translation sys
tem. First, the system obtains a meaning represen
tation for a sentence uttered in the source language. 
Then the resulting meaning representation structure 
is mapped onto a sentence in the target language 
using GENKIT (Tomita and Nyberg, 1988) with a 
sentence level generation grammar. Currently, the 
translation system deals with tlle scheduling domain 
where two speakers attempt to schedule a meeting 
together over the l)hone. This paper focuses on 
the Interaction phase. Details about the llypoth
esis Formation phase are found ill (Rosd, 1997). 
2 Interactive
Repair In Depth 
As mentioned above, ROSE repairs extragrammat
ical input ill two phases. The first phase, t{epair 
ttypothesis Fornmtion, is responsible for assembling 
a ranked set of tell or fewer hypotheses about the 
meaning of the ungrammatical utterance expressed 
in the source language. This phase is itself divided 
into two stages, Partial Parsing and Combination. 
The Partial Parsing stag(" is similar to the concept 
of the listener "casting his net" for comprehensi
ble fi'agments of speech. A robust skipping parser 
(Lavie, 1995) is used to obtain an analysis for islands 
of the speaker's sentence. Ill the Combination stage, 
the fi'agments fi'om the partial parse are assembled 
into a ranked set of alternative nmaning represen
tation hypotheses. A genetic programming (Koza, 
1992; Koza, 1994) approach is used to search for 
dill>rent ways to combine the fi'agments in order to 
avoid requiring any hand-crafted repair rules. Our 
genetic programming approach has been shown pre
viously to be orders of magnitude more efficient than 
the mininmm distance parsing approach (Rosd and 
Lavie, 1997). In the second phase, Interaction with 
1129 
the user, the system generates a set of queries, nego
tiating with the speaker in order to narrow down to 
a single best meaning representation hypothesis. Or, 
if it determines based on the user's responses to its 
queries that none of its hypotheses are acceptable, 
it requests a rephrase. 
Inspired by (Clark and Wilkes-Gibbs, 1986; Clark 
and Schaefer, 1989), the goal of the Interaction 
Phase is to minimize collaborative effort between 
the system and the speaker while maintaining a. high 
level of interpretation accuracy. It uses this princi
ple in determining which portions of the speaker's 
utterance to question. Thus, it focuses its interac
tion on those portions of the speaker's meaning that 
it is particularly uncertain about. In its question
ing, it. attempts to display the state of the system's 
understanding, acknowledging information conveyed 
by the speaker as it becomes clear. The interaction 
process can be summarized as follows: The system 
first assesses the state of its understanding of what 
the speaker has said by extracting features that. dis
tinguish the top set of hypotheses from one another. 
It then builds upon this understanding by cycling 
through the following four step process: selecting a 
feature; generating a natural language query from 
this feature; updating its list of alternative hypothe
ses based on the user's answer; and finally updating 
its list of distinguishing features based on the re
maining set of alternative hypotheses. 
2.1 Extracting
Distinguishing Featm'es 
In the example in Figure 1, the Hypothesis Forma
tion phase produces three alternative hypotheses. 
The hypotheses are ranked using a trained evalua
tion function, but the hypothesis ranked first is not 
guaranteed to be best. in this case, the hypothe
sis ranked as second is the best. hypothesis. The 
hypotheses are expressed in a frame-based featnre 
structure representation. Above each hypothesis is 
the corresponding text generated by the system for 
the associated feature structure. 
In order for the system to return the correct hy
pothesis, it, must use interaction to narrow down the 
list of alternatives to the best single one. The first 
task of the Interaction Mechanism is to determine 
what the system knows about, what the speaker has 
said and what it is not certain about. It does this 
by comparing the top set of repair hypotheses and 
extracting a set of features that distinguish them 
fl'om one another. The set of distinguishing features 
corresponding to the example set of alternative hy
potheses can be found in Figure 2. 
Tile meaning representation's recursive structure 
is made up of fl'ames with slots that can be filled ei
ther with other frames or with atomic fillers. These 
compositional structures can be thought of as trees, 
with the top level frame being the root of the tree 
aim branches attached through slot.s. Tile features 
Sentence: What did you say 'bout what 
was your schedule for the twenty sixth of May? 
Alternative Hypotheses: 
"What will be scheduled for the twenty
sixth of May" 
((frame *schedule) 
(what ((frame *what)(wh +))) (wheu ((frame *simple-ti,ne)(day 26)(month ,5)))) 
"You will schedule what for the twenty
sixth of May?" 
((fl'ame *schedule) 
(who ((frame *you))) 
(what ((frame *what)(wh +))) 
(when ((frame *simple-time)(day 26)(month 5)))) 
"Will schedule for tile twenty-sixth 
of May" 
((fl'ame *schedule) 
(when ((frame *simple-time)(day 26)(month 5)))) 
Figure 1: Example Alternative Hypotheses 
((f *schedule) (s who)) 
((f *you)) 
((f *schedule) (s what)) 
((f *schedule) (s what)(f *what)) 
((f *what)) 
((f +)) 
((f *what)(s wh)(f +)) 
((f *schedule)(s who)(f *you)) 
((f *schedule)(s what)(f *what)(s wh)(f +)) 
Figure 2: Distinguishing Features 
used in the system to distinguish alternative mean
ing representation structures from one another spec
ify paths down this tree structure. Thus, the distin
guishing features that are extracted are always an
chored in a fralne or atomic filler, marked by an f 
in Figure 2. Within a feature, a frame may be fol
lowed by a slot, marked by an s. And a slot may 
be followed by a frame or atolnic filler, and so on. 
These features are generated by comparing the set. 
of feature structures returned fi'om the ttypothesis 
Formation phase. No knowledge about what the fea
tures mean is needed in order to generate or use 
these features. Thus, the feature based approach 
is completely domain independent. It can be used 
without modification with any frame-based meaning 
representation. 
1130 
When a feature is applied to a meaning repre
sentation structure, a value is obtained. Thus, fea
tures can be used to assign meaning representatiou 
structures to classes according to what value is ob
tained for each when the feature is applied. For 
example, the feature ((f *schedule) (s who)(f 
*you)), distinguishes structures that contain the 
filler *you in the who slot in the *schedule frame 
h-ore those that do not. When it is applied to struc
tures that contain the specified fl'ame in the specified 
slot., it returns true. When it is applied to structures 
that do not, it returns false. Thus, it groups the 
first and third hypotheses in one class, and the sec
ond hypothesis in another class. Because tile wdue 
of a feature that ends in a frame or atomic filler 
can have either true or false as its value, these are 
called yes/no fe, atures. When a feature that ends ill 
a slot, such as ((f *schedule)(s who)), is applied 
to a feature structure, the value is tile filler in the 
specified slot. These features are called wh-features. 
Each feature is associated with a question that 
tile systern could ask the user. The purpose of the 
generated question is to determine what the value 
of the feature should be. The sysi.em can then kee I) 
those hypotheses that are consistent with that fea
ture value and eliminate from consideration the rest. 
Generating a natural language question from a fea
ture is discussed in section 2.3. 
2.2 Selecting
a Feature 
Once a set of features is extracted, the s3,stem en
ters a loop ill which it selects a feature from the 
list, generates a query, and then updates the list of 
alternative hypotheses and remaining distinguishing 
features based on the user's response. It attempts 
to ask the most informative questions first in order 
to limit the nmnber of necessary questions. It. uses 
the following four criteria in making its selection: 
. Askable: Is it possible to ask a natural ques
tion from it? 
. Evaluatable: Does it ask about a single repair 
or set of repairs that always occur together? 
. In Focus: Does it involve informatiou from the 
common ground? , Most Intbrmative: Is it likely to result in the 
greatest search space reduction? 
First, tile set of features is narrowed down to those 
features that represent askable questions. For exam
pie, it, is not natural to ask about the filler of a par
ticular slot in a particular h'ame if it is not known 
whether the ideal meaning representation structure 
contains that frame. Also, it is awkward to gen
erate a wh-question based on a feature of length 
greater than two. For example, a question corre
sponding t,o ((f *how)(s what)(f *interval) (s 
end) ) might be phrased something like "How is tile 
time ending when?". So even-lengthed features more 
than two elements long are also eliminated at this 
stage. 
The next criterion considered by the Interaction 
phase is evahmt.ability. In order for a Yes/No ques
tion to be evaluatable, it. must confirm only a single 
repair action. Otherwise, if the user responds with 
"No" it. cannot be determined whether the user is 
rejecting both repair actions or only one of them. 
Next, the set of features is narrowed down to those 
that can easily be identified as being ill fOCUS. In or
der to do this, the system prefers to use features 
that ow~rlap with structures that all of tile alter
native hypotheses have in common. Thus, the sys
tem encodes as much common ground knowledge in 
each question as possible. The structures that all 
of the alternative hypotheses share are called no,l
controversial substruetu~vs. As the negotiation con
tinues, these tend to be structures that have been 
confirmed through interaction. Including these sub
structures has the effect of having questions tend 
to follow in a natural succession. It. also has tile 
other desirable effect that the system's state of un
derstanding the speaker's sentence is indicated to 
the speaker. 
The final piece of information used in selecting 
between those remaining features is the expected 
search reduction. The expected search reduction in
dicates how much tile search space, can be expected 
to be reduced once tile answer to tile corresponding 
question is obtained from the user. Equation l is 
for cah:ulating £'i., the expected search reduction of 
feature number f. 
'zl l = × (L -<j) (,) 
i=1 
L is the number of' alternative llypotheses. As 
mention above, each feature can be used to assign 
the hypotheses to equivalence classes, li,I is the 
number of alternative hypotheses ill the ith equiv
a.lence class of feature f. If the value for feature f 
associated with the (:lass of length lij is the correct 
value, li,I will be the new size of the search space. 
In this case, the actual search reduction will be tile 
current number of hypotheses, L, minus tile number 
of alternative hypotheses in the resulting set, li,I. 
Intuitively, the expected search reduction of a fea
ture is the sum over all of a feature's equivalence 
classes of the percentage of hypotheses in that (:lass 
times tim reduction in the search space assuming the 
associated value for that feature is correct. 
The first three criteria select a subset of tile cur
rent distinguishing features which the final crite
rion then ranks. Note that all of these criteria call 
be evaluated without the system having any under
standing about what the features actually mean. 
1131 
Selected Feature: 
((f *schedule)(s wha,t)(f *what)(s wh)(f +)) 
Non-controversial Structures if Answer 
to Question is Yes: 
((when ((lnonth 5)(day 26)(frame *simple-tilne))) 
(frame *schedule) 
(what ((wh +)(frame *what)))) 
Question Structure: 
((when ((month 5)(day 26)(frame *simple-time))) 
(frame *schedule) 
(what ((wh +)(frame *what)))) 
Question Text: 
Was something like WHAT WILL BE 
SCHEDULED FOR. THE TWENTY-SIXTH 
OF MAY part of what you meant? 
Figure 3: Query Text Generation 
2.3 Generating
Query Text 
The selected feature is used to generate a query for 
the user. First, a skeleton structure is built from 
the feature, with top level frame equivalent to the 
franle at the root of the feature. Then the skeleton 
structure is filled out with the non-controversial sub
structures. If the question is a Yes/No question, it 
includes all of the substructures that would be non
controversial assuming the answer to the question 
is "Yes". Since information confirmed by the pre
vious question is now considered non-controversial, 
the result of the previous interaction is made evident 
in how the current question is phrased. An exam
pie of a question generated with this process can be 
found ill Figure 3. 
If the selected feature is a wh-feature, i.e., if it is 
an even lengthed feature, the question is generated 
in the form of a wh-question. Otherwise the text 
is generated declaratively and the generated text is 
inserted into the following formula: "Was something 
like XXX part of what you meant?", where XXX is 
filled in with the generatec't text. The set of alter
native answers based on the set of Mternative hy
potheses is presented to the user. For wh-questions, 
a final alternative, "None of these alternatives are 
acceptable", is made available. Again, no particu
lar domain knowledge is necessary for the purpose 
of generating query text from features since the sen
tence level generation component fi'om the system 
can be used as is. 
2.4 Processing
the UseFs Response 
Once the user has responded with the correct, value 
for the feature, only the alternative hypotheses that 
have that value for that feature are kept, and the rest 
"What will be scheduled for the twenty
sixth of May" 
((what ((frallle *what)(wh -4-))) (when ((frame *simple-time)(day 26)(month 5))) 
(frame *schedule)))) 
"You will schedule what for the twenty
sixth of May?" 
((what ((fralne *what)(wh +))) 
(frame *schedule) 
(when ((frame *simple-time)(day 26)(nlonth 5))) 
(who ((frame *yon)))) 
Figure 4: Remaining Hypotheses 
((f *schedule)(s who)) 
((f *you)) 
((f *schedule)(s who)(f *you)) 
Figure 5: Remaining Distillguishing Features 
are eliminated. In the case of a wh-question, if tile 
user selects "None of these alternatives are accept
able", all of the alternative hypothesized structures 
are eliminated and a rephrase is requested. After 
this step, all of the features that no longer parti
tion the search space into equivalence classes are also 
eliminated. In the example, assume the answer to 
the generated question in Figure 3 was "Yes". Thus, 
the result is that two of the original three hypothe
ses are remaining, displayed in Figure 4, and the re
maining set of features that still partition tile search 
space can be found in Figure 5. 
If one or more distinguishing features rema.in, the 
cycle begins again by selecting a feature, generating 
a question, and so on until the system narrows down 
to the final result. If the user does not answer posi
tively to any of the system's questions by the time it. 
runs out of distinguishing features regarding a par
ticular sentence, the system loses confidence ill its 
set of hypotheses and requests a rephrase. 
3 Using
Discourse Information 
Though discourse processing is not essential to 
the ROSE approach, discourse information has 
been found to be useful in robust interpretation 
(Ramshaw, 1994; Smith, 1992). In this section we 
discuss how discourse information can be used for 
focusing the interaction between system and user on 
the task level rather than on the literal meaning of 
the user's utterance. 
A plan-based discourse processor (Ros6 et al., 
1995) provides contextual expectations that guide 
the system in the manner in which it formulates 
1132 
Sentence: What about any time but the ten to twelve 
slot on Tuesday the thirtieth? 
Hypothesis 1: 
"tlow about from ten o'clock till twelve o'clock 
~lkmsday the thirtieth any time" 
((frame *how) 
(when (*multiple* 
((end ((frame *simple-time) (hoar 12))) 
(start ((frame *simple-time)(hour 10))) 
(incl~excl inclusive) 
(frame *interval)) 
({frame *slmple-tlme) 
(day 30) 
(day-of-week tuesday)) 
((specifier any) (name time) 
(frame *special-time))))) 
Hypothesis 2: 
"From ten ()'clock till Tuesday the thirtieth 
twelw~ o'clock" 
( (frame *interval) 
(incl-excl inclusive) 
(start ((fl'ame *simple-time)(hour 10))) 
(end (*multiple* 
({frame *simple-time) 
(day 30) 
(day-of-week tuesday)) 
(( frame *simple-tin,e) (hour 12))))) 
Selected Feature: ((f *how)(s when)(f *interval)) 
Query Without discourse: Was something like 
"how about from ten o'clock till twelve 'clock" part 
of what you meant? 
Query With discourse: Are you suggesting that 
Tuesday November the thirtieth from ten a.m. till 
twelve a.m. is a good time to meet? 
Figure 6: Modified Question Generation 
queries to the user. By computing a structure for 
the dialogue, the discourse processor is able to iden
tify the speech act. performed by each sentence. Ad
ditionally, it, augments temporal expressions from 
context. Based on this information, it computes 
the constraints on the speaker's schedule expressed 
by each sentence. Each constraint associates a sta
tus with a particular speaker's schedule for time 
slots within the time indicated by the temporal 
expression. There are seven possible statuses, in
eluding accepted, suggested, preferred, neutral, 
d±spreferred, busy, and rejected. 
As discussed above, the Interaction Mechanism 
uses features that distinguish between alternative 
hypotheses to divide the set of alternative repair hy
potheses into classes. Each member within the same 
class has the same value for the associated feature. 
By comparing computed status and augnlented tem
poral information for alternative repair hypotheses 
within tile same class, it is possible to determine 
what common implications for tile task each member 
or most of tile members ill the associated (:lass have. 
Thus, it is possible to compute what implications 
for the task are associated with the corresponding 
value for the feature. By comparing this common in
formation across classes, it is possible to determine 
whether the feature makes a consistent distinction 
on the task level, ff so, it is possible to take this 
distinguishing information and use it for refocusing 
the associated question on the task level rather than 
on the level of the sentence's literal meaning. 
In tile example in Figure 6, the parser is not able 
to correctly process the "but", causing it to miss 
the fact that the speaker intended any other time 
besides ten to twelve rather than particularly ten to 
twelve. Two alternative hypotheses are constructed 
during the Hyl)othesis F()rmation phase. However, 
neither hypothesis correctly represents the meaning 
of tile sentence. In this case, the purpose of the 
interaction is to indicate to the system that neither 
of the hypotheses are correct attd that a rephrase is 
needed. This will be accomplished when the user 
answers negatively to the system's query since the 
user will not have responded positively to any of the 
system's queries regarding this sentence. 
The system selects the feature ((f *how)(s 
when)(f *interval)) to distinguish the two hy
potheses fi'om one another. Its generated query is 
thus "Was something like HOW ABOUT FROM 
TEN OCLOC'K TILL 3'WI"AA/E OCLOCK part of 
what you meant?". The discom'se processor returns 
a different result for each of these two representa
tions. In particular, only the tirst hypothesis con
tains enough intbrmation for the discourse proces
sor to compute any scheduling constraints since it 
contains both a temporal expression and a top level 
semantic frame. It would create a constraint associ
ating the status of suggested with a representation 
for Tuesday the thirtieth from ten ()'clock till twelve 
o'clock. The other hypothesis contains date infor
mation but no status inforlnation. Based on this 
difference, tile system can generate a query asking 
whether or not the user expressed this coustraint. Its 
query is "Are you suggesting that Tuesday, Novem
ber the thirtieth from ten a.m. till twelve a.m. is a 
good time to meet?" The suggested status is as
sociated with a template that looks like "Are you 
suggesting that XXX is a good t, ime to lneetT' Tile 
XXX is then filled in with tile text generated from 
tile temporal expression using the regular system 
generation grannnar. 
4 Evaluation

An empirical eva.luation was conducted ill or
der to determine how much improvement can be 
gained with limited amounts of interaction in tile 
1133 
\[ Parser 
Top Hypothesis 
1 Question

2 Questions

3 Questions

Bad 
85.0% 
64.0% 
61.39% 
59.41% 
53.47% 
Okay 
12.0% 
28.0% 
28.71% 
28.71% 
32.67% 
Perfect 
3.0% 
8.0% 
9.9% 
11.88% 
13.86% 
Total Acceptable 
15.o% 
36.0% 
38.61 
40.59% 
46.53% 
Figure 7: '15"anslation Quality As Maxiumm Number of Questions Increases 
domain independent ROSE approach. This evalu
ation is all end-to-end evaluation where a sentence 
expressed in the source language is parsed into a 
language independent meaning representation using 
the ROSE approach. This meaning representation 
is then mapped onto a sentence in the target, lan
guage. In this case both the source language and 
the target language are English. An additional eva.1
uation demonstrates the improvement in interaction 
quality that can be gained by introducing available 
domain information. 
4.1 Domain
Independent Repair 
First the system automatically selected 100 sen
tences from a previously unseen corpus of 500 sen
tences. These 1O0 sentences are the first 100 sen
tences in the set that a parse quality heuristic sim
ilar to that described in (Lavie, 1995) indicated to 
be of low quality. The parse quality heuristic evalu
ates how much skipping was necessary in the parser 
in order to arrive a.t a partial parse and how well 
the parser's analysis scores statistically. It should 
be kept. in mind, then, that this testing corpus is 
composed of 100 of the most difficult sentences fi'om 
the original corpus. 
The goal of tile ewduation was to coml)ute aver
age performance per question asked and to compare 
this with the performance with using only the partial 
parser as well as with using only tile Hypothesis For
ruction phase. In each case performance was lnea
sured in terms of a translation quality score assigned 
by an independent human judge to the generated 
natural language target text.. Scores of Bad, Okay, 
and Perfect were assigned. A score of Bad indicates 
that the translation does not capture the original 
meaning of the input sentence. Okay indicates that, 
the translation captures the meaning of tile input 
sentence, but not in a completely natural manner. 
Perfect, indicates both that the resulting translation 
captures the meaning of the original sentence and 
that. it, does so in a smooth and fluent maimer. 
Eight native speakers of English who had never 
previously used the translation system participated 
in this evaluation to interact with tile system. For 
each sentence, the participants were presented with 
the original sentence and with three or fewer ques
tions to answer. Tile parse result, the result of re
pair without interaction, and tile result for each user 
after each question were recorded in order to be 
graded later by the independent judge mentioned 
above. Note that this evaluation was conducted on 
the nosiest portion of the corpus, not on an aver
age set of naturally occurring utterances. While this 
evaluation indicates that repair withont interaction 
yields an £cceptable result, ill only 36% of these dif
ficult cases, in an evaluation over tile entire corpus, 
it was determined to return an acceptable result in 
78% of tile cases. 
A global parameter was set such that the system 
never asked more than a maximum of three ques
tions. This limitation was placed on the system in 
order to keel) the task from becoming too tedious 
and time consuming for the users. It was estimated 
that three questions was approximately the maxi
lnmn number of questions that users would be will
ing to answer per sentence. 
The results are presented in Figure 7. Repair 
without interaction achieves a 25% reduction in er
ror rate. Since the partial parser only produced suf
ficient chunks for building all acceptable repair hy
pothesis in about 26% of tile cases where it did not 
produce all acceptable hypothesis by itself, the max
imum reduction in error rate was 26%. Thus, a 25% 
reduction ill error rate without interaction is a very 
positive result. Additionally, interaction increases 
the system's average translation quality above that 
of repair without interaction. With three questions, 
tile system achieves a 37% reduction in error rate 
over partial parsing alone. 
4.2 Discourse
Based Interaction 
in a final evaluation, the quality of questions based 
only on feature information was compared with that 
of questions focused on the task level using discourse 
information. Tile discourse processor was only able 
to provide sufficient information for reformulating 
22% of the questions in terms of the task. The rea
son is that this discourse processor only provides in
formation for reformulating questions distinguishing 
between meaning representations that differ in terms 
of status and augmented telnporal information. 
Four independent human judges were asked to 
grade pairs of questions, assigning a score between 
1 and 5 for relevance and form and indicating which 
question they would prefer to answer. They were 
instructed t.o think of relevance in terms of how use
1134 
fld they expected the question would be in helping a 
computer understand the sentence the question was 
intended to clarify. For form, they were instructed 
to evaluate how natural and smooth sounding the 
generated question was. 
Illteraction without discourse received on average 
2.7 for form and 2.4 for relevance. Interaction with 
discourse, on the other hand, received 4.1 for form 
and 3.7 for relevance. Subjects preferred the dis
course influenced question in 73.6% of the cases, ex
pressed no preference in 14.8% of the cases, and pre
ferred interaction without discourse in 11.6% of the 
cases. Though the discourse influenced question was 
not preferred universely, this evaluation supports the 
claixn thai humans prefer to receive clarifications on 
the task level and indicates that further exploration 
in using discourse information in repair, and partic
ularly in interaction, is a promising a.venue for future 
research. 
5 Conclusions
and Current 
Directions 
This paper presents a domain independent, interac
tive approach to robust interpretation. Where other 
interactive approaches to robust interpretation have 
depended upon domain dependent repair rules, the 
approach described here operates efficiently without 
any such hand-coded repair knowledge. An empir
ical evaluation demonstrates that limited amounts 
of focused interaction allow this repair approach to 
achieve a a7% reduetion in error rate over a corpus of 
noisy sentences. A further evaluation demonstrates 
that this doinain independent approach combines 
easily with available domain knowledge in order to 
improve the quality of the interaction. Introducing 
discourse information yields a preferable query in 
74% of the eases where discourse information ap
plies. Interaction in the current ROSE approach 
is limited to confirming hypotheses about how the 
fragments of the partial parse can be combined and 
requesting rephrases. It would be interesting to gen
erate and test hypotheses about information missing 
fi'om the partial parse, perhaps using information 
predicted by the discourse context. 

References 

It. H. Clark and E. F. Schaefer. 1989. Contributing 
to discourse. Cognitive Science, 13:259-294. 

H. I1. (;lark and D. Wilkes-Gibbs. 1986. Referring 
as a collaborative process. Cognition, 22:1-39. 

M. Danieli and E. Gerbino. 1995. Metrics for evalu
ating dialogue strategies in a spoken language sys
tem. In Working Notes of the AAAI ,5'pring Sym
posium on Empirical Methods in Discourse Inter
pretation and Generation. 

E. IIateh. 1983. Simplified input and second 
language acquisition. In I{. Andersen, editor, 
Pidginizatiort and Crcolization as Language Ac
qmsition. Newbury House Publishers. 

D. 1{. Hipp. 1992. Design and Development of 
Spoken Natural-Languag( Duflog Parsing Systems. 
Ph.l). thesis, l)ept, of Computer Science, l)uke 
University. 

a. Koza. 1992. Genetic f~roqra'mming: On llu Pro
grammin 9 of Computers by Means of Natural Se
lection. MIT Press. 
,I. Koza. 1994. Genetic Programminq ii. MIT l'r(~ss. 
A. l,avie, 1). Gates, M. Gavalda, L. Mayfiehl, and 

A. Waibel L. Levin. 1996. MultiAingual transla
tion of spontaneously spoken language in a limited 
domain. In Proceedings of COLING 95, Kope,
haqen. 

A. l,avie. 1995. A Grammar Based leol,,,st Parser 
I%r 5>ontaneous 5'peech. Ph.D. thesis, School of 
C, omputer Science, C, arnegie Mellon University. 
J. F. Lehman. 198{t. Adaptive Parsin.q: Self
Ea:tending Natural Language Intc.rfaces. Ph.l). 
thesis, School of Coniputer Science, (;arnegie Mel
lon University. 

L. A. l/anlshaw. 1994. Correcting real-world 
spelling errors using a model of the problem
solving context. Computational lntellige,ce, 
10(2). 

C. P. It os6. and A. Lavie. 1997. An efficient distribn
tion of' labor in a two stage robust iiiterpretation 
process, hi PTvceedmgs of tl~e Second Conference 
on lTmpirical Methods in Natural Language Pro
ccssing. 

C. P. ll,os5 and A. Waibel. 1994. tleeovering fi'om 
parser failures: A hybrid statistical/symbolic ap
proach. In Proceedings of The Balancing Act: 
Combining Symbolic and Statistical Approaches to 
Lanquage workshop at the 32nd Annual Meeting of 
the A CL. 

C. l'. Ros¢!, B. Di Eugenio, L. S. Levin, and C. Van 
Ess-l)ykema. 1995. l)iscourse processing of dia
logues with multiple threads, hi Proceedings of 
the A CL. 

C. P. F{.osd. 1997. Robust I,tcractive Dialogue Inter
prctation. Ph.D. thesis, School of Computer Sci
ence, Carnegie Mellon University. 

R. Smith. 1992. A Compulational Model of 
E.vpectation-Driven Mixed-Initiative Dialog Pro
cessing. Ph.D. thesis, CS Dept., l)uke University. 

M. Tomita and E. H. Nyberg. 1988. Generation kit 
and transformation kit version a.2: User's man
ual. Technical Report CMU-CMT-88-MEMO, 
Carnegie Mellon University, Pittsburgh, PA. 

G. Van Noord. 1996. Robust parsing with the head
corner parser. In l)roecedings of the Eight Euro
pean Summer School In Logic , Language and In
formation, Prague, Czech Republic. 

